[
  {
    "id": "q1",
    "type": "drag-sequence",
    "question": [
      "You have an app that manages feedback.",
      "You need to ensure that the app can detect negative comments by using the Sentiment Analysis API in Azure AI Language.",
      "The solution must ensure that the managed feedback remains on your company’s internal network.",
      "Which three actions should you perform in sequence?"
    ],
    "options": [
      { "id": "a", "text": "Provision the Language service resource in Azure" },
      { "id": "b", "text": "Deploy a Docker container on an on-premises server" },
      { "id": "c", "text": "Run the container and query the prediction endpoint" },
      { "id": "d", "text": "Upload data to Azure Blob Storage" }
    ],
    "correctAnswer": ["a", "b", "c"],
    "explanation": "The correct sequence is: Provision the Language service → Deploy the container on-premises → Run the container and query it locally. Uploading to Azure is not required because the solution must keep data on-premises."
  },
  {
    "id": "q2",
    "type": "dropdown-pair",
    "question": [
      "You are building a chatbot.",
      "You need to use the Content Moderator service to identify messages that contain sexually explicit language.",
      "Which section in the response from the service will contain the category score, and which category will be assigned to the message?"
    ],
    "leftOptions": [
      { "id": "classification", "text": "Section-Classification" },
      { "id": "pi", "text": "Section-pi" },
      { "id": "terms", "text": "Section-Terms" }
    ],
    "rightOptions": [
      { "id": "cat1", "text": "Category-1" },
      { "id": "cat2", "text": "Category-2" },
      { "id": "cat3", "text": "Category-3" }
    ],
    "correctAnswer": { "left": "classification", "right": "cat1" },
    "explanation": "The 'Classification' section returns the likelihood of content falling into various categories like Category-1 for sexual content."
  },
  {
    "id": "q3",
    "type": "drag-sequence",
    "question": [
      "You have a question-answering project in Azure AI Language.",
      "You need to migrate the project to a Language service instance in a different Azure region.",
      "What are the three sequential actions you should perform to accomplish this?"
    ],
    "options": [
      { "id": "1", "text": "Train and publish model from the new instance." },
      { "id": "2", "text": "Enable custom entity recognition from the new instance." },
      { "id": "3", "text": "From the original instance, export the existing project." },
      { "id": "4", "text": "From the new instance, import the project file." },
      { "id": "5", "text": "From the new instance, regenerate the keys." }
    ],
    "correctAnswer": ["3", "4", "1"],
    "explanation": "To migrate a question-answering project, first export the project from the original instance (3), then import it into the new instance (4), and finally train and publish the model from the new instance (1)."
  },
  {
    "id": "q4",
    "type": "drag-sequence",
    "question": [
      "You are using a Conversational Language Understanding (CLU) service to handle natural language input from users of a web-based customer agent.",
      "Users report that the agent frequently responds with a generic message: 'Sorry, I don't understand that.'",
      "You need to improve the agent's ability to respond to user requests effectively.",
      "Which three actions should you perform in sequence to achieve this?"
    ],
    "options": [
      { "id": "1", "text": "Validate the utterances logged for review and modify the models." },
      { "id": "2", "text": "Add prebuilt domain models as required." },
      { "id": "3", "text": "Enable active learning." },
      { "id": "4", "text": "Enable log collection by using Log Analytics." },
      { "id": "5", "text": "Train and republish the language understanding model." },
      { "id": "6", "text": "Migrate authoring to an Azure resource authoring key." }
    ],
    "correctAnswer": ["3", "1", "5"],
    "explanation": "To improve the model's effectiveness, first enable active learning (3) to collect useful utterances, then validate and refine the model using logged utterances (1), and finally retrain and republish the model (5)."
  },
  {
    "id": "q5",
    "type": "boolean",
    "question": [
      "You develop an application to identify species of plants by training a Custom Vision model.",
      "You receive images of new plant species.",
      "You need to add the new images to the classifier.",
      "Solution: You add the new images, and then use the Smart Labeler tool.",
      "Does this meet the goal?"
    ],
    "correctAnswer": "False",
    "explanation": "❌ No. The Smart Labeler helps in suggesting tags for existing classes but does not automatically train or classify new species. Additional manual labeling and retraining is required."
  },
  {
    "id": "q6",
    "type": "boolean",
    "question": [
      "You develop an application to identify species of plants by training a Custom Vision model.",
      "You receive images of new plant species.",
      "You need to add the new images to the classifier.",
      "Solution: You create a new model, and then upload the new images and labels.",
      "Does this meet the goal?"
    ],
    "correctAnswer": "False",
    "explanation": "❌ No. Creating a new model is unnecessary in this case. You can update the existing classifier by adding new images and labels, then retraining it. Creating a new model would not retain the knowledge from the existing trained data."
  },
  {
    "id": "q7",
    "type": "boolean",
    "question": [
      "You develop an application to identify species of plants by training a Custom Vision model.",
      "You receive images of new plant species.",
      "You need to add the new images to the classifier.",
      "Solution: You add the new images and labels to the existing model.",
      "You retrain the model, and then publish the model.",
      "Does this meet the goal?"
    ],
    "correctAnswer": "True",
    "explanation": "✅ Yes. This is the correct approach — updating the existing model with new labeled data, retraining, and publishing ensures continuity and improved accuracy for new classes."
  },
  {
    "id": "q8",
    "type": "drag-sequence",
    "question": [
      "You are building an application to identify defects in factory-produced components that are unique to your business.",
      "To achieve this, you need to use the Custom Vision API for fault detection.",
      "Which three steps should you follow in order?"
    ],
    "options": [
      { "id": "1", "text": "Train the classifier model" },
      { "id": "2", "text": "Upload and tag images" },
      { "id": "3", "text": "Initialize the training dataset" },
      { "id": "4", "text": "Train the object detection model" },
      { "id": "5", "text": "Create a project" }
    ],
    "correctAnswer": ["5", "2", "4"],
    "explanation": "To use Custom Vision for defect detection, you first create a project, then upload and tag images with appropriate labels. After that, you train the object detection model using the tagged data."
  },
  {
    "id": "q9",
    "type": "drag-sequence",
    "question": [
      "You are tasked with helping the Finance-Analysts team build an automated document extraction system using Azure AI Document Intelligence.",
      "The team needs to design a custom model that can extract structured data from various financial forms.",
      "To achieve this, they will use the built-in labeling tool in Document Intelligence Studio.",
      "Which three steps should the Finance-Analysts team follow to build and train their custom model? Select the steps in the correct sequence."
    ],
    "options": [
      { "id": "1", "text": "Label key fields in the uploaded documents" },
      { "id": "2", "text": "Create a pre-trained model for extracting data" },
      { "id": "3", "text": "Create a combined model for multiple document types" },
      { "id": "4", "text": "Train the custom extraction model" },
      { "id": "5", "text": "Create a new project and upload sample documents" }
    ],
    "correctAnswer": ["5", "1", "4"],
    "explanation": "To build a custom model with Azure AI Document Intelligence, first create a project and upload documents. Then, label the key fields using the labeling tool. Finally, train the custom model based on the labeled data. Pre-trained and combined models are optional and not required in this core flow."
  },
  {
    "id": "q10",
    "type": "drag-sequence",
    "question": [
      "You have a Language Understanding solution that you want to deploy using a Docker container on your host computer.",
      "Which three actions should you perform in sequence to prepare and run the solution on a host computer?",
      "Select the correct options from below:"
    ],
    "options": [
      { "id": "1", "text": "From the Language Understanding portal, export the solution as a package file." },
      { "id": "2", "text": "From the host computer, move the package file to the Docker input directory." },
      { "id": "3", "text": "From the host computer, build the container and specify the output directory." },
      { "id": "4", "text": "From the Language Understanding portal, retrain the model." },
      { "id": "5", "text": "From the host computer, run the container and specify the input directory." }
    ],
    "correctAnswer": ["1", "2", "5"],
    "explanation": "To deploy a Language Understanding (LUIS) solution in a Docker container, you first export the model as a package from the Language Understanding portal. Next, move the package file to the Docker container's input directory. Finally, run the container and point it to the input directory to start serving the model."
  },
  {
    "id": "q11",
    "type": "dropdown-pair",
    "question": [
      "You are developing an application that needs to automatically route incoming customer email to either the English or Spanish support teams based on the language of the message.",
      "Which Azure Cognitive Services API should you use?",
      "To answer, select two appropriate options."
    ],
    "leftOptions": [
      { "id": "1", "text": "https://centralus.api.cognitive.micro..." },
      { "id": "2", "text": "https://eu.api.cognitive.microsofttra..." },
      { "id": "3", "text": "https://portal.azure.com" }
    ],
    "rightOptions": [
      { "id": "1", "text": "/translator/text/v3.0/translate?to=es" },
      { "id": "2", "text": "/text/analytics/v3.1/languages" },
      { "id": "3", "text": "/translator/text/v3.0/translate?to=en" }
    ],
    "correctAnswer": { "left": "1", "right": "2" },
    "explanation": "To route based on language, use the Language Detection endpoint from the Text Analytics API. The base endpoint (centralus cognitive) and the path '/text/analytics/v3.1/languages' are appropriate for identifying the language of incoming text."
  },
  {
    "id": "q12",
    "type": "drag-sequence",
    "question": [
      "You have a Custom Vision service project that uses the General domain for classification and contains a trained model.",
      "You need to export the model for use on a network that is disconnected from the internet.",
      "Which three actions should you perform in sequence?"
    ],
    "options": [
      { "id": "A", "text": "Optimize model for edge deployment" },
      { "id": "B", "text": "Retrain the model" },
      { "id": "C", "text": "Export the model" },
      { "id": "D", "text": "Change the classification type" },
      { "id": "E", "text": "Create a new classification model" },
      { "id": "F", "text": "Change Domains to General (compact)" }
    ],
    "correctAnswer": ["F", "B", "C"],
    "explanation": "To export the model for offline use, you need to change the domain to General (compact) (F), retrain the model (B), and then export it (C)."
  },
  {
    "id": "q13",
    "type": "drag-sequence",
    "question": [
      "You've trained an Azure Custom Vision model to classify clothing products using the Retail domain.",
      "To integrate this model into your iOS app, what steps should you perform in sequence?"
    ],
    "options": [
      { "id": "A", "text": "Adjust the model to use a different domain." },
      { "id": "B", "text": "Train the model again after changing the domain." },
      { "id": "C", "text": "Export the model for deployment." },
      { "id": "D", "text": "Delete the training data." },
      { "id": "E", "text": "Test the model." }
    ],
    "correctAnswer": ["A", "B", "C"],
    "explanation": "To export a Custom Vision model for mobile use, you need to first switch to a compact domain (A), retrain the model (B), and then export it for deployment (C)."
  },
  {
    "id": "q14",
    "type": "single",
    "question": [
      "You have an app that analyzes images by using the Computer Vision API.",
      "You need to configure the app to provide an output for users who are vision impaired.",
      "The solution must provide the output in complete sentences.",
      "Which API call should you perform?"
    ],
    "options": [
      { "id": "a", "text": "readInStreamAsync" },
      { "id": "b", "text": "analyzeImagesByDomainInStreamAsync" },
      { "id": "c", "text": "tagImageInStreamAsync" },
      { "id": "d", "text": "describeImageInStreamAsync" }
    ],
    "correctAnswer": "d",
    "explanation": "The correct method is 'describeImageInStreamAsync', which returns a human-readable description of the image in complete sentences — suitable for assisting users with visual impairments."
  },
  {
    "id": "q15",
    "type": "drag-sequence",
    "question": [
      "Your company's product recognition mobile app uses a Custom Vision model.",
      "You've acquired 1,000 unlabeled product images.",
      "You are asked to improve the model's accuracy.",
      "Time is crucial, and you need to retrain the model quickly.",
      "In the Custom Vision portal, which three steps should you take to efficiently retrain the model with the new images?"
    ],
    "options": [
      { "id": "1", "text": "Upload the images by category." },
      { "id": "2", "text": "Get suggested tags." },
      { "id": "3", "text": "Upload all the images." },
      { "id": "4", "text": "Group the images locally into category folders." },
      { "id": "5", "text": "Review the suggestions and confirm the tags." },
      { "id": "6", "text": "Tag the images manually." }
    ],
    "correctAnswer": ["3", "2", "5"],
    "explanation": "To retrain the model quickly, upload all the images (3), use the Smart Labeler to get suggested tags (2), and then review and confirm those suggestions (5). This avoids time-consuming manual or local pre-organization steps."
  },
  {
    "id": "q16",
    "type": "drag-sequence",
    "question": [
      "You have a Custom Vision resource each in dev and prod environment.",
      "You've trained an image classification model named MyTestModel in MyProject on dev.",
      "To deploy MyTestModel to prod, which three actions should you perform in sequence?"
    ],
    "options": [
      { "id": "1", "text": "In dev use the Export Project endpoint." },
      { "id": "2", "text": "In dev use the Get Projects endpoint." },
      { "id": "3", "text": "In prod use the Import Project endpoint." },
      { "id": "4", "text": "In dev use the ExportIteration endpoint." },
      { "id": "5", "text": "In dev use the GetIterations endpoint." },
      { "id": "6", "text": "In prod use the UpdateProject endpoint." }
    ],
    "correctAnswer": ["2", "1", "3"],
    "explanation": "To deploy the model from dev to prod, first get the list of projects in dev (2), then export the desired project (1), and finally import it into prod (3)."
  },
  {
    "id": "q17",
    "type": "dropdown-pair",
    "question": [
      "You have a form recognition task.",
      "Which prebuilt model is best suited to extract key-value pairs from invoices?"
    ],
    "leftOptions": [
      { "id": "1", "text": "prebuilt-receipt" },
      { "id": "2", "text": "prebuilt-invoice" },
      { "id": "3", "text": "prebuilt-idDocument" }
    ],
    "rightOptions": [
      { "id": "a", "text": "Receipts" },
      { "id": "b", "text": "Invoices" },
      { "id": "c", "text": "ID Documents" }
    ],
    "correctAnswer": { "left": "2", "right": "b" },
    "explanation": "The prebuilt-invoice model is designed to extract key-value pairs from invoices."
  },
  {
    "id": "q18",
    "type": "single",
    "question": [
      "You want to analyze customer feedback by extracting key phrases using Azure Text Analytics.",
      "You need to make the analysis more accurate by filtering out common stop words.",
      "Which Text Analytics feature should you enable?"
    ],
    "options": [
      { "id": "a", "text": "Language detection" },
      { "id": "b", "text": "Sentiment analysis" },
      { "id": "c", "text": "Key phrase extraction" },
      { "id": "d", "text": "Stop words filtering" }
    ],
    "correctAnswer": "d",
    "explanation": "Enabling stop words filtering improves key phrase extraction by excluding common words that add no semantic value."
  },
  {
    "id": "q19",
    "type": "single",
    "question": [
      "You want to implement Named Entity Recognition (NER) in your app to identify person names, organizations, and locations in user text.",
      "Which Azure Cognitive Service endpoint should you call?"
    ],
    "options": [
      { "id": "a", "text": "/text/analytics/v3.1/entities/recognition/general" },
      { "id": "b", "text": "/vision/v3.2/analyze" },
      { "id": "c", "text": "/customvision/v3.0/Prediction" },
      { "id": "d", "text": "/translator/text/v3.0/translate" }
    ],
    "correctAnswer": "a",
    "explanation": "The Named Entity Recognition endpoint is part of Text Analytics and is accessed via the entities recognition API."
  },
  {
    "id": "q20",
    "type": "drag-sequence",
    "question": [
      "You need to create an Azure AI Text Analytics pipeline that takes customer emails,",
      "detects language, performs sentiment analysis, and extracts key phrases.",
      "In what order should the tasks be performed?"
    ],
    "options": [
      { "id": "1", "text": "Detect language" },
      { "id": "2", "text": "Extract key phrases" },
      { "id": "3", "text": "Perform sentiment analysis" },
      { "id": "4", "text": "Normalize text" }
    ],
    "correctAnswer": ["1", "4", "3", "2"],
    "explanation": "The typical processing flow is to detect language first, normalize text for consistent processing, then perform sentiment analysis, and finally extract key phrases."
  },
  {
    "id": "q21",
    "type": "boolean",
    "question": [
      "You are using Azure Form Recognizer prebuilt receipt model to extract data from scanned receipts.",
      "The model does not recognize a new field called 'Tax ID'.",
      "Solution: You create a custom model using labeled data to extract the Tax ID field.",
      "Does this meet the goal?"
    ],
    "correctAnswer": "True",
    "explanation": "✅ Custom models trained on labeled data can extract fields not recognized by the prebuilt receipt model."
  },
  {
    "id": "q22",
    "type": "drag-sequence",
    "question": [
      "You are tasked with creating a multi-lingual chatbot using Azure Conversational Language Understanding.",
      "Which three steps should you perform in order?"
    ],
    "options": [
      { "id": "1", "text": "Create the language understanding app for the primary language" },
      { "id": "2", "text": "Train and publish the app" },
      { "id": "3", "text": "Clone the app for additional languages" },
      { "id": "4", "text": "Enable multi-turn conversation" },
      { "id": "5", "text": "Integrate the chatbot with channels" }
    ],
    "correctAnswer": ["1", "2", "3"],
    "explanation": "Create the app in the primary language, train and publish it, then clone it to add support for other languages."
  },
  {
    "id": "q23",
    "type": "dropdown-pair",
    "question": [
      "You want to implement a sentiment analysis service with custom categories to classify product reviews.",
      "Which service and endpoint combination will help you achieve this?"
    ],
    "leftOptions": [
      { "id": "1", "text": "Azure Text Analytics" },
      { "id": "2", "text": "Azure Language Understanding (LUIS)" },
      { "id": "3", "text": "Azure Custom Vision" }
    ],
    "rightOptions": [
      { "id": "1", "text": "/sentiment/v3.1/custom" },
      { "id": "2", "text": "/language/:analyze-text" },
      { "id": "3", "text": "/vision/analyze" }
    ],
    "correctAnswer": { "left": "1", "right": "1" },
    "explanation": "Azure Text Analytics custom sentiment endpoint supports custom categories for sentiment classification."
  },
  {
    "id": "q24",
    "type": "boolean",
    "question": [
      "You want to detect whether an image contains adult or racy content using Azure Computer Vision.",
      "You configure the Analyze Image API with the 'Adult' visual feature enabled.",
      "Solution: You send images to the API and check the 'isAdultContent' property in the response.",
      "Does this meet the goal?"
    ],
    "correctAnswer": "True",
    "explanation": "✅ The 'isAdultContent' boolean property indicates the presence of adult or racy content in the image."
  },
  {
    "id": "q25",
    "type": "single",
    "question": [
      "You have a Custom Speech service in Azure.",
      "You want to improve the accuracy of the speech-to-text model for industry-specific terminology.",
      "Which method should you use?"
    ],
    "options": [
      { "id": "a", "text": "Use the prebuilt model without changes" },
      { "id": "b", "text": "Create and train a custom speech model with phrase lists" },
      { "id": "c", "text": "Use Custom Vision service" },
      { "id": "d", "text": "Use Language Understanding (LUIS)" }
    ],
    "correctAnswer": "b",
    "explanation": "Training a custom speech model with phrase lists improves recognition for domain-specific words."
  },
  {
    "id": "q26",
    "type": "drag-sequence",
    "question": [
      "You want to deploy an Azure Text Analytics solution that complies with data residency regulations and processes data locally.",
      "Which three steps should you perform?"
    ],
    "options": [
      { "id": "1", "text": "Provision an Azure Text Analytics resource with Private Endpoint." },
      { "id": "2", "text": "Deploy Text Analytics container on-premises." },
      { "id": "3", "text": "Export the model from Azure portal." },
      { "id": "4", "text": "Run the container locally and configure local data storage." },
      { "id": "5", "text": "Send data to the public Azure Text Analytics endpoint." }
    ],
    "correctAnswer": ["1", "2", "4"],
    "explanation": "Provision resource with Private Endpoint to ensure network isolation, deploy container on-premises, and run it locally with proper data storage configuration to comply with data residency."
  },
  {
    "id": "q27",
    "type": "single",
    "question": [
      "You want to analyze video content to detect objects and activities using Azure Video Analyzer.",
      "Which component of the Azure Video Analyzer solution is responsible for extracting insights from video streams?"
    ],
    "options": [
      { "id": "a", "text": "Edge modules" },
      { "id": "b", "text": "Video Analyzer service" },
      { "id": "c", "text": "Stream processor" },
      { "id": "d", "text": "Video Indexer" }
    ],
    "correctAnswer": "c",
    "explanation": "The Stream Processor is responsible for processing video streams and extracting insights such as objects and activities."
  },
  {
    "id": "q28",
    "type": "drag-sequence",
    "question": [
      "You want to create a pipeline for document processing using Azure AI Document Intelligence.",
      "Which three steps should you perform in order?"
    ],
    "options": [
      { "id": "1", "text": "Upload the documents to a Blob Storage container" },
      { "id": "2", "text": "Create a Document Intelligence project" },
      { "id": "3", "text": "Label the documents with key-value pairs" },
      { "id": "4", "text": "Train the custom model" },
      { "id": "5", "text": "Deploy the model" }
    ],
    "correctAnswer": ["1", "2", "3"],
    "explanation": "You first upload the documents, then create a project, and label the documents before training and deploying the model."
  },{
  "id": "q29",
  "type": "single",
  "question": "You are developing an application using Azure AI Services which will be used to:\n• Analyze images\n• Detect objects, faces, and text in images\n\nThe solution must ensure:\n• Minimum latency\n• Optimized performance\n\nWhich Azure AI Service should you use?",
  "options": [
    {
      "id": "a",
      "text": "Azure AI Vision"
    },
    {
      "id": "b",
      "text": "Azure AI Face"
    },
    {
      "id": "c",
      "text": "Azure AI Document Intelligence"
    }
  ],
  "correctAnswer": "a",
  "explanation": "✅ The Azure AI Vision service supports analyzing images for objects, faces, and text. It provides comprehensive image analysis capabilities in one service, making it optimal for scenarios requiring high performance and low latency."
},{
  "id": "q30",
  "type": "single",
  "question": "E-commerce company, GreenCycle, wants to enhance its product pages by optimizing images for:\n• Various devices\n• Different screen sizes\n\nTheir product team uploads high-resolution images.\nThese images need to be:\n• Resized\n• Cropped automatically for mobile thumbnails\n\nWhich feature of **AI Vision AnalyzeImage** should you use?",
  "options": [
    {
      "id": "a",
      "text": "Detect objects"
    },
    {
      "id": "b",
      "text": "Generate caption"
    },
    {
      "id": "c",
      "text": "Smart crop"
    },
    {
      "id": "d",
      "text": "Tag Visual Features"
    }
  ],
  "correctAnswer": "c",
  "explanation": "✅ Smart crop automatically generates image crops optimized for the region of interest, making it ideal for creating responsive thumbnails for mobile and other devices."
},{
  "id": "q31",
  "type": "single",
  "question": [
    "You are developing an application using Azure AI Services.",
    "Which Azure AI Service should you use to analyze images and detect objects, faces, and text with minimum latency and optimized performance?"
  ],
  "options": [
    { "id": "a", "text": "Azure AI Vision" },
    { "id": "b", "text": "Azure AI Face" },
    { "id": "c", "text": "Azure AI Document Intelligence" }
  ],
  "correctAnswer": "a",
  "explanation": "Azure AI Vision is the best service to analyze images and detect objects, faces, and text with optimized performance."
},

{
  "id": "q32",
  "type": "single",
  "question": [
    "An e-commerce company wants to optimize images for mobile thumbnails.",
    "Which feature of AI Vision AnalyzeImage should you use?"
  ],
  "options": [
    { "id": "a", "text": "Detect objects" },
    { "id": "b", "text": "Generate caption" },
    { "id": "c", "text": "Smart crop" },
    { "id": "d", "text": "Tag Visual Features" }
  ],
  "correctAnswer": "c",
  "explanation": "Smart crop allows images to be resized and cropped optimized for thumbnails."
},{
  "id": "q33",
  "type": "multiple",
  "question": [
    "You are developing a method that uses the Computer Vision client library.",
    "The method will perform optical character recognition (OCR) in images.",
    "During testing, you discover that the call to the GetReadResultAsync method occurs before the read operation is complete.",
    "You need to prevent the GetReadResultAsync method from proceeding until the read operation is complete.",
    "Which two actions should you perform? Each correct answer presents part of the solution.",
    "NOTE: Each correct selection is worth one point.",
    "Code snippet:",
    "public static async Task ReadFileUrl(ComputerVisionClient client, string urlFile)",
    "{",
    "    const int numberOfCharsInOperationId = 36;",
    "    var txtHeaders = await client.ReadAsync(urlFile, language: \"en\");",
    "    string opLocation = txtHeaders.OperationLocation;",
    "    string operationId = opLocation.Substring(opLocation.Length - numberOfCharsInOperationId);",
    "    ReadOperationResult results;",
    "    results = await client.GetReadResultAsync(Guid.Parse(operationId));",
    "    var textUrlFileResults = results.AnalyzeResult.ReadResults;",
    "    foreach (ReadResult page in textUrlFileResults)",
    "    {",
    "        foreach (Line line in page.Lines)",
    "        {",
    "            Console.WriteLine(line.Text);",
    "        }",
    "    }",
    "}"
  ],
  "options": [
    { "id": "A", "text": "Wrap the call to GetReadResultAsync within a loop that contains a delay." },
    { "id": "B", "text": "Add code to verify the results.Status value." },
    { "id": "C", "text": "Add code to verify the status of the txtHeaders.Status value." },
    { "id": "D", "text": "Remove the Guid.Parse(operationId) parameter." }
  ],
  "correctAnswer": ["A", "B"],
  "explanation": "To ensure GetReadResultAsync waits until the read operation is complete, wrap the call in a loop with a delay to poll the status, and check the results.Status to confirm completion before proceeding."
},{
  "id": "q34",
  "type": "single",
  "question": [
    "You're building a corporate website that will showcase meeting recordings using Azure Video Analyzer for Media.",
    "You embed the Player and Insights widgets into your webpage.",
    "The solution must:",
    "• Enable keyword search within the video content.",
    "• Display identified people and their names from the video.",
    "• Show video captions in US English.",
    "How should you configure the URLs for the widgets to meet these requirements?",
    "Cognitive Insights Widget URL:",
    "https://www.videoindexer.ai/embed/ins.... &controls= ..................................",
    "Player Widget URL:",
    "https://www.videoindexer.ai/embed/pla.... &captions= ..........................."
  ],
  "options": [
    { "id": "A", "text": "widgets=people, search, controls=search; showcaptions=true, captions=en-US" },
    { "id": "B", "text": "widgets=false, controls=search; showcaptions=true, captions=en-US" },
    { "id": "C", "text": "widgets=people,keywords, controls=search; showcaptions=true, captions=en-US" }
  ],
  "correctAnswer": "C",
  "explanation": "Option C correctly configures the Insights widget to show people and keywords widgets with search controls, while the Player widget is set to show captions in US English."
},{
  "id": "q35",
  "type": "single",
  "question": [
    "You are building an app that will include one million scanned magazine articles.",
    "Each article will be stored as an image file.",
    "You need to configure the app to extract text from the images.",
    "The solution must minimize development effort.",
    "What should you include in the solution?"
  ],
  "options": [
    { "id": "A", "text": "Computer vision image analysis" },
    { "id": "B", "text": "The read API in Azure AI Vision" },
    { "id": "C", "text": "Document Intelligence" },
    { "id": "D", "text": "Azure AI service for language" }
  ],
  "correctAnswer": "B",
  "explanation": "The Read API in Azure AI Vision is designed for OCR and can efficiently extract text from images, minimizing development effort."
},{
  "id": "q36",
  "type": "multiple",
  "question": [
    "You use the Custom Vision service to build a classifier.",
    "After training is complete, you need to evaluate the classifier.",
    "Which two metrics are available for review?",
    "Each correct answer presents a complete solution.",
    "NOTE: Each correct selection is worth one point."
  ],
  "options": [
    { "id": "A", "text": "recall" },
    { "id": "B", "text": "F-score" },
    { "id": "C", "text": "weighted accuracy" },
    { "id": "D", "text": "precision" },
    { "id": "E", "text": "area under the curve (AUC)" }
  ],
  "correctAnswer": ["A", "D"],
  "explanation": "Custom Vision provides precision and recall as standard evaluation metrics. These help in assessing how well your classifier is performing in terms of relevant and correct predictions."
},
{
  "id": "q37",
  "type": "single",
  "question": [
    "You have created an object detection project named ‘MyDetectionProject’.",
    "You trained the custom vision model and the following is the performance of the model based on training data.",
    "Select the most appropriate choice for the following statement based on the information in the image:",
    "**The percentage of false positives is**"
  ],
  "options": [
    { "id": "a", "text": "0" },
    { "id": "b", "text": "66.7" },
    { "id": "c", "text": "95.8" },
    { "id": "d", "text": "50" },
    { "id": "e", "text": "100" }
  ],
  "correctAnswer": "a",
  "explanation": "✅ The percentage of false positives is 0%, meaning the model did not incorrectly classify any negative example as positive."
},{
  "id": "q38",
  "type": "single",
  "question": [
    "The value for the number of true positives divided by the total number of true positives and false negatives is",
    "[answer choice]%."
  ],
  "options": [
    { "id": "a", "text": "0" },
    { "id": "b", "text": "66.7" },
    { "id": "c", "text": "95.8" },
    { "id": "d", "text": "50" },
    { "id": "e", "text": "100" }
  ],
  "correctAnswer": "b",
  "explanation": "✅ This value represents **recall**. Based on the image's data, the recall (true positives / (true positives + false negatives)) is 66.7%."
},{
  "id": "q39",
  "type": "single",
  "question": [
    "You have an Azure subscription that contains an Azure AI Content Safety resource named CS1.",
    "You plan to build an app that will analyze user-generated documents and identify obscure offensive terms.",
    "You need to create a dictionary that will contain the offensive terms.",
    "The solution must minimize development effort.",
    "What should you use?"
  ],
  "options": [
    { "id": "a", "text": "a text classifier" },
    { "id": "b", "text": "language detection" },
    { "id": "c", "text": "text moderation" },
    { "id": "d", "text": "a blocklist" }
  ],
  "correctAnswer": "d",
  "explanation": "✅ A **blocklist** in Azure AI Content Safety allows you to define custom offensive terms (including obscure ones) with minimal development effort. This is ideal for user-generated content filtering."
},{
  "id": "q40",
  "type": "single",
  "question": [
    "You have an Azure subscription with an Azure OpenAI resource named AI1.",
    "You developed a chatbot using AI1 to generate answers for specific questions.",
    "You need to prevent users from bypassing the built-in safety mechanisms when asking questions.",
    "Which Azure AI Content Safety feature should you implement?"
  ],
  "options": [
    { "id": "a", "text": "Monitor online activity" },
    { "id": "b", "text": "Jailbreak risk detection" },
    { "id": "c", "text": "Moderate text content" },
    { "id": "d", "text": "Protected material text detection" }
  ],
  "correctAnswer": "b",
  "explanation": "✅ Jailbreak risk detection is a feature in Azure AI Content Safety that identifies attempts to circumvent content filters or safety mechanisms, which is essential when deploying chatbots built with Azure OpenAI."
}
,{
  "id": "q41",
  "type": "single",
  "question": [
    "You have an Azure subscription that contains an Azure OpenAI resource named AI1 and an Azure AI Content Safety resource named CS1.",
    "You build a chatbot that uses AI1 to provide generative answers to specific questions and CS1 to check input and output for objectionable content.",
    "You need to optimize the content filter configurations by running tests on sample questions.",
    "Select the most appropriate solution from the options given below."
  ],
  "options": [
    { "id": "a", "text": "From Content Safety Studio, you use the Moderate text content feature to run the tests." },
    { "id": "b", "text": "From Content Safety Studio, you use the Safety metaprompt feature to run the tests." },
    { "id": "c", "text": "From Content Safety Studio, you use the Monitor online activity feature to run the tests." },
    { "id": "d", "text": "From Content Safety Studio, you use the Protected material detection feature to run the tests." }
  ],
  "correctAnswer": "a",
  "explanation": "✅ The 'Moderate text content' feature in Content Safety Studio allows you to test and configure filtering thresholds by submitting sample prompts or completions, ensuring objectionable input/output is flagged correctly."
},{
  "id": "q42",
  "type": "dropdown-pair",
  "question": [
    "You have an Azure subscription that contains an Azure AI Content Safety resource.",
    "You are building a social media app that will enable users to share images.",
    "You need to configure the app to moderate inappropriate content uploaded by the users.",
    "How should you complete the code? To answer, select the appropriate options in the answer area."
  ],
  "leftOptions": [
    { "id": "1", "text": "AnalyzeTextOptions" },
    { "id": "2", "text": "BlocklistClient" },
    { "id": "3", "text": "ContentSafetyClient" },
    { "id": "4", "text": "TextCategoriesAnalysis" }
  ],
  "rightOptions": [
    { "id": "5", "text": "AnalyzeImage(request)" },
    { "id": "6", "text": "client.AnalyzeImage(request)" },
    { "id": "7", "text": "client.AnalyzeText(request)" },
    { "id": "8", "text": "request.AnalyzeImage(client)" }
  ],
  "correctAnswer": {
    "left": "3",
    "right": "6"
  },
  "explanation": "To moderate image content using Azure AI Content Safety, you instantiate the ContentSafetyClient and then call the AnalyzeImage method on the client instance passing the request."
},{
  "id": "q43",
  "type": "multiple",
  "question": [
    "You have an Azure subscription with an Azure AI Content Safety resource named CS1.",
    "You need to send a request to CS1 to detect hateful language in user input.",
    "How should you complete the curl command URL?"
  ],
  "options": [
    { "id": "a", "text": "completions" },
    { "id": "b", "text": "contentsafety" },
    { "id": "c", "text": "healthinsights/" },
    { "id": "d", "text": "language/" },
    { "id": "e", "text": "embeddings" },
    { "id": "f", "text": "text:analyze" },
    { "id": "g", "text": "text/blocklists" }
  ],
  "correctAnswer": ["b", "f"],
  "explanation": "The Azure AI Content Safety endpoint URL must include 'contentsafety' to target the service, and 'text:analyze' is the correct path to analyze text for hateful language detection."
},{
  "id": "q44",
  "type": "single",
  "question": [
    "You have an Azure subscription that contains an Azure AI Content Safety resource named CS1.",
    "You create a test image that contains a square.",
    "You submit the test image to CS1 by using the curl command and the following command-line parameters.",
    "",
    "What should you expect as the output?"
  ],
  "options": [
    { "id": "a", "text": "0" },
    { "id": "b", "text": "0.0" },
    { "id": "c", "text": "7" },
    { "id": "d", "text": "100" }
  ],
  "correctAnswer": "a",
  "explanation": "The output score for the test image containing a simple shape like a square is expected to be 0, indicating no flagged content or zero severity."
},{
  "id": "q45",
  "type": "single",
  "question": [
    "You are developing a custom question answering project in Azure AI Service for Language.",
    "The project will be used by a chatbot to provide answers to user queries.",
    "You need to configure the project to support multi-turn conversations,",
    "where the chatbot can ask follow-up questions or clarify user intent.",
    "What should you do to enable this functionality?"
  ],
  "options": [
    { "id": "a", "text": "Add follow-up prompts." },
    { "id": "b", "text": "Enable active learning." },
    { "id": "c", "text": "Add alternate questions." },
    { "id": "d", "text": "Enable chit-chat." }
  ],
  "correctAnswer": "a",
  "explanation": "Adding follow-up prompts allows the chatbot to engage in multi-turn conversations by asking clarifying questions or guiding the user for additional input."
},{
  "id": "q46",
  "type": "single",
  "question": [
    "You're developing a contact management chatbot using Azure AI Language Understanding.",
    "The FindContact intent helps users search for contacts by location.",
    "Your manager provides you with the following list of phrases to use for training the model:",
    "\"Find contacts in North Virginia.\", \"Who do I know in Sydney?\", \"Search for contacts in Melbourne.\"",
    "You need to implement these phrases in Language Understanding to improve the model.",
    "What should you do?"
  ],
  "options": [
    { "id": "a", "text": "You create a new utterance for each phrase in the FindContact intent" },
    { "id": "b", "text": "You create a new intent for location." },
    { "id": "c", "text": "You create a new entity for the domain." },
    { "id": "d", "text": "You create a new pattern in the FindContact intent." }
  ],
  "correctAnswer": "a",
  "explanation": "To improve the model, you add new utterances (example phrases) under the existing FindContact intent. This helps the model better recognize user requests related to finding contacts by location."
},{
  "id": "q47",
  "type": "multiple",
  "question": [
    "You have trained a Conversational Language Understanding (CLU) model to interpret natural language input from users.",
    "Before deploying the model, you need to evaluate its accuracy.",
    "What are two methods you can use to evaluate the model's performance?"
  ],
  "options": [
    { "id": "a", "text": "From the language authoring REST endpoint, retrieve the model evaluation summary." },
    { "id": "b", "text": "From Language Studio, enable Active Learning, and then validate the utterances logged for review." },
    { "id": "c", "text": "From Language Studio, select Model performance." },
    { "id": "d", "text": "From the Azure portal, enable log collection in Log Analytics, and then analyze the logs." }
  ],
  "correctAnswer": ["a", "c"],
  "explanation": "You can evaluate the model by retrieving the evaluation summary via the REST endpoint and by viewing Model performance metrics directly in Language Studio."
},{
  "id": "q48",
  "type": "single",
  "question": [
    "You have an Azure AI Language model named IntentModel that identifies the intent of text input.",
    "You are developing a Python application named ChatApp and need to configure it to use IntentModel.",
    "Which Python package should you add to ChatApp to integrate with the Azure AI Language service?"
  ],
  "options": [
    { "id": "a", "text": "azure-ai-language-conversations" },
    { "id": "b", "text": "azure-cognitiveservices-language-textanalytics" },
    { "id": "c", "text": "azure.cognitiveservices.speech" },
    { "id": "d", "text": "azure-ai-documentintelligence" }
  ],
  "correctAnswer": "a",
  "explanation": "The azure-ai-language-conversations package is used to integrate Azure AI Language models that handle conversational and intent recognition tasks."
},{
  "id": "q49",
  "type": "single",
  "question": [
    "You are building a conversational language understanding model.",
    "You need to enable active learning.",
    "What should you do?"
  ],
  "options": [
    { "id": "a", "text": "Add show-all-intents=true to the prediction endpoint query." },
    { "id": "b", "text": "Enable speech priming." },
    { "id": "c", "text": "Add log=true to the prediction endpoint query." },
    { "id": "d", "text": "Enable sentiment analysis." }
  ],
  "correctAnswer": "c",
  "explanation": "Adding log=true to the prediction endpoint query enables active learning by logging utterances for review and improvement."
},{
  "id": "q50",
  "type": "single",
  "question": [
    "You develop a Conversational Language Understanding model by using Language Studio.",
    "During testing, users receive incorrect responses to requests that do NOT relate to the capabilities of the model.",
    "You need to ensure that the model identifies spurious (not genuine or authentic: false) requests.",
    "How should you solve this?"
  ],
  "options": [
    { "id": "a", "text": "Enable active learning." },
    { "id": "b", "text": "Add entities." },
    { "id": "c", "text": "Add examples to the None intent." },
    { "id": "d", "text": "Add examples to the custom intents." }
  ],
  "correctAnswer": "c",
  "explanation": "Adding examples to the None intent helps the model recognize requests that don't belong to any defined intent, thus identifying spurious inputs."
},{
  "id": "q51",
  "type": "single",
  "question": [
    "You are working on a project to develop a language model using Azure's AI Language Understanding (classic) service.",
    "You need to allow additional team members to contribute to the development and training of the model.",
    "What should you use to grant access to the authoring resources for your Language Understanding (classic) service?"
  ],
  "options": [
    { "id": "a", "text": "The Access control (IAM) page for the authoring resources in the Azure portal" },
    { "id": "b", "text": "Access Policy in Azure subscription management page in the Azure portal." },
    { "id": "c", "text": "The Access control (IAM) page for the prediction resources in the Azure portal" },
    { "id": "d", "text": "The Microsoft Active Directory Portal." }
  ],
  "correctAnswer": "a",
  "explanation": "To grant team members access to authoring resources for the Language Understanding (classic) service, use the Access control (IAM) page specifically for the authoring resources in the Azure portal."
},{
  "id": "q52",
  "type": "single",
  "question": [
    "You are building a Conversational Language Understanding (CLU) model using the Language Services portal.",
    "After exporting the model as a JSON file, you analyze the following sample:",
    "{",
    "  \"text\": \"average amount of rain by month at chicago last year\",",
    "  \"intent\": \"Weather.CheckWeatherValue\",",
    "  \"entities\": [",
    "    {",
    "      \"entity\": \"Weather.WeatherRange\",",
    "      \"startPos\": 0,",
    "      \"endPos\": 6,",
    "      \"children\": []",
    "    },",
    "    {",
    "      \"entity\": \"Weather.WeatherCondition\",",
    "      \"startPos\": 18,",
    "      \"endPos\": 21,",
    "      \"children\": []",
    "    },",
    "    {",
    "      \"entity\": \"Weather.Historic\",",
    "      \"startPos\": 23,",
    "      \"endPos\": 30,",
    "      \"children\": []",
    "    }",
    "  ]",
    "}",
    "In the utterance \"average amount of rain by month at chicago last year\", what does the Weather.Historic entity correspond to?"
  ],
  "options": [
    { "id": "a", "text": "by month" },
    { "id": "b", "text": "chicago" },
    { "id": "c", "text": "rain" },
    { "id": "d", "text": "location" }
  ],
  "correctAnswer": "a",
  "explanation": "The Weather.Historic entity corresponds to the phrase 'by month' which is located at positions 23 to 30 in the utterance, representing a time or range descriptor."
},{
  "id": "53",
  "type": "matching-pairs",
  "question": [
    "Match the phrases with their entity types."
  ],
  "leftOptions": [
    { "id": "1", "text": "New York" },
    { "id": "2", "text": "email@xyz.com" },
    { "id": "3", "text": "3 audit business" }
  ],
  "rightOptions": [
    { "id": "a", "text": "geographyv2" },
    { "id": "b", "text": "email" },
    { "id": "c", "text": "machinelearned" }
  ],
  "correctAnswer": {
    "1": "a",
    "2": "b",
    "3": "c"
  },
  "explanation": "Match the location, email and phrase with their correct entity types."
},{
  "id": "q54",
  "type": "single",
  "question": [
    "You are developing a Conversational Language Understanding (CLU) model for an e-commerce chatbot.",
    "The chatbot prompts users to provide their billing address, which can be spoken or typed.",
    "Which entity type should you use to capture billing addresses effectively?"
  ],
  "options": [
    { "id": "a", "text": "machine learned" },
    { "id": "b", "text": "Regex" },
    { "id": "c", "text": "geographyV2" },
    { "id": "d", "text": "Pattern.any" },
    { "id": "e", "text": "list" }
  ],
  "correctAnswer": "a",
  "explanation": "Billing addresses are complex and vary greatly in format, so a machine learned entity is best suited to accurately capture this type of information in the CLU model."
},{
  "id": "q55",
  "type": "single",
  "question": [
    "In LUIS (Language Understanding Intelligent Service), which type of entity is designed to capture any sequence of words in a user utterance, providing flexibility to match varying input patterns?"
  ],
  "options": [
    { "id": "a", "text": "Prebuilt Entity" },
    { "id": "b", "text": "Simple Entity" },
    { "id": "c", "text": "List Entity" },
    { "id": "d", "text": "Pattern.any" }
  ],
  "correctAnswer": "d",
  "explanation": "The Pattern.any entity in LUIS is designed to capture any sequence of words, offering flexibility in matching variable input patterns."
},{
  "id": "q56",
  "type": "single",
  "question": [
    "You are building an Azure AI Language Understanding (LUIS) solution.",
    "During development, you notice that many intents include similar utterances containing airport names or airport codes.",
    "You need to minimize the number of utterances required to train the model while ensuring accurate recognition of airport-related information.",
    "Which type of custom entity should you use?"
  ],
  "options": [
    { "id": "a", "text": "List" },
    { "id": "b", "text": "machine-learning" },
    { "id": "c", "text": "Number" },
    { "id": "d", "text": "Regex" }
  ],
  "correctAnswer": "a",
  "explanation": "A List entity is useful for capturing a predefined set of values, such as airport names or codes, minimizing the training utterances needed."
},{
  "id": "q57",
  "type": "single",
  "question": [
    "You are developing a Natural Language Processing model for a customer support chatbot that collects shipping addresses from users.",
    "Users can either type or dictate their shipping address when interacting with the chatbot.",
    "Which entity type should you implement to effectively capture shipping addresses?"
  ],
  "options": [
    { "id": "a", "text": "machine learned" },
    { "id": "b", "text": "Regex" },
    { "id": "c", "text": "geographyV2" },
    { "id": "d", "text": "Pattern.any" },
    { "id": "e", "text": "list" }
  ],
  "correctAnswer": "a",
  "explanation": "A machine-learned entity is best suited for capturing complex inputs like shipping addresses that may vary significantly in format."
},{
  "id": "q58",
  "type": "single",
  "question": [
    "You have a Language service resource that performs the following:",
    "• Sentiment analysis",
    "• Named Entity Recognition (NER)",
    "• Personally Identifiable Information (PII) identification",
    "You need to prevent the resource from persisting input data once the data is analyzed.",
    "Which query parameter in the Language service API should you configure?"
  ],
  "options": [
    { "id": "a", "text": "model-version" },
    { "id": "b", "text": "piiCategories" },
    { "id": "c", "text": "showStats" },
    { "id": "d", "text": "loggingOptOut" }
  ],
  "correctAnswer": "d",
  "explanation": "The 'loggingOptOut' query parameter prevents the Language service from persisting input data, ensuring user data privacy."
},{
  "id": "q59",
  "type": "multiple",
  "question": [
    "You plan to deploy a containerized Text Analytics Sentiment Analysis service on an Azure virtual machine using Docker.",
    "To run the container, you need to:",
    "- Use the latest version of the Text Analytics Sentiment Analysis container.",
    "- Configure the container to use your Cognitive Services endpoint URI: https://TechCorp.cognitiveservices.azure.com",
    "How should you complete the Docker run command? Select 2 options in the correct order."
  ],
  "options": [
    { "id": "a", "text": "mcr.microsoft.com/azure-cognitive-services/text analytics/keyphrase" },
    { "id": "b", "text": "http://TechCorp.blob.core.windows.net" },
    { "id": "c", "text": "mcr.microsoft.com/azure-cognitive-services/textanalytics/sentiment" },
    { "id": "d", "text": "https://TechCorp.cognitiveservices.azure.com" }
  ],
  "correctAnswer": ["c", "d"],
  "explanation": "The correct container image is the latest Text Analytics Sentiment Analysis container 'mcr.microsoft.com/azure-cognitive-services/textanalytics/sentiment'. You also need to configure the 'Billing' environment variable with your Cognitive Services endpoint URI, which is 'https://TechCorp.cognitiveservices.azure.com'."
},{
  "id": "q60",
  "type": "single",
  "question": [
    "You have an Azure subscription that contains an Azure Cognitive Service for Language resource.",
    "You need to identify the URL of the REST interface for the Language service.",
    "Which blade should you use in the Azure portal?"
  ],
  "options": [
    { "id": "a", "text": "Identity" },
    { "id": "b", "text": "Keys and Endpoint" },
    { "id": "c", "text": "Networking" },
    { "id": "d", "text": "Properties" }
  ],
  "correctAnswer": "b",
  "explanation": "The 'Keys and Endpoint' blade in the Azure portal provides the API endpoint URL and the keys required to access the REST interface for the Azure Cognitive Service for Language."
},{
  "id": "q61",
  "type": "single",
  "question": [
    "You are developing a solution that extracts key phrases from text using Azure AI Text Analytics.",
    "You write the following code:",
    "",
    "```csharp",
    "var response = client.ExtractKeyPhrases(\"Our tour of Paris included Eiffel Tower.\");",
    "```",
    "",
    "What are the expected key phrases?"
  ],
  "options": [
    { "id": "a", "text": "Our, Eiffel, Included" },
    { "id": "b", "text": "Included, Paris, Our" },
    { "id": "c", "text": "Tour, Paris, Eiffel Tower" },
    { "id": "d", "text": "Client, Tour, Tower" }
  ],
  "correctAnswer": "c",
  "explanation": "The `ExtractKeyPhrases` API identifies important noun phrases. For the input 'Our tour of Paris included Eiffel Tower', the correct key phrases extracted would be 'Tour', 'Paris', and 'Eiffel Tower'."
},{
  "id": "q62",
  "type": "single",
  "question": [
    "You are planning to create a word cloud based on the reviews of a company's products.",
    "Which Text Analytics REST API endpoint should you use?"
  ],
  "options": [
    { "id": "a", "text": "entities/recognition/general" },
    { "id": "b", "text": "sentiment" },
    { "id": "c", "text": "/entities/recognition/pii" },
    { "id": "d", "text": "keyPhrases" }
  ],
  "correctAnswer": "d",
  "explanation": "To create a word cloud based on reviews, you should extract the key phrases that represent important terms from the text. The 'keyPhrases' endpoint is designed to do this by identifying the main concepts in unstructured text."
},{
  "id": "q63",
  "type": "multiple",
  "question": [
    "To build a multilingual chatbot that sends different responses for positive and negative messages,",
    "which two Language Service APIs should you use?"
  ],
  "options": [
    { "id": "a", "text": "Linked entities from a well-known knowledge base" },
    { "id": "b", "text": "Named Entity Recognition" },
    { "id": "c", "text": "PII recognition" },
    { "id": "d", "text": "Detect Language" },
    { "id": "e", "text": "Sentiment Analysis" }
  ],
  "correctAnswer": ["d", "e"],
  "explanation": "To build a multilingual chatbot, you must first detect the language of the input using the **Detect Language** API. Then, use **Sentiment Analysis** to classify the message as positive or negative, allowing your chatbot to tailor responses accordingly."
},{
  "id": "q64",
  "type": "matching-pairs",
  "question": [
    "Match the requirements with the appropriate Azure AI Language service."
  ],
  "leftOptions": [
    { "id": "1", "text": "Identify messages with phone number in it" },
    { "id": "2", "text": "Extract domain-specific entities" }
  ],
  "rightOptions": [
    { "id": "a", "text": "Custom named entity recognition (NER)" },
    { "id": "b", "text": "Language detection" },
    { "id": "c", "text": "Named Entity Recognition (NER)" },
    { "id": "d", "text": "Personally Identifiable Information (PII) detection" },
    { "id": "e", "text": "Sentiment analysis" }
  ],
  "correctAnswer": {
    "1": "d",
    "2": "a"
  },
  "explanation": "Phone numbers are considered PII and are detected using the Personally Identifiable Information (PII) detection service. Domain-specific entities require Custom Named Entity Recognition (NER)."
},{
  "id": "65",
  "type": "single",
  "question": [
    "You want to analyze public sentiment about your brand on social media using natural language processing.",
    "Which Azure service should you use?"
  ],
  "options": [
    { "id": "a", "text": "Content Moderator" },
    { "id": "b", "text": "Computer Vision" },
    { "id": "c", "text": "Language service" },
    { "id": "d", "text": "Document Intelligence" }
  ],
  "correctAnswer": "c",
  "explanation": "The Azure Language service provides natural language processing capabilities such as sentiment analysis, which is suitable for analyzing public sentiment about your brand."
},{
  "id": "66",
  "type": "single",
  "question": [
    "You have an Azure OpenAI model.",
    "You are building a web app by using the Azure OpenAI SDK.",
    "You need to configure the App to connect to your OpenAI model.",
    "What information must you provide?"
  ],
  "options": [
    { "id": "a", "text": "the endpoint, key, and model name" },
    { "id": "b", "text": "the deployment name, key, and model name" },
    { "id": "c", "text": "the deployment name, endpoint, and key" },
    { "id": "d", "text": "the endpoint, key, and model type" }
  ],
  "correctAnswer": "c",
  "explanation": "To connect to an Azure OpenAI model using the SDK, you need the deployment name (which identifies the specific deployed model), the endpoint URL of your Azure OpenAI resource, and the subscription key for authentication."
},{
  "id": "67",
  "type": "single",
  "question": [
    "You have an Azure OpenAI model.",
    "You are building a web app by using the Azure OpenAI SDK.",
    "You need to configure the App to connect to your OpenAI model.",
    "What information must you provide?"
  ],
  "options": [
    { "id": "a", "text": "the endpoint, key, and model name" },
    { "id": "b", "text": "the deployment name, key, and model name" },
    { "id": "c", "text": "the deployment name, endpoint, and key" },
    { "id": "d", "text": "the endpoint, key, and model type" }
  ],
  "correctAnswer": "c",
  "explanation": "To connect to an Azure OpenAI model using the SDK, you need the deployment name (which identifies the specific deployed model), the endpoint URL of your Azure OpenAI resource, and the subscription key for authentication."
},{
  "id": "68",
  "type": "single",
  "question": [
    "You have an Azure subscription.",
    "You need to build an app that will compare documents for semantic similarity.",
    "The solution must meet the following requirements:",
    "• Return numeric vectors that represent the tokens of each document.",
    "• Minimize development effort.",
    "Which Azure OpenAI model should you use?"
  ],
  "options": [
    { "id": "a", "text": "GPT-3.5" },
    { "id": "b", "text": "GPT-4" },
    { "id": "c", "text": "embeddings" },
    { "id": "d", "text": "DALL-E" }
  ],
  "correctAnswer": "c",
  "explanation": "The embeddings model generates numeric vector representations of text which are suitable for semantic similarity comparisons and require minimal development effort."
},{
  "id": "q69",
  "type": "matching-pairs",
  "question": [
    "Complete the following code snippet by matching each placeholder with the correct value:",
    "```",
    "new ChatCompletionsOptions()  ",
    "{  ",
    "    Messages =  ",
    "    {  ",
    "        new ChatMessage(..............., @\" What are the benefits of creative writing? \")  ",
    "    },    ",
    "    .................... = (float)1.0,    ",
    "    MaxTokens = 800,  ",
    "};",
    "```"
  ],
  "leftOptions": [
    { "id": "1", "text": "ChatMessage role" },
    { "id": "2", "text": "Parameter to increase creativity" }
  ],
  "rightOptions": [
    { "id": "a", "text": "ChatRole.User" },
    { "id": "b", "text": "ChatRole.Assistant" },
    { "id": "c", "text": "Temperature" },
    { "id": "d", "text": "PresencePenalty" }
  ],
  "correctAnswer": {
    "1": "a",
    "2": "c"
  },
  "explanation": "Use ChatRole.User to specify that the message is from the user. The 'Temperature' parameter controls creativity; higher values increase creativity and reduce determinism."
},{
  "id": "q70",
  "type": "multiple",
  "question": [
    "Using Azure OpenAI resource a model is built with the below settings:",
    "• Top probabilities: 0.5",
    "• Temperature: 1",
    "• Max response tokens: 100",
    "",
    "Here is the model response:",
    "```json",
    "{",
    "  \"choices\": [",
    "    {",
    "      \"finish_reason\": \"stop\",",
    "      \"index\": 0,",
    "      \"message\": {",
    "        \"content\": \"The founders of Microsoft are Bill Gates and Paul Allen. They co-founded the company in 1975.\",",
    "        \"role\": \"assistant\"",
    "      }",
    "    }",
    "  ],",
    "  \"model\": \"gpt-3.5-turbo-0301\",",
    "  \"usage\": {",
    "    \"completion_tokens\": 86,",
    "    \"prompt_tokens\": 37,",
    "    \"total_tokens\": 123",
    "  }",
    "}",
    "```",
    "Which among the following statements are true?"
  ],
  "options": [
    { "id": "a", "text": "The model used was text-embedding-ada-002" },
    { "id": "b", "text": "The prompt_tokens value will be included in the calculation of the Max response tokens value." },
    { "id": "c", "text": "The subscription will be charged 86 tokens for the execution of the session." },
    { "id": "d", "text": "The text completion was truncated because the Max response tokens value was exceeded." },
        { "id": "e", "text": "None" }
  ],
  "correctAnswer": ["e"],
  "explanation": "None of the statements are correct. The model used is gpt-3.5-turbo, not text-embedding-ada-002. Prompt tokens are not counted in the max response token limit. Charges are based on total tokens (prompt + completion), not just completion. The completion stopped naturally, not due to token limits."
},{
  "id": "q72",
  "type": "matching-pairs",
  "question": [
    "Complete the Python code snippet to upload company data and ensure the chatbot uses it to answer questions:",
    "import openai",
    "import os",
    "response = openai. .......... .create(",
    "    messages=[{\"role\": \"user\", \"content\": \"What are the different Azure AI services?\"}],",
    "    deployment_id=os.environ.get(\"AOAIDeploymentId\"),",
    "    dataSources=[",
    "        {",
    "            \"type\": \".............\",",
    "            \"parameters\": {",
    "                \"endpoint\": os.environ.get(\"SearchEndpoint\"),",
    "                \"key\": os.environ.get(\"SearchKey\"),",
    "                \"indexName\": os.environ.get(\"SearchIndex\"),",
    "            }",
    "        }",
    "    ]",
    ")",
    "print(response)"
  ],
  "leftOptions": [
    { "id": "1", "text": "openai method to call" },
    { "id": "2", "text": "type value in dataSources" }
  ],
  "rightOptions": [
    { "id": "a", "text": "Chat.completions" },
    { "id": "b", "text": "Completion" },
    { "id": "c", "text": "Embedding" },
    { "id": "d", "text": "AzureAISearch" },
    { "id": "e", "text": "AzureDocumentIntelligence" },
    { "id": "f", "text": "Blobstorage" }
  ],
  "correctAnswer": {
    "1": "a",
    "2": "d"
  },
  "explanation": "The openai method 'Chat.completions' is used to generate chat responses. The 'type' in dataSources should be 'AzureAISearch' to connect with Azure Cognitive Search for enriched data."
},{
  "id": "q73",
  "type": "single",
  "question": [
    "What is the most appropriate Azure role to assign to User1 to ensure they can identify resource endpoints, view available models for deployment, and generate text and images using deployed models in Azure OpenAI Studio, following the principle of least privilege?"
  ],
  "options": [
    { "id": "a", "text": "Cognitive Services OpenAI User" },
    { "id": "b", "text": "Cognitive Services Contributor" },
    { "id": "c", "text": "Cognitive Services Usages Reader" },
    { "id": "d", "text": "Cognitive Services OpenAI Contributor" }
  ],
  "correctAnswer": "a",
  "explanation": "The 'Cognitive Services OpenAI User' role allows users to identify resource endpoints, view available deployed models, and generate text/images with least privilege, while Contributor roles grant broader permissions not required here."
}
,{
  "id": "q74",
  "type": "single",
  "question": [
    "You're fine-tuning an Azure OpenAI model with 500 prompt-completion pairs. What file format should you use to ensure seamless data ingestion and processing by the Azure OpenAI fine-tuning API?"
  ],
  "options": [
    { "id": "a", "text": "TSV" },
    { "id": "b", "text": "JSONL (JSON Lines)" },
    { "id": "c", "text": "JSON" },
    { "id": "d", "text": "CSV" }
  ],
  "correctAnswer": "b",
  "explanation": "Azure OpenAI fine-tuning API expects training data in JSONL format, where each line is a JSON object with 'prompt' and 'completion' fields for seamless processing."
},{
  "id": "q75",
  "type": "multiple",
  "question": [
    "Select the TRUE statements about the following Azure OpenAI chatbot code snippet."
  ],
  "codeSnippet": [
    "import openai",
    "openai.api_key = key",
    "openai.api_base = endpoint",
    "response = openai.ChatCompletion.create(",
    "    engine=deployment_name,",
    "    messages=[",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},",
    "        {\"role\": \"user\", \"content\": \"What is an LLM?\"}",
    "    ]",
    ")",
    "print(response['choices'][0]['message']['content'])"
  ],
  "options": [
    { "id": "a", "text": "The code guarantees that responses will be sourced from the latest, up-to-date AI research." },
    { "id": "b", "text": "Changing \"What is an LLM?\" to \"What is an LLM in the context of AI models?\" will produce a more accurate response." },
    { "id": "c", "text": "The code will only retrieve definitions from trusted AI sources." },
    { "id": "d", "text": "Changing \"You are a helpful assistant.\" to \"You must answer only within the context of AI language models.\" will produce a more relevant response." }
  ],
  "correctAnswer": ["b", "d"],
  "explanation": "The prompt content can improve accuracy and relevance. However, the API response depends on the underlying model and does not guarantee up-to-date or source-verified information."
},
{
  "id": "q76",
  "type": "single",
  "question": [
    "You are developing a solution to process receipt images using the Document Intelligence API.",
    "After uploading the images, the API returns a JSON response with extracted data, including various fields and their confidence scores.",
    "To ensure the accuracy of the extracted information, you need to establish a criterion that triggers a manual review by a designated team if any of the confidence scores for the extracted fields fall below a certain threshold.",
    "Given the JSON structure shown below, which expression should you use to trigger a manual review?",
    "{",
    "  \"documentResults\": [",
    "    {",
    "      \"docType\": \"prebuilt: receipt\",",
    "      \"pageRange\": [],",
    "      \"fields\": {",
    "        \"ReceiptType\": {",
    "          \"type\": \"string\",",
    "          \"valueString\": \"Itemized\",",
    "          \"confidence\": 0.672",
    "        },",
    "        \"MerchantName\": {",
    "          \"type\": \"string\",",
    "          \"valueString\": \"Tailwind\",",
    "          \"text\": \"Tailwind\",",
    "          \"boundingBox\": [],",
    "          \"page\": 1,",
    "          \"confidence\": 0.913,",
    "          \"elements\": []",
    "        }",
    "      }",
    "    }",
    "  ]",
    "}"
  ],
  "options": [
    { "id": "a", "text": "documentResults.fields.ReceiptType.confidence less than 0.5" },
    { "id": "b", "text": "documentResults.fields.ReceiptType.confidence greater than 0.7" },
    { "id": "c", "text": "documentResults.fields.*.confidence less than 0.7" },
    { "id": "d", "text": "documentResults.fields.MerchantName.valueString == \"Tailwind\"" }
  ],
  "correctAnswer": "c",
  "explanation": "To trigger manual review when any extracted field has a confidence score below a threshold, you need to check all fields dynamically. The expression 'documentResults.fields.*.confidence less than 0.7' captures this by evaluating all confidence values in the fields object."
},{
  "id": "q77",
  "type": "matching-pairs",
  "question": [
    "Complete the following code snippet by matching each placeholder with the correct code fragment to call the Azure OpenAI resource and print the response."
  ],
  "codeSnippet": [
    "openai.api_key = key",
    "openai.api_base = endpoint",
    "response = ……………………….( ",
    "    engine=deployment_name,",
    "    prompt=\"What is Azure OpenAI?\"  ",
    ")",
    "Print ………………………"
  ],
  "leftOptions": [
    { "id": "1", "text": "Method to call OpenAI resource" },
    { "id": "2", "text": "Print statement to output response text" }
  ],
  "rightOptions": [
    { "id": "a", "text": "openai.ChatCompletion.create" },
    { "id": "b", "text": "openai.Embedding.create" },
    { "id": "c", "text": "openai.Image.create" },
    { "id": "d", "text": "(response.choices[0].text)" },
    { "id": "e", "text": "(response.text)" },
    { "id": "f", "text": "(response.id)" }
  ],
  "correctAnswer": {
    "1": "a",
    "2": "d"
  },
  "explanation": "To generate a text completion, use openai.ChatCompletion.create. The response's text output is found in response.choices[0].text."
},{
  "id": "q78",
  "type": "matching-pairs",
  "question": [
    "Complete the Azure CLI command below by matching each placeholder with the correct value to deploy an Azure OpenAI resource using a customer-managed key stored in Azure Key Vault:\n\naz cognitiveservices account create \\ \n  -n myopenai \\ \n  -g myresourcegroup \\ \n  --kind ........... \\ \n  --sku S-1 \\ \n  --location westus2 \\ \n  --.............. '{\\ \n    \"keySource\": \"Microsoft.KeyVault\",\\ \n    \"keyVaultProperties\": {\\ \n      \"keyName\": \"mykey\",\\ \n      \"keyVersion\": \"1.0\",\\ \n      \"keyVaultUri\": \"https://mykeyvault.vault.azure.net/\"\\ \n    }\\ \n  }'"
  ],
  "leftOptions": [
    { "id": "1", "text": "Value for --kind" },
    { "id": "2", "text": "Option for enabling customer-managed key encryption" }
  ],
  "rightOptions": [
    { "id": "a", "text": "OpenAI" },
    { "id": "b", "text": "LanguageAuthoring" },
    { "id": "c", "text": "AlServices" },
    { "id": "d", "text": "--api-properties" },
    { "id": "e", "text": "--assign-identity" },
    { "id": "f", "text": "--encryption" }
  ],
  "correctAnswer": {
    "1": "a",
    "2": "f"
  },
  "explanation": "Use '--kind OpenAI' to specify the Azure OpenAI resource type. Use '--encryption' to configure the resource to use customer-managed keys stored in Azure Key Vault for data encryption."
},{
  "id": "q79",
  "type": "multiple",
  "question": [
    "You need to upload speech samples to a Speech Studio project for use in training.",
    "How should you upload the samples?"
  ],
  "options": [
    { "id": "a", "text": "Combine the speech samples into a single audio file in the .wma format and upload the file." },
    { "id": "b", "text": "Upload a .zip file that contains a collection of audio files in the .wav format and a corresponding text transcript file." },
    { "id": "c", "text": "Upload individual audio files in the FLAC format and manually upload a corresponding transcript in Microsoft Word format." },
    { "id": "d", "text": "Upload individual audio files in the .wma format." }
  ],
  "correctAnswer": ["b"],
  "explanation": "To train custom models in Speech Studio, you should upload a .zip file that includes multiple .wav audio files along with a transcript file in the specified format. Other formats like .wma or Word documents are not supported for training."
},{
  "id": "q80",
  "type": "multiple",
  "question": [
    "What should you upload to a voice talent profile for a custom neural voice in a retail self-service check-out system?"
  ],
  "options": [
    {
      "id": "a",
      "text": "A five-minute .wmp or .mp3 file of the voice talent describing the retailer."
    },
    {
      "id": "b",
      "text": "A .zip file that contains 60-second .wav files and the associated transcripts as .txt files."
    },
    {
      "id": "c",
      "text": "A five-minute .flac audio file and the associated transcript as a .txt file."
    },
    {
      "id": "d",
      "text": "A .wav or .mp3 file of the voice talent consenting to the creation of a synthetic version of their voice."
    }
  ],
  "correctAnswer": ["b"],
  "explanation": "For Custom Neural Voice training in Azure, you must upload a .zip file containing 60-second .wav audio clips and corresponding .txt transcript files. This structured dataset is required to train the synthetic voice model accurately."
},{
  "id": "q81",
  "type": "multiple",
  "question": [
    "You are assigned a new job to develop an app that will be used in motor vehicles which make use of the text-to-speech functionality of Azure AI Speech service.",
    "How can you optimize text-to-speech voice quality for the app using SSML?"
  ],
  "options": [
    { "id": "a", "text": "the style attribute of the mstts:express-as element" },
    { "id": "b", "text": "the effect attribute of the voice element" },
    { "id": "c", "text": "Use mstts:ttsembedding element" },
    { "id": "d", "text": "Use volume attribute in prosody element" }
  ],
  "correctAnswer": ["b"],
  "explanation": "To optimize TTS voice quality in Azure AI Speech using SSML, the `effect` attribute of the `<voice>` element can be used to apply voice transformation effects like 'style', 'whispered', or 'robotic'. This allows fine-tuning of voice output for specialized environments like motor vehicle systems."
},{
  "id": "q82",
  "type": "multiple",
  "question": [
    "How would you complete the following SSML script for a text-to-speech app that uses a custom neural voice, ensuring the voice has a calm tone and imitates a young adult female?",
    "Select two most appropriate answers from the options.",
    "",
    "Example snippet:",
    "",
    "<mstts:express-as ...=\"YoungAdultFemale\" ...=\"gentle\">",
    "How can I assist you?",
    "</mstts:express-as>"
  ],
  "options": [
    { "id": "a", "text": "role" },
    { "id": "b", "text": "style" },
    { "id": "c", "text": "styledegree" },
    { "id": "d", "text": "type" },
    { "id": "e", "text": "voice" }
  ],
  "correctAnswer": ["a", "b"],
  "explanation": "In SSML for Azure TTS, `role` is used to define the persona (e.g., YoungAdultFemale), and `style` defines the speaking tone (e.g., gentle). These attributes help create more natural and expressive speech output."
},{
  "id": "q83",
  "type": "matching-pairs",
  "question": [
    "How do you translate an English lecture into French and German using Azure AI services? The function named append_to_transcript_file takes translated text and a language identifier as input parameters. Complete the code snippet with appropriate options.",
    "",
    "```python",
    "speech_key = os.environ['SPEECH_SUBSCRIPTION_KEY']",
    "service_region = os.environ['SPEECH_SERVICE_REGION']",
    "",
    "def translate_speech():",
    "    translation_config = speechsdk.translation.SpeechTranslationConfig(subscription=speech_key, region=service_region)",
    "    translation_config.speech_recognition_language = \"en\"",
    "",
    "    languages = ",
    "    for language in languages:",
    "        translation_config.add_target_language(language)",
    "",
    "    audio_config = speechsdk.audio.AudioConfig(use_default_microphone=True)",
    "    recognizer = speechsdk.translation.",
    "        translation_config=translation_config, audio_config=audio_config)",
    "",
    "    result = recognizer.recognize_once()",
    "",
    "    if result.reason == speechsdk.ResultReason.TranslatedSpeech:",
    "        append_to_transcript_file(result.text, \"en\")",
    "        for language in result.translations:",
    "            append_to_transcript_file(result.translations[language], language)",
    "",
    "translate_speech()",
    "```"
  ],
  "leftOptions": [
    { "id": "1", "text": "Languages list" },
    { "id": "2", "text": "Recognizer initialization" }
  ],
  "rightOptions": [
    { "id": "a", "text": "(['fr', 'de'])" },
    { "id": "b", "text": "(['French', 'German'])" },
    { "id": "c", "text": "(['en-GB'])" },
    { "id": "d", "text": "(['languages'])" },
    { "id": "e", "text": "SpeechTranslator(" },
    { "id": "f", "text": "SpeechSynthesizer(" },
    { "id": "g", "text": "TranslationRecognizer(" },
    { "id": "h", "text": "SpeakerRecognizer(" }
  ],
  "correctAnswer": {
    "1": "a",
    "2": "g"
  },
  "explanation": "The target languages for translation are specified as language codes like ['fr', 'de']. The correct recognizer class to use for speech translation in Azure Speech SDK is TranslationRecognizer."
},{
  "id": "q84",
  "type": "matching-pairs",
  "question": [
    "A leading online education platform, \"LearnGo\", wants to create engaging video courses with personalized narration. They require a customized voice that resonates with their brand identity. The narration should be synthesized from scripted text.",
    "As a developer, design a solution using Azure AI Services to:",
    "1. Create a custom voice for LearnGo's narration.",
    "2. Generate audio narration from scripted text.",
    "",
    "Match the tasks with the appropriate Azure AI service or tool."
  ],
  "leftOptions": [
    { "id": "1", "text": "Create a custom voice for LearnGo's narration" },
    { "id": "2", "text": "Generate audio narration from scripted text" }
  ],
  "rightOptions": [
    { "id": "a", "text": "Microsoft Bot Framework Composer" },
    { "id": "b", "text": "The Azure portal" },
    { "id": "c", "text": "The Language Understanding portal" },
    { "id": "d", "text": "The Speech Studio portal" },
    { "id": "e", "text": "Language Understanding" },
    { "id": "f", "text": "Speaker Recognition" },
    { "id": "g", "text": "Speech-to-text" },
    { "id": "h", "text": "Text-to-speech" }
  ],
  "correctAnswer": {
    "1": "d",
    "2": "h"
  },
  "explanation": "To create a custom neural voice, you use the Speech Studio portal, which provides tools for voice customization and training. To generate narration from scripted text, Text-to-Speech service is used to synthesize audio from text."
},
  {
    "id": "q85",
    "type": "dropdown-pair",
  "question": [
    "You are developing a customer call handling solution which will receive calls from Spanish-speaking and Hindi-speaking callers. The system is supposed to perform the following tasks:",
    "- The inbound voice is to be captured as text",
    "- Replay the received messages in English.",
    "Which Azure AI Services should you use?"
  ],
    "leftOptions": [
    { "id": "a", "text": "Speaker Recognition" },
    { "id": "b", "text": "Speech-to-text" },
    { "id": "c", "text": "Text-to-speech" },
    { "id": "d", "text": "Translator" }
    ],
    "rightOptions": [
    { "id": "e", "text": "Speech-to-text only" },
    { "id": "f", "text": "Speech-to-text and Language" },
    { "id": "g", "text": "Speaker Recognition and Language" },
    { "id": "h", "text": "Text-to-speech and Language" },
    { "id": "i", "text": "Text-to-speech and Translator" }
    ],
    "correctAnswer": { "left": "b", "right": "i" },
     "explanation": "To capture messages from Spanish and Hindi speakers, the system should use Speech-to-text to transcribe the audio and Translator to convert it into English text. To replay the messages in English, use Text-to-speech to synthesize the translated English text back into audio, combined with Translator to ensure language conversion."
  },
  {
  "id": "q86",
  "type": "single",
  "question": [
    "You have an Azure subscription.",
    "You need to build an app that will compare documents for semantic similarity. The solution must meet the following requirements:",
    "• Return numeric vectors that represent the tokens of each document.",
    "• Minimize development effort.",
    "Which Azure OpenAI model should you use?"
  ],
  "options": [
    { "id": "a", "text": "GPT-3.5" },
    { "id": "b", "text": "GPT-4" },
    { "id": "c", "text": "embeddings" },
    { "id": "d", "text": "DALL-E" }
  ],
  "correctAnswer": "c",
  "explanation": "Embeddings models in Azure OpenAI are designed to return numeric vectors that represent the meaning of text. These vectors can then be compared to compute semantic similarity efficiently, making them ideal for document comparison tasks."
},
{
  "id": "q87",
  "type": "multiple",
  "question": [
    "You have an Azure AI Search service that has experienced a steady increase in query volume over the past 6 months.",
    "You notice that some search requests are being throttled.",
    "What possible action can you take to reduce query throttling?"
  ],
  "options": [
    { "id": "a", "text": "You add indexes." },
    { "id": "b", "text": "You enable customer-managed key (CMK) encryption." },
    { "id": "c", "text": "You add replicas." },
    { "id": "d", "text": "You migrate to an AI Search service that uses a higher tier." },
    { "id": "e", "text": "Deploy the application and a private endpoint to a virtual network." }
  ],
  "correctAnswer": ["c", "d"],
  "explanation": "To reduce query throttling in Azure AI Search, you can add **replicas** to handle more queries simultaneously, or **migrate to a higher tier** for increased capacity. Adding indexes, enabling CMK, or deploying to a private endpoint does not directly address throttling."
},
{
  "id": "q88",
  "type": "single",
  "question": [
    "You're developing a contact management chatbot using Azure AI Language Understanding.",
    "The FindContact intent helps users search for contacts by location.",
    "Your manager provides you with the following list of phrases to use for training the model:",
    "- Find contacts in North Virginia.",
    "- Who do I know in Sydney?",
    "- Search for contacts in Melbourne.",
    "You need to implement these phrases in Language Understanding to improve the model.",
    "What should you do?"
  ],
  "options": [
    { "id": "a", "text": "You create a new utterance for each phrase in the FindContact intent" },
    { "id": "b", "text": "You create a new intent for location." },
    { "id": "c", "text": "You create a new entity for the domain." },
    { "id": "d", "text": "You create a new pattern in the FindContact intent" }
  ],
  "correctAnswer": "a",
  "explanation": "To improve a specific intent like FindContact, you should add more training data by creating new utterances. Each phrase represents a variation in how users might phrase the same intent, helping the model generalize better. Location can be captured as an entity within those utterances."
},
{
  "id": "q89",
  "type": "dropdown-pair",
  "question": [
    "Your factory produces cardboard packaging for food products but experiences intermittent internet connectivity.",
    "Each package must contain four product samples, and you need a Custom Vision model to identify defects and verify product count while providing defect locations to an operator.",
    "Which project type and domain should you choose?"
  ],
  "leftOptions": [
    { "id": "classification", "text": "Classification" },
    { "id": "object-detection", "text": "Object Detection" }
  ],
  "rightOptions": [
    { "id": "retail", "text": "Retail" },
    { "id": "general-compact", "text": "General (compact)" },
    { "id": "general", "text": "General" },
    { "id": "manufacturing", "text": "Manufacturing" }
  ],
  "correctAnswer": {
    "left": "object-detection",
    "right": "general-compact"
  },
  "explanation": "To detect individual product samples and locate defects, 'Object Detection' is required. Since the environment has intermittent internet, the 'General (compact)' domain enables edge deployment on devices without needing constant cloud connectivity."
},
{
  "id": "q90",
  "type": "single",
  "question": [
    "You are developing a solution for a retail store that wants to monitor customer behavior in the store to improve sales and customer experience.",
    "The store has multiple cameras installed throughout and wants to gather data on:",
    "• How many people enter the store.",
    "• Which sections of the store get the most foot traffic.",
    "• How long customers spend in different areas.",
    "You need to analyze video feeds from the cameras to gather these insights in real time.",
    "Which Azure AI service feature should you use to achieve this?"
  ],
  "options": [
    { "id": "a", "text": "Speech-to-text in the Azure AI Speech service" },
    { "id": "b", "text": "Object detection in Azure AI Custom Vision" },
    { "id": "c", "text": "Spatial analysis in Azure AI Vision" },
    { "id": "d", "text": "Azure AI Video Indexer" }
  ],
  "correctAnswer": "c",
  "explanation": "Spatial analysis in Azure AI Vision is designed specifically for real-time video analytics in physical environments. It can track people, count entries, measure dwell time, and identify high-traffic areas—making it ideal for retail behavior analysis. The other options are not optimized for real-time spatial or behavioral video analysis."
},{
  "id": "q91",
  "type": "dropdown-pair",
  "question": [
    "You have a library that contains thousands of images.",
    "You need to tag the images as photographs, drawings, or clipart.",
    "Which service endpoint and response property should you use?",
    "NOTE: Each correct selection is worth one point."
  ],
  "leftOptions": [
    { "id": "doc-ocr", "text": "Document intelligence OCR extraction" },
    { "id": "custom-classification", "text": "Custom vision image classification" },
    { "id": "custom-detection", "text": "Custom vision object detection" },
    { "id": "ai-vision", "text": "AI Vision analyze images" }
  ],
  "rightOptions": [
    { "id": "metadata", "text": "Metadata" },
    { "id": "description", "text": "Description" },
    { "id": "tags", "text": "Tags" },
    { "id": "imagetype", "text": "Imagetype" },
    { "id": "objects", "text": "Objects" }
  ],
  "correctAnswer": {
    "left": "ai-vision",
    "right": "imagetype"
  },
  "explanation": "To determine image categories like photograph, drawing, or clipart, use the Azure AI Vision 'analyze image' API. This includes the 'imagetype' field, which provides image classification. Custom Vision is for custom models, while AI Vision offers built-in visual analysis features."
},    {
  "id": "q92",
  "type": "multiple",
  "question": [
    "A customer uses Azure AI Search and plans to enable server-side encryption using customer-managed keys (CMK) stored in Azure Key Vault.",
    "What are three implications of this change?"
  ],
  "options": [
    { "id": "a", "text": "The index size will increase." },
    { "id": "b", "text": "Query times will increase." },
    { "id": "c", "text": "A self-signed X.509 certificate is required." },
    { "id": "d", "text": "The index size will decrease." },
    { "id": "e", "text": "Query times will decrease." },
    { "id": "f", "text": "Azure Key Vault is required." }
  ],
  "correctAnswer": ["b", "c", "f"],
  "explanation": "Enabling customer-managed keys (CMK) in Azure AI Search requires integration with Azure Key Vault (F), and the customer must provide a self-signed X.509 certificate (C). Using CMK can increase query latency (B) due to additional encryption key access overhead. Index size is not directly impacted."
},
{
  "id": "q93",
  "type": "multiple",
  "question": [
    "A customer uses Azure AI Search and plans to enable server-side encryption using customer-managed keys (CMK) stored in Azure Key Vault.",
    "What are three implications of this change?"
  ],
  "options": [
    { "id": "a", "text": "The index size will increase." },
    { "id": "b", "text": "Query times will increase." },
    { "id": "c", "text": "A self-signed X.509 certificate is required." },
    { "id": "d", "text": "The index size will decrease." },
    { "id": "e", "text": "Query times will decrease." },
    { "id": "f", "text": "Azure Key Vault is required." }
  ],
  "correctAnswer": ["a", "b", "f"],
  "explanation": "When enabling server-side encryption with customer-managed keys (CMK) in Azure AI Search, integration with Azure Key Vault is required (F). Query performance may be affected and can lead to increased latency (B). Additionally, encrypted indexes may lead to slightly larger storage footprints (A) due to encryption overhead."
},{
  "id": "q94",
  "type": "drag-sequence",
  "question": [
    "You are building a retail chatbot that uses Custom Question Answering, a feature of Azure AI Language.",
    "You upload an internal support document to train the model. The document contains the following question: \"What is your return policy?\"",
    "Users report that the chatbot returns the default answer when they ask:",
    "\"Can I return a product after purchase?\"",
    "However, the chatbot returns the correct answer when users ask:",
    "\"What is your return policy?\"",
    "Both questions should return the same answer.",
    "You need to increase the accuracy of the chatbot responses.",
    "Which three actions should you perform in sequence?"
  ],
  "options": [
    { "id": "1", "text": "Add a new question and answer (QnA) pair." },
    { "id": "2", "text": "Retrain the model." },
    { "id": "3", "text": "Add additional questions to the document." },
    { "id": "4", "text": "Republish the model." },
    { "id": "5", "text": "Add alternative phrasing to the question and answer (QnA) pair." }
  ],
  "correctAnswer": ["5", "2", "4"],
  "explanation": "First, add alternative phrasing to the QnA pair so the model recognizes varied user questions. Then retrain and republish the model to apply these improvements."
},{
  "id": "q95",
  "type": "drag-sequence",
  "question": [
    "You have a chatbot that uses a custom Question Answering application.",
    "You enable active learning for the knowledge base used by the application.",
    "You need to integrate user input into the model.",
    "Which four actions should you perform in sequence?"
  ],
  "options": [
    { "id": "1", "text": "For the knowledge base, select Show active learning suggestions." },
    { "id": "2", "text": "Create a new model with alternate questions." },
    { "id": "3", "text": "Approve and reject suggestions." },
    { "id": "4", "text": "Save and train the knowledge base." },
    { "id": "5", "text": "Select the properties of the Azure Cognitive Services resource." },
    { "id": "6", "text": "Publish the knowledge base." },
    { "id": "7", "text": "Modify the automation task logic app to run an Azure Resource Manager template that creates the Azure Cognitive Services resource." }
  ],
  "correctAnswer": ["1", "3", "4", "6"],
  "explanation": "To integrate user input using active learning, first show active learning suggestions, then approve or reject them, save and train the knowledge base, and finally publish the updated knowledge base."
},{
  "id": "q96",
  "type": "drag-sequence",
  "question": [
    "Using Azure AI Services you have built a speech model which transcribes physics lectures.",
    "When you put the model for testing it is not able to transcribe scientific terms.",
    "How will you improve the accuracy of the model?",
    "Select the most appropriate five steps which you should perform in sequence to achieve the goal."
  ],
  "options": [
    { "id": "1", "text": "Create a project with inbuilt-scientific term detector." },
    { "id": "2", "text": "Deploy the model." },
    { "id": "3", "text": "Create a speech-to-text model." },
    { "id": "4", "text": "Create a Custom Speech project." },
    { "id": "5", "text": "Upload training datasets." },
    { "id": "6", "text": "Train the model." },
    { "id": "7", "text": "Create a Conversational Language Understanding model." }
  ],
  "correctAnswer": ["4", "3", "5", "6", "2"],
  "explanation": "To improve transcription accuracy for scientific terms, start by creating a Custom Speech project. Then create a speech-to-text model, upload domain-specific training datasets, train the model, and finally deploy it."
},
 {
  "id": "q97",
  "type": "single",
  "question": [
    "You have an application, utilizing a custom Azure AI Document Intelligence model to recognize contract documents.",
    "You now need to support an additional contract format.",
    "Requirements:",
    "Minimize development effort.",
    "Select one option to fulfill the requirement."
  ],
  "options": [
    { "id": "a", "text": "Create a new training set and add the new contract format to the new training set." },
    { "id": "b", "text": "Lower the accuracy threshold of App1." },
    { "id": "c", "text": "Use a pre-trained language model for image recognition." },
    { "id": "d", "text": "Add the additional contract format to the existing training set and retrain the model." },
    { "id": "e", "text": "Create and train a new custom model." }
  ],
  "correctAnswer": "d",
  "explanation": "To support a new contract format with minimal development effort, the best approach is to add examples of the new format to the existing training set and retrain the custom Document Intelligence model. This avoids building a new model from scratch while improving recognition accuracy for the new format."
},
{
  "id": "q98",
  "type": "single",
  "question": [
    "You are developing a solution to process receipt images using the Document Intelligence API.",
    "After uploading the images, the API returns a JSON response with extracted data, including various fields and their confidence scores.",
    "To ensure the accuracy of the extracted information, you need to establish a criterion that triggers a manual review by a designated team if any of the confidence scores for the extracted fields fall below a certain threshold.",
    "Given the JSON structure shown below, which expression should you use to trigger a manual review?",
    "{",
    "  \"documentResults\": [",
    "    {",
    "      \"docType\": \"prebuilt: receipt\",",
    "      \"pageRange\": [],",
    "      \"fields\": {",
    "        \"ReceiptType\": {",
    "          \"type\": \"string\",",
    "          \"valueString\": \"Itemized\",",
    "          \"confidence\": 0.672",
    "        },",
    "        \"MerchantName\": {",
    "          \"type\": \"string\",",
    "          \"valueString\": \"Tailwind\",",
    "          \"text\": \"Tailwind\",",
    "          \"boundingBox\": [],",
    "          \"page\": 1,",
    "          \"confidence\": 0.913,",
    "          \"elements\": []",
    "        }",
    "      }",
    "    }",
    "  ]",
    "}"
  ],
  "options": [
    { "id": "a", "text": "documentResults.fields.ReceiptType.confidence less than 0.5" },
    { "id": "b", "text": "documentResults.fields.ReceiptType.confidence greater than 0.7" },
    { "id": "c", "text": "documentResults.fields.*.confidence less than 0.7" },
    { "id": "d", "text": "documentResults.fields.MerchantName.valueString == \"Tailwind\"" }
  ],
  "correctAnswer": "c",
  "explanation": "To trigger manual review when any extracted field has a confidence score below a threshold, you need to check all fields dynamically. The expression 'documentResults.fields.*.confidence less than 0.7' captures this by evaluating all confidence values in the fields object."
},
{
  "id": "q99",
  "type": "single",
  "question": [
    "Which Azure AI Document Intelligence model is best suited to extract the following data from scanned documents:",
    "",
    "• Billing address",
    "• Due date",
    "• Subtotal",
    "• Shipping address",
    "• Total tax",
    "• Customer ID",
    "• Amount due"
  ],
  "options": [
    { "id": "a", "text": "Contract" },
    { "id": "b", "text": "Invoice" },
    { "id": "c", "text": "General document" },
    { "id": "d", "text": "Custom extraction model" }
  ],
  "correctAnswer": "b",
  "explanation": "The prebuilt 'Invoice' model in Azure AI Document Intelligence is specifically designed to extract common invoice fields such as billing address, due date, subtotal, shipping address, tax, customer ID, and amount due, making it the best fit with minimal development effort."
},{
  "id": "q100",
  "type": "multiple",
  "question": [
    "You are building a solution that students will use to find references for essays.",
    "You use the following code to start building the solution:",
    "",
    "```csharp",
    "using Azure;",
    "using System;",
    "using Azure.AI.TextAnalytics;",
    "",
    "private static readonly AzureKeyCredential credentials = new AzureKeyCredential(\"key\");",
    "private static readonly Uri endpoint = new Uri(\"endpoint\");",
    "",
    "static void EntityLinker (TextAnalyticsClient client)",
    "{",
    "    var response = client.RecognizeLinkedEntities(",
    "        \"Our tour guide took us up the Space Needle during our trip to Seattle last week.\");",
    "}",
    "```",
    "",
    "For each of the following statements, select **Yes** if the statement is true. Otherwise, select **No**."
  ],
  "options": [
    {
      "id": "a",
      "text": "The code will detect the language of documents.",
      "isCorrect": false
    },
    {
      "id": "b",
      "text": "The url attribute returned for each linked entity will be a Bing search link.",
      "isCorrect": false
    },
    {
      "id": "c",
      "text": "The matches attribute returned for each linked entity will provide the location in a document where the entity is referenced.",
      "isCorrect": true
    }
  ],
  "correctAnswer": ["c"],
  "explanation": "The `RecognizeLinkedEntities` method is used to link entities in the text to entries in a knowledge base such as Wikipedia. It does **not** detect language (you'd need a separate call for that), and the `url` is usually a link to a Wikipedia article, **not Bing**. The `matches` attribute identifies text spans where the entity appears, so option C is correct."
},
{
  "id": "q101",
  "type": "matching-pairs",
  "question": [
    "Match the actions with the correct method for accessing Azure OpenAI deployments."
  ],
  "leftOptions": [
    { "id": "1", "text": "Provide access to AI1dev by using" },
    { "id": "2", "text": "Connect to the deployment by using" }
  ],
  "rightOptions": [
    { "id": "a", "text": "A bearer token" },
    { "id": "b", "text": "An API key" },
    { "id": "c", "text": "An AD token" },
    { "id": "d", "text": "A deployment name" },
    { "id": "e", "text": "A deployment endpoint" }
  ],
  "correctAnswer": {
    "1": "b",
    "2": "d"
  },
  "explanation": "To access Azure OpenAI resources, you typically use an API key. Each app should connect using the specific deployment name configured in the Azure OpenAI Studio."
},{
  "id": "q102",
  "type": "single",
  "question": [
    "You are setting up an Azure OpenAI resource for a chatbot application using an Azure Resource Manager (ARM) template.",
    "The chatbot is expected to handle a significant volume of queries, approximately 600 requests per minute.",
    "You need to configure the Azure OpenAI resource to support 600 requests per minute.",
    "How would you adjust the deployment template to meet this requirement?"
  ],
  "options": [
    { "id": "a", "text": "count, 100" },
    { "id": "b", "text": "capacity, 600" },
    { "id": "c", "text": "capacity, 100" },
    { "id": "d", "text": "size, 1" }
  ],
  "correctAnswer": "c",
  "explanation": "In Azure OpenAI ARM templates, the `capacity` field under `sku` defines the number of compute units allocated. Setting `capacity: 100` enables the resource to handle higher throughput, such as 600 requests per minute, depending on the model and request complexity."
},{
  "id": "q103",
  "type": "single",
  "question": [
    "You've integrated Azure OpenAI's GPT-4 model into your application, MyApp.",
    "MyApp occasionally returns responses containing hate speech.",
    "You need to prevent MyApp from generating responses with hate speech.",
    "How do you achieve the goal?"
  ],
  "options": [
    { "id": "a", "text": "the Temperature parameter" },
    { "id": "b", "text": "a content filter" },
    { "id": "c", "text": "abuse monitoring" },
    { "id": "d", "text": "the Frequency penalty parameter" }
  ],
  "correctAnswer": "b",
  "explanation": "To prevent harmful or inappropriate outputs such as hate speech, Azure OpenAI provides built-in content filtering. The content filter analyzes and blocks inappropriate content before it's returned to the user. Parameters like Temperature or Frequency penalty affect creativity and repetition but do not prevent unsafe content."
},{
  "id": "q104",
  "type": "single",
  "question": [
    "You're developing an e-commerce platform that requires automated generation of product titles and descriptions from existing product specifications.",
    "Which Azure AI model is best suited for generating concise and relevant product titles from detailed product descriptions?"
  ],
  "options": [
    { "id": "a", "text": "Embeddings" },
    { "id": "b", "text": "DALL·E" },
    { "id": "c", "text": "Whisper" },
    { "id": "d", "text": "GPT-4" }
  ],
  "correctAnswer": "d",
  "explanation": "GPT-4 is a large language model optimized for natural language generation tasks such as summarization, content generation, and rephrasing. It is best suited for transforming detailed product specs into concise, human-readable titles and descriptions. Embeddings are used for similarity search, DALL·E for image generation, and Whisper for speech recognition."
},{
  "id": "q105",
  "type": "single",
  "question": [
    "You have an Azure subscription and 1000 ASCII files.",
    "You need to identify files that contain specific phrases and the chosen solution must use cosine similarity.",
    "Which Azure OpenAI model should you use?"
  ],
  "options": [
    { "id": "a", "text": "text-embedding-ada-002" },
    { "id": "b", "text": "GPT-4" },
    { "id": "c", "text": "GPT-35 Turbo" },
    { "id": "d", "text": "GPT-4-32k" }
  ],
  "correctAnswer": "a",
  "explanation": "The `text-embedding-ada-002` model is specifically designed for generating vector embeddings that capture the semantic meaning of text. These embeddings can be compared using cosine similarity, making this model ideal for identifying documents with semantically similar content. GPT models are designed for text generation rather than efficient embedding and similarity comparison."
},{
  "id": "q106",
  "type": "single",
  "question": [
    "You're developing a conversational AI using Azure OpenAI's GPT-35 Turbo model.",
    "The System Message you provided is:",
    "\"You're an AI assistant that helps people find information. Answer in as few words as possible.\"",
    "Determine which type of prompt engineering technique this system message represents."
  ],
  "options": [
    { "id": "a", "text": "Affordance" },
    { "id": "b", "text": "Few-shot learning" },
    { "id": "c", "text": "Chain of thought" },
    { "id": "d", "text": "Priming" }
  ],
  "correctAnswer": "d",
  "explanation": "This is an example of **priming**, where the system prompt sets the tone, context, and behavior of the assistant before any user input. Priming helps guide the model’s responses without requiring additional input examples."
}
]
