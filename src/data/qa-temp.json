[
  {
    "id": "q1",
    "type": "drag-sequence",
    "question": [
      "You have an app that manages feedback.",
      "You need to ensure that the app can detect negative comments by using the Sentiment Analysis API in Azure AI Language.",
      "The solution must ensure that the managed feedback remains on your company’s internal network.",
      "Which three actions should you perform in sequence?"
    ],
    "options": [
      { "id": "a", "text": "Provision the Language service resource in Azure" },
      { "id": "b", "text": "Deploy a Docker container on an on-premises server" },
      { "id": "c", "text": "Run the container and query the prediction endpoint" },
      { "id": "d", "text": "Upload data to Azure Blob Storage" }
    ],
    "correctAnswer": ["a", "b", "c"],
    "explanation": "The correct sequence is: Provision the Language service → Deploy the container on-premises → Run the container and query it locally. Uploading to Azure is not required because the solution must keep data on-premises."
  },
  {
    "id": "q2",
    "type": "dropdown-pair",
    "question": [
      "You are building a chatbot.",
      "You need to use the Content Moderator service to identify messages that contain sexually explicit language.",
      "Which section in the response from the service will contain the category score, and which category will be assigned to the message?"
    ],
    "leftOptions": [
      { "id": "classification", "text": "Section-Classification" },
      { "id": "pi", "text": "Section-pi" },
      { "id": "terms", "text": "Section-Terms" }
    ],
    "rightOptions": [
      { "id": "cat1", "text": "Category-1" },
      { "id": "cat2", "text": "Category-2" },
      { "id": "cat3", "text": "Category-3" }
    ],
    "correctAnswer": { "left": "classification", "right": "cat1" },
    "explanation": "The 'Classification' section returns the likelihood of content falling into various categories like Category-1 for sexual content."
  },
  {
    "id": "q3",
    "type": "drag-sequence",
    "question": [
      "You have a question-answering project in Azure AI Language.",
      "You need to migrate the project to a Language service instance in a different Azure region.",
      "What are the three sequential actions you should perform to accomplish this?"
    ],
    "options": [
      { "id": "1", "text": "Train and publish model from the new instance." },
      { "id": "2", "text": "Enable custom entity recognition from the new instance." },
      { "id": "3", "text": "From the original instance, export the existing project." },
      { "id": "4", "text": "From the new instance, import the project file." },
      { "id": "5", "text": "From the new instance, regenerate the keys." }
    ],
    "correctAnswer": ["3", "4", "1"],
    "explanation": "To migrate a question-answering project, first export the project from the original instance (3), then import it into the new instance (4), and finally train and publish the model from the new instance (1)."
  },
  {
    "id": "q4",
    "type": "drag-sequence",
    "question": [
      "You are using a Conversational Language Understanding (CLU) service to handle natural language input from users of a web-based customer agent.",
      "Users report that the agent frequently responds with a generic message: 'Sorry, I don't understand that.'",
      "You need to improve the agent's ability to respond to user requests effectively.",
      "Which three actions should you perform in sequence to achieve this?"
    ],
    "options": [
      { "id": "1", "text": "Validate the utterances logged for review and modify the models." },
      { "id": "2", "text": "Add prebuilt domain models as required." },
      { "id": "3", "text": "Enable active learning." },
      { "id": "4", "text": "Enable log collection by using Log Analytics." },
      { "id": "5", "text": "Train and republish the language understanding model." },
      { "id": "6", "text": "Migrate authoring to an Azure resource authoring key." }
    ],
    "correctAnswer": ["3", "1", "5"],
    "explanation": "To improve the model's effectiveness, first enable active learning (3) to collect useful utterances, then validate and refine the model using logged utterances (1), and finally retrain and republish the model (5)."
  },
  {
    "id": "q5",
    "type": "boolean",
    "question": [
      "You develop an application to identify species of plants by training a Custom Vision model.",
      "You receive images of new plant species.",
      "You need to add the new images to the classifier.",
      "Solution: You add the new images, and then use the Smart Labeler tool.",
      "Does this meet the goal?"
    ],
    "correctAnswer": "False",
    "explanation": "❌ No. The Smart Labeler helps in suggesting tags for existing classes but does not automatically train or classify new species. Additional manual labeling and retraining is required."
  },
  {
    "id": "q6",
    "type": "boolean",
    "question": [
      "You develop an application to identify species of plants by training a Custom Vision model.",
      "You receive images of new plant species.",
      "You need to add the new images to the classifier.",
      "Solution: You create a new model, and then upload the new images and labels.",
      "Does this meet the goal?"
    ],
    "correctAnswer": "False",
    "explanation": "❌ No. Creating a new model is unnecessary in this case. You can update the existing classifier by adding new images and labels, then retraining it. Creating a new model would not retain the knowledge from the existing trained data."
  },
  {
    "id": "q7",
    "type": "boolean",
    "question": [
      "You develop an application to identify species of plants by training a Custom Vision model.",
      "You receive images of new plant species.",
      "You need to add the new images to the classifier.",
      "Solution: You add the new images and labels to the existing model.",
      "You retrain the model, and then publish the model.",
      "Does this meet the goal?"
    ],
    "correctAnswer": "True",
    "explanation": "✅ Yes. This is the correct approach — updating the existing model with new labeled data, retraining, and publishing ensures continuity and improved accuracy for new classes."
  },
  {
    "id": "q8",
    "type": "drag-sequence",
    "question": [
      "You are building an application to identify defects in factory-produced components that are unique to your business.",
      "To achieve this, you need to use the Custom Vision API for fault detection.",
      "Which three steps should you follow in order?"
    ],
    "options": [
      { "id": "1", "text": "Train the classifier model" },
      { "id": "2", "text": "Upload and tag images" },
      { "id": "3", "text": "Initialize the training dataset" },
      { "id": "4", "text": "Train the object detection model" },
      { "id": "5", "text": "Create a project" }
    ],
    "correctAnswer": ["5", "2", "4"],
    "explanation": "To use Custom Vision for defect detection, you first create a project, then upload and tag images with appropriate labels. After that, you train the object detection model using the tagged data."
  },
  {
    "id": "q9",
    "type": "drag-sequence",
    "question": [
      "You are tasked with helping the Finance-Analysts team build an automated document extraction system using Azure AI Document Intelligence.",
      "The team needs to design a custom model that can extract structured data from various financial forms.",
      "To achieve this, they will use the built-in labeling tool in Document Intelligence Studio.",
      "Which three steps should the Finance-Analysts team follow to build and train their custom model? Select the steps in the correct sequence."
    ],
    "options": [
      { "id": "1", "text": "Label key fields in the uploaded documents" },
      { "id": "2", "text": "Create a pre-trained model for extracting data" },
      { "id": "3", "text": "Create a combined model for multiple document types" },
      { "id": "4", "text": "Train the custom extraction model" },
      { "id": "5", "text": "Create a new project and upload sample documents" }
    ],
    "correctAnswer": ["5", "1", "4"],
    "explanation": "To build a custom model with Azure AI Document Intelligence, first create a project and upload documents. Then, label the key fields using the labeling tool. Finally, train the custom model based on the labeled data. Pre-trained and combined models are optional and not required in this core flow."
  },
  {
    "id": "q10",
    "type": "drag-sequence",
    "question": [
      "You have a Language Understanding solution that you want to deploy using a Docker container on your host computer.",
      "Which three actions should you perform in sequence to prepare and run the solution on a host computer?",
      "Select the correct options from below:"
    ],
    "options": [
      { "id": "1", "text": "From the Language Understanding portal, export the solution as a package file." },
      { "id": "2", "text": "From the host computer, move the package file to the Docker input directory." },
      { "id": "3", "text": "From the host computer, build the container and specify the output directory." },
      { "id": "4", "text": "From the Language Understanding portal, retrain the model." },
      { "id": "5", "text": "From the host computer, run the container and specify the input directory." }
    ],
    "correctAnswer": ["1", "2", "5"],
    "explanation": "To deploy a Language Understanding (LUIS) solution in a Docker container, you first export the model as a package from the Language Understanding portal. Next, move the package file to the Docker container's input directory. Finally, run the container and point it to the input directory to start serving the model."
  },
  {
    "id": "q11",
    "type": "dropdown-pair",
    "question": [
      "You are developing an application that needs to automatically route incoming customer email to either the English or Spanish support teams based on the language of the message.",
      "Which Azure Cognitive Services API should you use?",
      "To answer, select two appropriate options."
    ],
    "leftOptions": [
      { "id": "1", "text": "https://centralus.api.cognitive.micro..." },
      { "id": "2", "text": "https://eu.api.cognitive.microsofttra..." },
      { "id": "3", "text": "https://portal.azure.com" }
    ],
    "rightOptions": [
      { "id": "1", "text": "/translator/text/v3.0/translate?to=es" },
      { "id": "2", "text": "/text/analytics/v3.1/languages" },
      { "id": "3", "text": "/translator/text/v3.0/translate?to=en" }
    ],
    "correctAnswer": { "left": "1", "right": "2" },
    "explanation": "To route based on language, use the Language Detection endpoint from the Text Analytics API. The base endpoint (centralus cognitive) and the path '/text/analytics/v3.1/languages' are appropriate for identifying the language of incoming text."
  },
  {
    "id": "q12",
    "type": "drag-sequence",
    "question": [
      "You have a Custom Vision service project that uses the General domain for classification and contains a trained model.",
      "You need to export the model for use on a network that is disconnected from the internet.",
      "Which three actions should you perform in sequence?"
    ],
    "options": [
      { "id": "A", "text": "Optimize model for edge deployment" },
      { "id": "B", "text": "Retrain the model" },
      { "id": "C", "text": "Export the model" },
      { "id": "D", "text": "Change the classification type" },
      { "id": "E", "text": "Create a new classification model" },
      { "id": "F", "text": "Change Domains to General (compact)" }
    ],
    "correctAnswer": ["F", "B", "C"],
    "explanation": "To export the model for offline use, you need to change the domain to General (compact) (F), retrain the model (B), and then export it (C)."
  },
  {
    "id": "q13",
    "type": "drag-sequence",
    "question": [
      "You've trained an Azure Custom Vision model to classify clothing products using the Retail domain.",
      "To integrate this model into your iOS app, what steps should you perform in sequence?"
    ],
    "options": [
      { "id": "A", "text": "Adjust the model to use a different domain." },
      { "id": "B", "text": "Train the model again after changing the domain." },
      { "id": "C", "text": "Export the model for deployment." },
      { "id": "D", "text": "Delete the training data." },
      { "id": "E", "text": "Test the model." }
    ],
    "correctAnswer": ["A", "B", "C"],
    "explanation": "To export a Custom Vision model for mobile use, you need to first switch to a compact domain (A), retrain the model (B), and then export it for deployment (C)."
  },
  {
    "id": "q14",
    "type": "single",
    "question": [
      "You have an app that analyzes images by using the Computer Vision API.",
      "You need to configure the app to provide an output for users who are vision impaired.",
      "The solution must provide the output in complete sentences.",
      "Which API call should you perform?"
    ],
    "options": [
      { "id": "a", "text": "readInStreamAsync" },
      { "id": "b", "text": "analyzeImagesByDomainInStreamAsync" },
      { "id": "c", "text": "tagImageInStreamAsync" },
      { "id": "d", "text": "describeImageInStreamAsync" }
    ],
    "correctAnswer": "d",
    "explanation": "The correct method is 'describeImageInStreamAsync', which returns a human-readable description of the image in complete sentences — suitable for assisting users with visual impairments."
  },
  {
    "id": "q15",
    "type": "drag-sequence",
    "question": [
      "Your company's product recognition mobile app uses a Custom Vision model.",
      "You've acquired 1,000 unlabeled product images.",
      "You are asked to improve the model's accuracy.",
      "Time is crucial, and you need to retrain the model quickly.",
      "In the Custom Vision portal, which three steps should you take to efficiently retrain the model with the new images?"
    ],
    "options": [
      { "id": "1", "text": "Upload the images by category." },
      { "id": "2", "text": "Get suggested tags." },
      { "id": "3", "text": "Upload all the images." },
      { "id": "4", "text": "Group the images locally into category folders." },
      { "id": "5", "text": "Review the suggestions and confirm the tags." },
      { "id": "6", "text": "Tag the images manually." }
    ],
    "correctAnswer": ["3", "2", "5"],
    "explanation": "To retrain the model quickly, upload all the images (3), use the Smart Labeler to get suggested tags (2), and then review and confirm those suggestions (5). This avoids time-consuming manual or local pre-organization steps."
  },
  {
    "id": "q16",
    "type": "drag-sequence",
    "question": [
      "You have a Custom Vision resource each in dev and prod environment.",
      "You've trained an image classification model named MyTestModel in MyProject on dev.",
      "To deploy MyTestModel to prod, which three actions should you perform in sequence?"
    ],
    "options": [
      { "id": "1", "text": "In dev use the Export Project endpoint." },
      { "id": "2", "text": "In dev use the Get Projects endpoint." },
      { "id": "3", "text": "In prod use the Import Project endpoint." },
      { "id": "4", "text": "In dev use the ExportIteration endpoint." },
      { "id": "5", "text": "In dev use the GetIterations endpoint." },
      { "id": "6", "text": "In prod use the UpdateProject endpoint." }
    ],
    "correctAnswer": ["2", "1", "3"],
    "explanation": "To deploy the model from dev to prod, first get the list of projects in dev (2), then export the desired project (1), and finally import it into prod (3)."
  },
  {
    "id": "q17",
    "type": "dropdown-pair",
    "question": [
      "You have a form recognition task.",
      "Which prebuilt model is best suited to extract key-value pairs from invoices?"
    ],
    "leftOptions": [
      { "id": "1", "text": "prebuilt-receipt" },
      { "id": "2", "text": "prebuilt-invoice" },
      { "id": "3", "text": "prebuilt-idDocument" }
    ],
    "rightOptions": [
      { "id": "a", "text": "Receipts" },
      { "id": "b", "text": "Invoices" },
      { "id": "c", "text": "ID Documents" }
    ],
    "correctAnswer": { "left": "2", "right": "b" },
    "explanation": "The prebuilt-invoice model is designed to extract key-value pairs from invoices."
  },
{
  "id": "q18",
  "type": "single",
  "question": [
    "You want to analyze customer feedback by extracting key phrases using Azure Text Analytics.",
    "You need to make the analysis more accurate by filtering out common stop words.",
    "Which Text Analytics feature should you enable?"
  ],
  "options": [
    { "id": "a", "text": "Language detection" },
    { "id": "b", "text": "Sentiment analysis" },
    { "id": "c", "text": "Key phrase extraction" },
    { "id": "d", "text": "Stop words filtering" }
  ],
  "correctAnswer": "c",
  "explanation": "Key phrase extraction is the Text Analytics feature that automatically extracts the most important phrases from text while internally filtering out common stop words such as 'and', 'is', and 'the'."
},
  {
    "id": "q19",
    "type": "single",
    "question": [
      "You want to implement Named Entity Recognition (NER) in your app to identify person names, organizations, and locations in user text.",
      "Which Azure Cognitive Service endpoint should you call?"
    ],
    "options": [
      { "id": "a", "text": "/text/analytics/v3.1/entities/recognition/general" },
      { "id": "b", "text": "/vision/v3.2/analyze" },
      { "id": "c", "text": "/customvision/v3.0/Prediction" },
      { "id": "d", "text": "/translator/text/v3.0/translate" }
    ],
    "correctAnswer": "a",
    "explanation": "The Named Entity Recognition endpoint is part of Text Analytics and is accessed via the entities recognition API."
  },
  {
    "id": "q20",
    "type": "drag-sequence",
    "question": [
      "You need to create an Azure AI Text Analytics pipeline that takes customer emails,",
      "detects language, performs sentiment analysis, and extracts key phrases.",
      "In what order should the tasks be performed?"
    ],
    "options": [
      { "id": "1", "text": "Detect language" },
      { "id": "2", "text": "Extract key phrases" },
      { "id": "3", "text": "Perform sentiment analysis" },
      { "id": "4", "text": "Normalize text" }
    ],
    "correctAnswer": ["1", "4", "3", "2"],
    "explanation": "The typical processing flow is to detect language first, normalize text for consistent processing, then perform sentiment analysis, and finally extract key phrases."
  },
  {
    "id": "q21",
    "type": "boolean",
    "question": [
      "You are using Azure Form Recognizer prebuilt receipt model to extract data from scanned receipts.",
      "The model does not recognize a new field called 'Tax ID'.",
      "Solution: You create a custom model using labeled data to extract the Tax ID field.",
      "Does this meet the goal?"
    ],
    "correctAnswer": "True",
    "explanation": "✅ Custom models trained on labeled data can extract fields not recognized by the prebuilt receipt model."
  },
  {
    "id": "q22",
    "type": "drag-sequence",
    "question": [
      "You are tasked with creating a multi-lingual chatbot using Azure Conversational Language Understanding.",
      "Which three steps should you perform in order?"
    ],
    "options": [
      { "id": "1", "text": "Create the language understanding app for the primary language" },
      { "id": "2", "text": "Train and publish the app" },
      { "id": "3", "text": "Clone the app for additional languages" },
      { "id": "4", "text": "Enable multi-turn conversation" },
      { "id": "5", "text": "Integrate the chatbot with channels" }
    ],
    "correctAnswer": ["1", "2", "3"],
    "explanation": "Create the app in the primary language, train and publish it, then clone it to add support for other languages."
  },
  {
    "id": "q23",
    "type": "dropdown-pair",
    "question": [
      "You want to implement a sentiment analysis service with custom categories to classify product reviews.",
      "Which service and endpoint combination will help you achieve this?"
    ],
    "leftOptions": [
      { "id": "1", "text": "Azure Text Analytics" },
      { "id": "2", "text": "Azure Language Understanding (LUIS)" },
      { "id": "3", "text": "Azure Custom Vision" }
    ],
    "rightOptions": [
      { "id": "1", "text": "/sentiment/v3.1/custom" },
      { "id": "2", "text": "/language/:analyze-text" },
      { "id": "3", "text": "/vision/analyze" }
    ],
    "correctAnswer": { "left": "1", "right": "1" },
    "explanation": "Azure Text Analytics custom sentiment endpoint supports custom categories for sentiment classification."
  },
  {
    "id": "q24",
    "type": "boolean",
    "question": [
      "You want to detect whether an image contains adult or racy content using Azure Computer Vision.",
      "You configure the Analyze Image API with the 'Adult' visual feature enabled.",
      "Solution: You send images to the API and check the 'isAdultContent' property in the response.",
      "Does this meet the goal?"
    ],
    "correctAnswer": "True",
    "explanation": "✅ The 'isAdultContent' boolean property indicates the presence of adult or racy content in the image."
  },
  {
    "id": "q25",
    "type": "single",
    "question": [
      "You have a Custom Speech service in Azure.",
      "You want to improve the accuracy of the speech-to-text model for industry-specific terminology.",
      "Which method should you use?"
    ],
    "options": [
      { "id": "a", "text": "Use the prebuilt model without changes" },
      { "id": "b", "text": "Create and train a custom speech model with phrase lists" },
      { "id": "c", "text": "Use Custom Vision service" },
      { "id": "d", "text": "Use Language Understanding (LUIS)" }
    ],
    "correctAnswer": "b",
    "explanation": "Training a custom speech model with phrase lists improves recognition for domain-specific words."
  },
  {
    "id": "q26",
    "type": "drag-sequence",
    "question": [
      "You want to deploy an Azure Text Analytics solution that complies with data residency regulations and processes data locally.",
      "Which three steps should you perform?"
    ],
    "options": [
      { "id": "1", "text": "Provision an Azure Text Analytics resource with Private Endpoint." },
      { "id": "2", "text": "Deploy Text Analytics container on-premises." },
      { "id": "3", "text": "Export the model from Azure portal." },
      { "id": "4", "text": "Run the container locally and configure local data storage." },
      { "id": "5", "text": "Send data to the public Azure Text Analytics endpoint." }
    ],
    "correctAnswer": ["1", "2", "4"],
    "explanation": "Provision resource with Private Endpoint to ensure network isolation, deploy container on-premises, and run it locally with proper data storage configuration to comply with data residency."
  },
{
  "id": "q27",
  "type": "single",
  "question": [
    "You want to analyze video content to detect objects and activities using Azure Video Analyzer.",
    "Which component of the Azure Video Analyzer solution is responsible for extracting insights from video streams?"
  ],
  "options": [
    { "id": "a", "text": "Edge modules" },
    { "id": "b", "text": "Video Analyzer service" },
    { "id": "c", "text": "Stream processor" },
    { "id": "d", "text": "Video Indexer" }
  ],
  "correctAnswer": "d",
  "explanation": "Azure Video Indexer is the component responsible for extracting rich insights from video streams, such as object detection, face recognition, speech transcription, and activity detection. While other components like edge modules and stream processors are used for ingesting and processing video streams, the actual insight extraction is performed by Video Indexer."
},
 {
  "id": "q28",
  "type": "drag-sequence",
  "question": [
    "You want to create a pipeline for document processing using Azure AI Document Intelligence.",
    "Which three steps should you perform in order?"
  ],
  "options": [
    { "id": "1", "text": "Upload the documents to a Blob Storage container" },
    { "id": "2", "text": "Create a Document Intelligence project" },
    { "id": "3", "text": "Label the documents with key-value pairs" },
    { "id": "4", "text": "Train the custom model" },
    { "id": "5", "text": "Deploy the model" }
  ],
  "correctAnswer": ["1", "2", "3"],
  "explanation": "To start building a custom document model with Azure AI Document Intelligence, the correct initial steps are: (1) Upload documents to Blob Storage to make them accessible to the training service, (2) Create a Document Intelligence project in the studio or via API to organize and manage the training data, and (3) Label the uploaded documents with key-value pairs to provide ground truth for model training. Only after labeling can you proceed to train (step 4) and deploy (step 5) the model."
}
,{
  "id": "q29",
  "type": "single",
  "question": "You are developing an application using Azure AI Services which will be used to:\n• Analyze images\n• Detect objects, faces, and text in images\n\nThe solution must ensure:\n• Minimum latency\n• Optimized performance\n\nWhich Azure AI Service should you use?",
  "options": [
    {
      "id": "a",
      "text": "Azure AI Vision"
    },
    {
      "id": "b",
      "text": "Azure AI Face"
    },
    {
      "id": "c",
      "text": "Azure AI Document Intelligence"
    }
  ],
  "correctAnswer": "a",
  "explanation": "✅ The Azure AI Vision service supports analyzing images for objects, faces, and text. It provides comprehensive image analysis capabilities in one service, making it optimal for scenarios requiring high performance and low latency."
},{
  "id": "q30",
  "type": "single",
  "question": "E-commerce company, GreenCycle, wants to enhance its product pages by optimizing images for:\n• Various devices\n• Different screen sizes\n\nTheir product team uploads high-resolution images.\nThese images need to be:\n• Resized\n• Cropped automatically for mobile thumbnails\n\nWhich feature of **AI Vision AnalyzeImage** should you use?",
  "options": [
    {
      "id": "a",
      "text": "Detect objects"
    },
    {
      "id": "b",
      "text": "Generate caption"
    },
    {
      "id": "c",
      "text": "Smart crop"
    },
    {
      "id": "d",
      "text": "Tag Visual Features"
    }
  ],
  "correctAnswer": "c",
  "explanation": "✅ Smart crop automatically generates image crops optimized for the region of interest, making it ideal for creating responsive thumbnails for mobile and other devices."
},{
  "id": "q31",
  "type": "single",
  "question": [
    "You're developing a conversational AI using Azure OpenAI's GPT-35 Turbo model.",
    "The System Message you provided is:",
    "\"You're an AI assistant that helps people find information. Answer in as few words as possible.\"",
    "Determine which type of prompt engineering technique this system message represents."
  ],
  "options": [
    { "id": "a", "text": "Affordance" },
    { "id": "b", "text": "Few-shot learning" },
    { "id": "c", "text": "Chain of thought" },
    { "id": "d", "text": "Priming" }
  ],
  "correctAnswer": "d",
  "explanation": "This is an example of **priming**, where the system prompt sets the tone, context, and behavior of the assistant before any user input. Priming helps guide the model’s responses without requiring additional input examples."
},
{
  "id": "q32",
  "type": "single",
  "question": [
    "An e-commerce company wants to optimize images for mobile thumbnails.",
    "Which feature of AI Vision AnalyzeImage should you use?"
  ],
  "options": [
    { "id": "a", "text": "Detect objects" },
    { "id": "b", "text": "Generate caption" },
    { "id": "c", "text": "Smart crop" },
    { "id": "d", "text": "Tag Visual Features" }
  ],
  "correctAnswer": "c",
  "explanation": "Smart crop allows images to be resized and cropped optimized for thumbnails."
},{
  "id": "q33",
  "type": "multiple",
  "question": [
    "You are developing a method that uses the Computer Vision client library.",
    "The method will perform optical character recognition (OCR) in images.",
    "During testing, you discover that the call to the GetReadResultAsync method occurs before the read operation is complete.",
    "You need to prevent the GetReadResultAsync method from proceeding until the read operation is complete.",
    "Which two actions should you perform? Each correct answer presents part of the solution.",
    "NOTE: Each correct selection is worth one point.",
    "Code snippet:",
    "public static async Task ReadFileUrl(ComputerVisionClient client, string urlFile)",
    "{",
    "    const int numberOfCharsInOperationId = 36;",
    "    var txtHeaders = await client.ReadAsync(urlFile, language: \"en\");",
    "    string opLocation = txtHeaders.OperationLocation;",
    "    string operationId = opLocation.Substring(opLocation.Length - numberOfCharsInOperationId);",
    "    ReadOperationResult results;",
    "    results = await client.GetReadResultAsync(Guid.Parse(operationId));",
    "    var textUrlFileResults = results.AnalyzeResult.ReadResults;",
    "    foreach (ReadResult page in textUrlFileResults)",
    "    {",
    "        foreach (Line line in page.Lines)",
    "        {",
    "            Console.WriteLine(line.Text);",
    "        }",
    "    }",
    "}"
  ],
  "options": [
    { "id": "A", "text": "Wrap the call to GetReadResultAsync within a loop that contains a delay." },
    { "id": "B", "text": "Add code to verify the results.Status value." },
    { "id": "C", "text": "Add code to verify the status of the txtHeaders.Status value." },
    { "id": "D", "text": "Remove the Guid.Parse(operationId) parameter." }
  ],
  "correctAnswer": ["A", "B"],
  "explanation": "To ensure GetReadResultAsync waits until the read operation is complete, wrap the call in a loop with a delay to poll the status, and check the results.Status to confirm completion before proceeding."
},{
  "id": "q34",
  "type": "single",
  "question": [
    "You're building a corporate website that will showcase meeting recordings using Azure Video Analyzer for Media.",
    "You embed the Player and Insights widgets into your webpage.",
    "The solution must:",
    "• Enable keyword search within the video content.",
    "• Display identified people and their names from the video.",
    "• Show video captions in US English.",
    "How should you configure the URLs for the widgets to meet these requirements?",
    "Cognitive Insights Widget URL:",
    "https://www.videoindexer.ai/embed/ins.... &controls= ..................................",
    "Player Widget URL:",
    "https://www.videoindexer.ai/embed/pla.... &captions= ..........................."
  ],
  "options": [
    { "id": "A", "text": "widgets=people, search, controls=search; showcaptions=true, captions=en-US" },
    { "id": "B", "text": "widgets=false, controls=search; showcaptions=true, captions=en-US" },
    { "id": "C", "text": "widgets=people,keywords, controls=search; showcaptions=true, captions=en-US" }
  ],
  "correctAnswer": "C",
  "explanation": "Option C correctly configures the Insights widget to show people and keywords widgets with search controls, while the Player widget is set to show captions in US English."
},{
  "id": "q35",
  "type": "single",
  "question": [
    "You are building an app that will include one million scanned magazine articles.",
    "Each article will be stored as an image file.",
    "You need to configure the app to extract text from the images.",
    "The solution must minimize development effort.",
    "What should you include in the solution?"
  ],
  "options": [
    { "id": "A", "text": "Computer vision image analysis" },
    { "id": "B", "text": "The read API in Azure AI Vision" },
    { "id": "C", "text": "Document Intelligence" },
    { "id": "D", "text": "Azure AI service for language" }
  ],
  "correctAnswer": "B",
  "explanation": "The Read API in Azure AI Vision is designed for OCR and can efficiently extract text from images, minimizing development effort."
},{
  "id": "q36",
  "type": "multiple",
  "question": [
    "You use the Custom Vision service to build a classifier.",
    "After training is complete, you need to evaluate the classifier.",
    "Which two metrics are available for review?",
    "Each correct answer presents a complete solution.",
    "NOTE: Each correct selection is worth one point."
  ],
  "options": [
    { "id": "A", "text": "recall" },
    { "id": "B", "text": "F-score" },
    { "id": "C", "text": "weighted accuracy" },
    { "id": "D", "text": "precision" },
    { "id": "E", "text": "area under the curve (AUC)" }
  ],
  "correctAnswer": ["A", "D"],
  "explanation": "Custom Vision provides precision and recall as standard evaluation metrics. These help in assessing how well your classifier is performing in terms of relevant and correct predictions."
},
{
  "id": "q37",
  "type": "single",
  "question": [
    "You have created an object detection project named ‘MyDetectionProject’.",
    "You trained the custom vision model and the following is the performance of the model based on training data.",
    "Select the most appropriate choice for the following statement based on the information in the image:",
    "**The percentage of false positives is**"
  ],
  "options": [
    { "id": "a", "text": "0" },
    { "id": "b", "text": "66.7" },
    { "id": "c", "text": "95.8" },
    { "id": "d", "text": "50" },
    { "id": "e", "text": "100" }
  ],
  "correctAnswer": "a",
  "explanation": "✅ The percentage of false positives is 0%, meaning the model did not incorrectly classify any negative example as positive."
},{
  "id": "q38",
  "type": "single",
  "question": [
    "The value for the number of true positives divided by the total number of true positives and false negatives is",
    "[answer choice]%."
  ],
  "options": [
    { "id": "a", "text": "0" },
    { "id": "b", "text": "66.7" },
    { "id": "c", "text": "95.8" },
    { "id": "d", "text": "50" },
    { "id": "e", "text": "100" }
  ],
  "correctAnswer": "b",
  "explanation": "✅ This value represents **recall**. Based on the image's data, the recall (true positives / (true positives + false negatives)) is 66.7%."
},{
  "id": "q39",
  "type": "single",
  "question": [
    "You have an Azure subscription that contains an Azure AI Content Safety resource named CS1.",
    "You plan to build an app that will analyze user-generated documents and identify obscure offensive terms.",
    "You need to create a dictionary that will contain the offensive terms.",
    "The solution must minimize development effort.",
    "What should you use?"
  ],
  "options": [
    { "id": "a", "text": "a text classifier" },
    { "id": "b", "text": "language detection" },
    { "id": "c", "text": "text moderation" },
    { "id": "d", "text": "a blocklist" }
  ],
  "correctAnswer": "d",
  "explanation": "✅ A **blocklist** in Azure AI Content Safety allows you to define custom offensive terms (including obscure ones) with minimal development effort. This is ideal for user-generated content filtering."
},{
  "id": "q40",
  "type": "single",
  "question": [
    "You have an Azure subscription with an Azure OpenAI resource named AI1.",
    "You developed a chatbot using AI1 to generate answers for specific questions.",
    "You need to prevent users from bypassing the built-in safety mechanisms when asking questions.",
    "Which Azure AI Content Safety feature should you implement?"
  ],
  "options": [
    { "id": "a", "text": "Monitor online activity" },
    { "id": "b", "text": "Jailbreak risk detection" },
    { "id": "c", "text": "Moderate text content" },
    { "id": "d", "text": "Protected material text detection" }
  ],
  "correctAnswer": "b",
  "explanation": "✅ Jailbreak risk detection is a feature in Azure AI Content Safety that identifies attempts to circumvent content filters or safety mechanisms, which is essential when deploying chatbots built with Azure OpenAI."
}
,{
  "id": "q41",
  "type": "single",
  "question": [
    "You have an Azure subscription that contains an Azure OpenAI resource named AI1 and an Azure AI Content Safety resource named CS1.",
    "You build a chatbot that uses AI1 to provide generative answers to specific questions and CS1 to check input and output for objectionable content.",
    "You need to optimize the content filter configurations by running tests on sample questions.",
    "Select the most appropriate solution from the options given below."
  ],
  "options": [
    { "id": "a", "text": "From Content Safety Studio, you use the Moderate text content feature to run the tests." },
    { "id": "b", "text": "From Content Safety Studio, you use the Safety metaprompt feature to run the tests." },
    { "id": "c", "text": "From Content Safety Studio, you use the Monitor online activity feature to run the tests." },
    { "id": "d", "text": "From Content Safety Studio, you use the Protected material detection feature to run the tests." }
  ],
  "correctAnswer": "a",
  "explanation": "✅ The 'Moderate text content' feature in Content Safety Studio allows you to test and configure filtering thresholds by submitting sample prompts or completions, ensuring objectionable input/output is flagged correctly."
},{
  "id": "q42",
  "type": "dropdown-pair",
  "question": [
    "You have an Azure subscription that contains an Azure AI Content Safety resource.",
    "You are building a social media app that will enable users to share images.",
    "You need to configure the app to moderate inappropriate content uploaded by the users.",
    "How should you complete the code? To answer, select the appropriate options in the answer area."
  ],
  "leftOptions": [
    { "id": "1", "text": "AnalyzeTextOptions" },
    { "id": "2", "text": "BlocklistClient" },
    { "id": "3", "text": "ContentSafetyClient" },
    { "id": "4", "text": "TextCategoriesAnalysis" }
  ],
  "rightOptions": [
    { "id": "5", "text": "AnalyzeImage(request)" },
    { "id": "6", "text": "client.AnalyzeImage(request)" },
    { "id": "7", "text": "client.AnalyzeText(request)" },
    { "id": "8", "text": "request.AnalyzeImage(client)" }
  ],
  "correctAnswer": {
    "left": "3",
    "right": "6"
  },
  "explanation": "To moderate image content using Azure AI Content Safety, you instantiate the ContentSafetyClient and then call the AnalyzeImage method on the client instance passing the request."
},{
  "id": "q43",
  "type": "multiple",
  "question": [
    "You have an Azure subscription with an Azure AI Content Safety resource named CS1.",
    "You need to send a request to CS1 to detect hateful language in user input.",
    "How should you complete the curl command URL?"
  ],
  "options": [
    { "id": "a", "text": "completions" },
    { "id": "b", "text": "contentsafety" },
    { "id": "c", "text": "healthinsights/" },
    { "id": "d", "text": "language/" },
    { "id": "e", "text": "embeddings" },
    { "id": "f", "text": "text:analyze" },
    { "id": "g", "text": "text/blocklists" }
  ],
  "correctAnswer": ["b", "f"],
  "explanation": "The Azure AI Content Safety endpoint URL must include 'contentsafety' to target the service, and 'text:analyze' is the correct path to analyze text for hateful language detection."
},{
  "id": "q44",
  "type": "single",
  "question": [
    "You have an Azure subscription that contains an Azure AI Content Safety resource named CS1.",
    "You create a test image that contains a square.",
    "You submit the test image to CS1 by using the curl command and the following command-line parameters.",
    "",
    "What should you expect as the output?"
  ],
  "options": [
    { "id": "a", "text": "0" },
    { "id": "b", "text": "0.0" },
    { "id": "c", "text": "7" },
    { "id": "d", "text": "100" }
  ],
  "correctAnswer": "a",
  "explanation": "The output score for the test image containing a simple shape like a square is expected to be 0, indicating no flagged content or zero severity."
},{
  "id": "q45",
  "type": "single",
  "question": [
    "You are developing a custom question answering project in Azure AI Service for Language.",
    "The project will be used by a chatbot to provide answers to user queries.",
    "You need to configure the project to support multi-turn conversations,",
    "where the chatbot can ask follow-up questions or clarify user intent.",
    "What should you do to enable this functionality?"
  ],
  "options": [
    { "id": "a", "text": "Add follow-up prompts." },
    { "id": "b", "text": "Enable active learning." },
    { "id": "c", "text": "Add alternate questions." },
    { "id": "d", "text": "Enable chit-chat." }
  ],
  "correctAnswer": "a",
  "explanation": "Adding follow-up prompts allows the chatbot to engage in multi-turn conversations by asking clarifying questions or guiding the user for additional input."
},{
  "id": "q46",
  "type": "single",
  "question": [
    "You're developing a contact management chatbot using Azure AI Language Understanding.",
    "The FindContact intent helps users search for contacts by location.",
    "Your manager provides you with the following list of phrases to use for training the model:",
    "\"Find contacts in North Virginia.\", \"Who do I know in Sydney?\", \"Search for contacts in Melbourne.\"",
    "You need to implement these phrases in Language Understanding to improve the model.",
    "What should you do?"
  ],
  "options": [
    { "id": "a", "text": "You create a new utterance for each phrase in the FindContact intent" },
    { "id": "b", "text": "You create a new intent for location." },
    { "id": "c", "text": "You create a new entity for the domain." },
    { "id": "d", "text": "You create a new pattern in the FindContact intent." }
  ],
  "correctAnswer": "a",
  "explanation": "To improve the model, you add new utterances (example phrases) under the existing FindContact intent. This helps the model better recognize user requests related to finding contacts by location."
},{
  "id": "q47",
  "type": "multiple",
  "question": [
    "You have trained a Conversational Language Understanding (CLU) model to interpret natural language input from users.",
    "Before deploying the model, you need to evaluate its accuracy.",
    "What are two methods you can use to evaluate the model's performance?"
  ],
  "options": [
    { "id": "a", "text": "From the language authoring REST endpoint, retrieve the model evaluation summary." },
    { "id": "b", "text": "From Language Studio, enable Active Learning, and then validate the utterances logged for review." },
    { "id": "c", "text": "From Language Studio, select Model performance." },
    { "id": "d", "text": "From the Azure portal, enable log collection in Log Analytics, and then analyze the logs." }
  ],
  "correctAnswer": ["a", "c"],
  "explanation": "You can evaluate the model by retrieving the evaluation summary via the REST endpoint and by viewing Model performance metrics directly in Language Studio."
},{
  "id": "q48",
  "type": "single",
  "question": [
    "You have an Azure AI Language model named IntentModel that identifies the intent of text input.",
    "You are developing a Python application named ChatApp and need to configure it to use IntentModel.",
    "Which Python package should you add to ChatApp to integrate with the Azure AI Language service?"
  ],
  "options": [
    { "id": "a", "text": "azure-ai-language-conversations" },
    { "id": "b", "text": "azure-cognitiveservices-language-textanalytics" },
    { "id": "c", "text": "azure.cognitiveservices.speech" },
    { "id": "d", "text": "azure-ai-documentintelligence" }
  ],
  "correctAnswer": "a",
  "explanation": "The azure-ai-language-conversations package is used to integrate Azure AI Language models that handle conversational and intent recognition tasks."
},{
  "id": "q49",
  "type": "single",
  "question": [
    "You are building a conversational language understanding model.",
    "You need to enable active learning.",
    "What should you do?"
  ],
  "options": [
    { "id": "a", "text": "Add show-all-intents=true to the prediction endpoint query." },
    { "id": "b", "text": "Enable speech priming." },
    { "id": "c", "text": "Add log=true to the prediction endpoint query." },
    { "id": "d", "text": "Enable sentiment analysis." }
  ],
  "correctAnswer": "c",
  "explanation": "Adding log=true to the prediction endpoint query enables active learning by logging utterances for review and improvement."
},{
  "id": "q50",
  "type": "single",
  "question": [
    "You develop a Conversational Language Understanding model by using Language Studio.",
    "During testing, users receive incorrect responses to requests that do NOT relate to the capabilities of the model.",
    "You need to ensure that the model identifies spurious (not genuine or authentic: false) requests.",
    "How should you solve this?"
  ],
  "options": [
    { "id": "a", "text": "Enable active learning." },
    { "id": "b", "text": "Add entities." },
    { "id": "c", "text": "Add examples to the None intent." },
    { "id": "d", "text": "Add examples to the custom intents." }
  ],
  "correctAnswer": "c",
  "explanation": "Adding examples to the None intent helps the model recognize requests that don't belong to any defined intent, thus identifying spurious inputs."
},{
  "id": "q51",
  "type": "single",
  "question": [
    "You are working on a project to develop a language model using Azure's AI Language Understanding (classic) service.",
    "You need to allow additional team members to contribute to the development and training of the model.",
    "What should you use to grant access to the authoring resources for your Language Understanding (classic) service?"
  ],
  "options": [
    { "id": "a", "text": "The Access control (IAM) page for the authoring resources in the Azure portal" },
    { "id": "b", "text": "Access Policy in Azure subscription management page in the Azure portal." },
    { "id": "c", "text": "The Access control (IAM) page for the prediction resources in the Azure portal" },
    { "id": "d", "text": "The Microsoft Active Directory Portal." }
  ],
  "correctAnswer": "a",
  "explanation": "To grant team members access to authoring resources for the Language Understanding (classic) service, use the Access control (IAM) page specifically for the authoring resources in the Azure portal."
},{
  "id": "q52",
  "type": "single",
  "question": [
    "You are building a Conversational Language Understanding (CLU) model using the Language Services portal.",
    "After exporting the model as a JSON file, you analyze the following sample:",
    "{",
    "  \"text\": \"average amount of rain by month at chicago last year\",",
    "  \"intent\": \"Weather.CheckWeatherValue\",",
    "  \"entities\": [",
    "    {",
    "      \"entity\": \"Weather.WeatherRange\",",
    "      \"startPos\": 0,",
    "      \"endPos\": 6,",
    "      \"children\": []",
    "    },",
    "    {",
    "      \"entity\": \"Weather.WeatherCondition\",",
    "      \"startPos\": 18,",
    "      \"endPos\": 21,",
    "      \"children\": []",
    "    },",
    "    {",
    "      \"entity\": \"Weather.Historic\",",
    "      \"startPos\": 23,",
    "      \"endPos\": 30,",
    "      \"children\": []",
    "    }",
    "  ]",
    "}",
    "In the utterance \"average amount of rain by month at chicago last year\", what does the Weather.Historic entity correspond to?"
  ],
  "options": [
    { "id": "a", "text": "by month" },
    { "id": "b", "text": "chicago" },
    { "id": "c", "text": "rain" },
    { "id": "d", "text": "location" }
  ],
  "correctAnswer": "a",
  "explanation": "The Weather.Historic entity corresponds to the phrase 'by month' which is located at positions 23 to 30 in the utterance, representing a time or range descriptor."
},{
  "id": "53",
  "type": "matching-pairs",
  "question": [
    "Match the phrases with their entity types."
  ],
  "leftOptions": [
    { "id": "1", "text": "New York" },
    { "id": "2", "text": "email@xyz.com" },
    { "id": "3", "text": "3 audit business" }
  ],
  "rightOptions": [
    { "id": "a", "text": "geographyv2" },
    { "id": "b", "text": "email" },
    { "id": "c", "text": "machinelearned" }
  ],
  "correctAnswer": {
    "1": "a",
    "2": "b",
    "3": "c"
  },
  "explanation": "Match the location, email and phrase with their correct entity types."
},{
  "id": "q54",
  "type": "single",
  "question": [
    "You are developing a Conversational Language Understanding (CLU) model for an e-commerce chatbot.",
    "The chatbot prompts users to provide their billing address, which can be spoken or typed.",
    "Which entity type should you use to capture billing addresses effectively?"
  ],
  "options": [
    { "id": "a", "text": "machine learned" },
    { "id": "b", "text": "Regex" },
    { "id": "c", "text": "geographyV2" },
    { "id": "d", "text": "Pattern.any" },
    { "id": "e", "text": "list" }
  ],
  "correctAnswer": "a",
  "explanation": "Billing addresses are complex and vary greatly in format, so a machine learned entity is best suited to accurately capture this type of information in the CLU model."
},{
  "id": "q55",
  "type": "single",
  "question": [
    "In LUIS (Language Understanding Intelligent Service), which type of entity is designed to capture any sequence of words in a user utterance, providing flexibility to match varying input patterns?"
  ],
  "options": [
    { "id": "a", "text": "Prebuilt Entity" },
    { "id": "b", "text": "Simple Entity" },
    { "id": "c", "text": "List Entity" },
    { "id": "d", "text": "Pattern.any" }
  ],
  "correctAnswer": "d",
  "explanation": "The Pattern.any entity in LUIS is designed to capture any sequence of words, offering flexibility in matching variable input patterns."
},{
  "id": "q56",
  "type": "single",
  "question": [
    "You are building an Azure AI Language Understanding (LUIS) solution.",
    "During development, you notice that many intents include similar utterances containing airport names or airport codes.",
    "You need to minimize the number of utterances required to train the model while ensuring accurate recognition of airport-related information.",
    "Which type of custom entity should you use?"
  ],
  "options": [
    { "id": "a", "text": "List" },
    { "id": "b", "text": "machine-learning" },
    { "id": "c", "text": "Number" },
    { "id": "d", "text": "Regex" }
  ],
  "correctAnswer": "a",
  "explanation": "A List entity is useful for capturing a predefined set of values, such as airport names or codes, minimizing the training utterances needed."
},{
  "id": "q57",
  "type": "single",
  "question": [
    "You are developing a Natural Language Processing model for a customer support chatbot that collects shipping addresses from users.",
    "Users can either type or dictate their shipping address when interacting with the chatbot.",
    "Which entity type should you implement to effectively capture shipping addresses?"
  ],
  "options": [
    { "id": "a", "text": "machine learned" },
    { "id": "b", "text": "Regex" },
    { "id": "c", "text": "geographyV2" },
    { "id": "d", "text": "Pattern.any" },
    { "id": "e", "text": "list" }
  ],
  "correctAnswer": "a",
  "explanation": "A machine-learned entity is best suited for capturing complex inputs like shipping addresses that may vary significantly in format."
},{
  "id": "q58",
  "type": "single",
  "question": [
    "You have a Language service resource that performs the following:",
    "• Sentiment analysis",
    "• Named Entity Recognition (NER)",
    "• Personally Identifiable Information (PII) identification",
    "You need to prevent the resource from persisting input data once the data is analyzed.",
    "Which query parameter in the Language service API should you configure?"
  ],
  "options": [
    { "id": "a", "text": "model-version" },
    { "id": "b", "text": "piiCategories" },
    { "id": "c", "text": "showStats" },
    { "id": "d", "text": "loggingOptOut" }
  ],
  "correctAnswer": "d",
  "explanation": "The 'loggingOptOut' query parameter prevents the Language service from persisting input data, ensuring user data privacy."
},{
  "id": "q59",
  "type": "multiple",
  "question": [
    "You plan to deploy a containerized Text Analytics Sentiment Analysis service on an Azure virtual machine using Docker.",
    "To run the container, you need to:",
    "- Use the latest version of the Text Analytics Sentiment Analysis container.",
    "- Configure the container to use your Cognitive Services endpoint URI: https://TechCorp.cognitiveservices.azure.com",
    "How should you complete the Docker run command? Select 2 options in the correct order."
  ],
  "options": [
    { "id": "a", "text": "mcr.microsoft.com/azure-cognitive-services/text analytics/keyphrase" },
    { "id": "b", "text": "http://TechCorp.blob.core.windows.net" },
    { "id": "c", "text": "mcr.microsoft.com/azure-cognitive-services/textanalytics/sentiment" },
    { "id": "d", "text": "https://TechCorp.cognitiveservices.azure.com" }
  ],
  "correctAnswer": ["c", "d"],
  "explanation": "The correct container image is the latest Text Analytics Sentiment Analysis container 'mcr.microsoft.com/azure-cognitive-services/textanalytics/sentiment'. You also need to configure the 'Billing' environment variable with your Cognitive Services endpoint URI, which is 'https://TechCorp.cognitiveservices.azure.com'."
},{
  "id": "q60",
  "type": "single",
  "question": [
    "You have an Azure subscription that contains an Azure Cognitive Service for Language resource.",
    "You need to identify the URL of the REST interface for the Language service.",
    "Which blade should you use in the Azure portal?"
  ],
  "options": [
    { "id": "a", "text": "Identity" },
    { "id": "b", "text": "Keys and Endpoint" },
    { "id": "c", "text": "Networking" },
    { "id": "d", "text": "Properties" }
  ],
  "correctAnswer": "b",
  "explanation": "The 'Keys and Endpoint' blade in the Azure portal provides the API endpoint URL and the keys required to access the REST interface for the Azure Cognitive Service for Language."
},{
  "id": "q61",
  "type": "single",
  "question": [
    "You are developing a solution that extracts key phrases from text using Azure AI Text Analytics.",
    "You write the following code:",
    "",
    "```csharp",
    "var response = client.ExtractKeyPhrases(\"Our tour of Paris included Eiffel Tower.\");",
    "```",
    "",
    "What are the expected key phrases?"
  ],
  "options": [
    { "id": "a", "text": "Our, Eiffel, Included" },
    { "id": "b", "text": "Included, Paris, Our" },
    { "id": "c", "text": "Tour, Paris, Eiffel Tower" },
    { "id": "d", "text": "Client, Tour, Tower" }
  ],
  "correctAnswer": "c",
  "explanation": "The `ExtractKeyPhrases` API identifies important noun phrases. For the input 'Our tour of Paris included Eiffel Tower', the correct key phrases extracted would be 'Tour', 'Paris', and 'Eiffel Tower'."
},{
  "id": "q62",
  "type": "single",
  "question": [
    "You are planning to create a word cloud based on the reviews of a company's products.",
    "Which Text Analytics REST API endpoint should you use?"
  ],
  "options": [
    { "id": "a", "text": "entities/recognition/general" },
    { "id": "b", "text": "sentiment" },
    { "id": "c", "text": "/entities/recognition/pii" },
    { "id": "d", "text": "keyPhrases" }
  ],
  "correctAnswer": "d",
  "explanation": "To create a word cloud based on reviews, you should extract the key phrases that represent important terms from the text. The 'keyPhrases' endpoint is designed to do this by identifying the main concepts in unstructured text."
},{
  "id": "q63",
  "type": "multiple",
  "question": [
    "To build a multilingual chatbot that sends different responses for positive and negative messages,",
    "which two Language Service APIs should you use?"
  ],
  "options": [
    { "id": "a", "text": "Linked entities from a well-known knowledge base" },
    { "id": "b", "text": "Named Entity Recognition" },
    { "id": "c", "text": "PII recognition" },
    { "id": "d", "text": "Detect Language" },
    { "id": "e", "text": "Sentiment Analysis" }
  ],
  "correctAnswer": ["d", "e"],
  "explanation": "To build a multilingual chatbot, you must first detect the language of the input using the **Detect Language** API. Then, use **Sentiment Analysis** to classify the message as positive or negative, allowing your chatbot to tailor responses accordingly."
},{
  "id": "q64",
  "type": "matching-pairs",
  "question": [
    "Match the requirements with the appropriate Azure AI Language service."
  ],
  "leftOptions": [
    { "id": "1", "text": "Identify messages with phone number in it" },
    { "id": "2", "text": "Extract domain-specific entities" }
  ],
  "rightOptions": [
    { "id": "a", "text": "Custom named entity recognition (NER)" },
    { "id": "b", "text": "Language detection" },
    { "id": "c", "text": "Named Entity Recognition (NER)" },
    { "id": "d", "text": "Personally Identifiable Information (PII) detection" },
    { "id": "e", "text": "Sentiment analysis" }
  ],
  "correctAnswer": {
    "1": "d",
    "2": "a"
  },
  "explanation": "Phone numbers are considered PII and are detected using the Personally Identifiable Information (PII) detection service. Domain-specific entities require Custom Named Entity Recognition (NER)."
},{
  "id": "65",
  "type": "single",
  "question": [
    "You want to analyze public sentiment about your brand on social media using natural language processing.",
    "Which Azure service should you use?"
  ],
  "options": [
    { "id": "a", "text": "Content Moderator" },
    { "id": "b", "text": "Computer Vision" },
    { "id": "c", "text": "Language service" },
    { "id": "d", "text": "Document Intelligence" }
  ],
  "correctAnswer": "c",
  "explanation": "The Azure Language service provides natural language processing capabilities such as sentiment analysis, which is suitable for analyzing public sentiment about your brand."
},{
  "id": "66",
  "type": "single",
  "question": [
    "You have an Azure OpenAI model.",
    "You are building a web app by using the Azure OpenAI SDK.",
    "You need to configure the App to connect to your OpenAI model.",
    "What information must you provide?"
  ],
  "options": [
    { "id": "a", "text": "the endpoint, key, and model name" },
    { "id": "b", "text": "the deployment name, key, and model name" },
    { "id": "c", "text": "the deployment name, endpoint, and key" },
    { "id": "d", "text": "the endpoint, key, and model type" }
  ],
  "correctAnswer": "c",
  "explanation": "To connect to an Azure OpenAI model using the SDK, you need the deployment name (which identifies the specific deployed model), the endpoint URL of your Azure OpenAI resource, and the subscription key for authentication."
},{
  "id": "67",
  "type": "single",
  "question": [
    "You have an Azure OpenAI model.",
    "You are building a web app by using the Azure OpenAI SDK.",
    "You need to configure the App to connect to your OpenAI model.",
    "What information must you provide?"
  ],
  "options": [
    { "id": "a", "text": "the endpoint, key, and model name" },
    { "id": "b", "text": "the deployment name, key, and model name" },
    { "id": "c", "text": "the deployment name, endpoint, and key" },
    { "id": "d", "text": "the endpoint, key, and model type" }
  ],
  "correctAnswer": "c",
  "explanation": "To connect to an Azure OpenAI model using the SDK, you need the deployment name (which identifies the specific deployed model), the endpoint URL of your Azure OpenAI resource, and the subscription key for authentication."
},{
  "id": "68",
  "type": "single",
  "question": [
    "You have an Azure subscription.",
    "You need to build an app that will compare documents for semantic similarity.",
    "The solution must meet the following requirements:",
    "• Return numeric vectors that represent the tokens of each document.",
    "• Minimize development effort.",
    "Which Azure OpenAI model should you use?"
  ],
  "options": [
    { "id": "a", "text": "GPT-3.5" },
    { "id": "b", "text": "GPT-4" },
    { "id": "c", "text": "embeddings" },
    { "id": "d", "text": "DALL-E" }
  ],
  "correctAnswer": "c",
  "explanation": "The embeddings model generates numeric vector representations of text which are suitable for semantic similarity comparisons and require minimal development effort."
},{
  "id": "q69",
  "type": "matching-pairs",
  "question": [
    "Complete the following code snippet by matching each placeholder with the correct value:",
    "```",
    "new ChatCompletionsOptions()  ",
    "{  ",
    "    Messages =  ",
    "    {  ",
    "        new ChatMessage(..............., @\" What are the benefits of creative writing? \")  ",
    "    },    ",
    "    .................... = (float)1.0,    ",
    "    MaxTokens = 800,  ",
    "};",
    "```"
  ],
  "leftOptions": [
    { "id": "1", "text": "ChatMessage role" },
    { "id": "2", "text": "Parameter to increase creativity" }
  ],
  "rightOptions": [
    { "id": "a", "text": "ChatRole.User" },
    { "id": "b", "text": "ChatRole.Assistant" },
    { "id": "c", "text": "Temperature" },
    { "id": "d", "text": "PresencePenalty" }
  ],
  "correctAnswer": {
    "1": "a",
    "2": "c"
  },
  "explanation": "Use ChatRole.User to specify that the message is from the user. The 'Temperature' parameter controls creativity; higher values increase creativity and reduce determinism."
},{
  "id": "q70",
  "type": "multiple",
  "question": [
    "Using Azure OpenAI resource a model is built with the below settings:",
    "• Top probabilities: 0.5",
    "• Temperature: 1",
    "• Max response tokens: 100",
    "",
    "Here is the model response:",
    "```json",
    "{",
    "  \"choices\": [",
    "    {",
    "      \"finish_reason\": \"stop\",",
    "      \"index\": 0,",
    "      \"message\": {",
    "        \"content\": \"The founders of Microsoft are Bill Gates and Paul Allen. They co-founded the company in 1975.\",",
    "        \"role\": \"assistant\"",
    "      }",
    "    }",
    "  ],",
    "  \"model\": \"gpt-3.5-turbo-0301\",",
    "  \"usage\": {",
    "    \"completion_tokens\": 86,",
    "    \"prompt_tokens\": 37,",
    "    \"total_tokens\": 123",
    "  }",
    "}",
    "```",
    "Which among the following statements are true?"
  ],
  "options": [
    { "id": "a", "text": "The model used was text-embedding-ada-002" },
    { "id": "b", "text": "The prompt_tokens value will be included in the calculation of the Max response tokens value." },
    { "id": "c", "text": "The subscription will be charged 86 tokens for the execution of the session." },
    { "id": "d", "text": "The text completion was truncated because the Max response tokens value was exceeded." },
        { "id": "e", "text": "All false" }
  ],
  "correctAnswer": ["e"],
  "explanation": "None of the statements are correct. The model used is gpt-3.5-turbo, not text-embedding-ada-002. Prompt tokens are not counted in the max response token limit. Charges are based on total tokens (prompt + completion), not just completion. The completion stopped naturally, not due to token limits."
},{
  "id": "q72",
  "type": "matching-pairs",
  "question": [
    "Complete the Python code snippet to upload company data and ensure the chatbot uses it to answer questions:",
    "import openai",
    "import os",
    "response = openai. .......... .create(",
    "    messages=[{\"role\": \"user\", \"content\": \"What are the different Azure AI services?\"}],",
    "    deployment_id=os.environ.get(\"AOAIDeploymentId\"),",
    "    dataSources=[",
    "        {",
    "            \"type\": \".............\",",
    "            \"parameters\": {",
    "                \"endpoint\": os.environ.get(\"SearchEndpoint\"),",
    "                \"key\": os.environ.get(\"SearchKey\"),",
    "                \"indexName\": os.environ.get(\"SearchIndex\"),",
    "            }",
    "        }",
    "    ]",
    ")",
    "print(response)"
  ],
  "leftOptions": [
    { "id": "1", "text": "openai method to call" },
    { "id": "2", "text": "type value in dataSources" }
  ],
  "rightOptions": [
    { "id": "a", "text": "Chat.completions" },
    { "id": "b", "text": "Completion" },
    { "id": "c", "text": "Embedding" },
    { "id": "d", "text": "AzureAISearch" },
    { "id": "e", "text": "AzureDocumentIntelligence" },
    { "id": "f", "text": "Blobstorage" }
  ],
  "correctAnswer": {
    "1": "a",
    "2": "d"
  },
  "explanation": "The openai method 'Chat.completions' is used to generate chat responses. The 'type' in dataSources should be 'AzureAISearch' to connect with Azure Cognitive Search for enriched data."
},{
  "id": "q73",
  "type": "single",
  "question": [
    "What is the most appropriate Azure role to assign to User1 to ensure they can identify resource endpoints, view available models for deployment, and generate text and images using deployed models in Azure OpenAI Studio, following the principle of least privilege?"
  ],
  "options": [
    { "id": "a", "text": "Cognitive Services OpenAI User" },
    { "id": "b", "text": "Cognitive Services Contributor" },
    { "id": "c", "text": "Cognitive Services Usages Reader" },
    { "id": "d", "text": "Cognitive Services OpenAI Contributor" }
  ],
  "correctAnswer": "a",
  "explanation": "The 'Cognitive Services OpenAI User' role allows users to identify resource endpoints, view available deployed models, and generate text/images with least privilege, while Contributor roles grant broader permissions not required here."
}
,{
  "id": "q74",
  "type": "single",
  "question": [
    "You're fine-tuning an Azure OpenAI model with 500 prompt-completion pairs. What file format should you use to ensure seamless data ingestion and processing by the Azure OpenAI fine-tuning API?"
  ],
  "options": [
    { "id": "a", "text": "TSV" },
    { "id": "b", "text": "JSONL (JSON Lines)" },
    { "id": "c", "text": "JSON" },
    { "id": "d", "text": "CSV" }
  ],
  "correctAnswer": "b",
  "explanation": "Azure OpenAI fine-tuning API expects training data in JSONL format, where each line is a JSON object with 'prompt' and 'completion' fields for seamless processing."
},{
  "id": "q75",
  "type": "multiple",
  "question": [
    "Select the TRUE statements about the following Azure OpenAI chatbot code snippet."
  ],
  "codeSnippet": [
    "import openai",
    "openai.api_key = key",
    "openai.api_base = endpoint",
    "response = openai.ChatCompletion.create(",
    "    engine=deployment_name,",
    "    messages=[",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},",
    "        {\"role\": \"user\", \"content\": \"What is an LLM?\"}",
    "    ]",
    ")",
    "print(response['choices'][0]['message']['content'])"
  ],
  "options": [
    { "id": "a", "text": "The code guarantees that responses will be sourced from the latest, up-to-date AI research." },
    { "id": "b", "text": "Changing \"What is an LLM?\" to \"What is an LLM in the context of AI models?\" will produce a more accurate response." },
    { "id": "c", "text": "The code will only retrieve definitions from trusted AI sources." },
    { "id": "d", "text": "Changing \"You are a helpful assistant.\" to \"You must answer only within the context of AI language models.\" will produce a more relevant response." }
  ],
  "correctAnswer": ["b", "d"],
  "explanation": "The prompt content can improve accuracy and relevance. However, the API response depends on the underlying model and does not guarantee up-to-date or source-verified information."
},
{
  "id": "q76",
  "type": "single",
  "question": [
    "You are developing a solution to process receipt images using the Document Intelligence API.",
    "After uploading the images, the API returns a JSON response with extracted data, including various fields and their confidence scores.",
    "To ensure the accuracy of the extracted information, you need to establish a criterion that triggers a manual review by a designated team if any of the confidence scores for the extracted fields fall below a certain threshold.",
    "Given the JSON structure shown below, which expression should you use to trigger a manual review?",
    "{",
    "  \"documentResults\": [",
    "    {",
    "      \"docType\": \"prebuilt: receipt\",",
    "      \"pageRange\": [],",
    "      \"fields\": {",
    "        \"ReceiptType\": {",
    "          \"type\": \"string\",",
    "          \"valueString\": \"Itemized\",",
    "          \"confidence\": 0.672",
    "        },",
    "        \"MerchantName\": {",
    "          \"type\": \"string\",",
    "          \"valueString\": \"Tailwind\",",
    "          \"text\": \"Tailwind\",",
    "          \"boundingBox\": [],",
    "          \"page\": 1,",
    "          \"confidence\": 0.913,",
    "          \"elements\": []",
    "        }",
    "      }",
    "    }",
    "  ]",
    "}"
  ],
  "options": [
    { "id": "a", "text": "documentResults.fields.ReceiptType.confidence less than 0.5" },
    { "id": "b", "text": "documentResults.fields.ReceiptType.confidence greater than 0.7" },
    { "id": "c", "text": "documentResults.fields.*.confidence less than 0.7" },
    { "id": "d", "text": "documentResults.fields.MerchantName.valueString == \"Tailwind\"" }
  ],
  "correctAnswer": "c",
  "explanation": "To trigger manual review when any extracted field has a confidence score below a threshold, you need to check all fields dynamically. The expression 'documentResults.fields.*.confidence less than 0.7' captures this by evaluating all confidence values in the fields object."
},{
  "id": "q77",
  "type": "matching-pairs",
  "question": [
    "Complete the following code snippet by matching each placeholder with the correct code fragment to call the Azure OpenAI resource and print the response."
  ],
  "codeSnippet": [
    "openai.api_key = key",
    "openai.api_base = endpoint",
    "response = ……………………….( ",
    "    engine=deployment_name,",
    "    prompt=\"What is Azure OpenAI?\"  ",
    ")",
    "Print ………………………"
  ],
  "leftOptions": [
    { "id": "1", "text": "Method to call OpenAI resource" },
    { "id": "2", "text": "Print statement to output response text" }
  ],
  "rightOptions": [
    { "id": "a", "text": "openai.ChatCompletion.create" },
    { "id": "b", "text": "openai.Embedding.create" },
    { "id": "c", "text": "openai.Image.create" },
    { "id": "d", "text": "(response.choices[0].text)" },
    { "id": "e", "text": "(response.text)" },
    { "id": "f", "text": "(response.id)" }
  ],
  "correctAnswer": {
    "1": "a",
    "2": "d"
  },
  "explanation": "To generate a text completion, use openai.ChatCompletion.create. The response's text output is found in response.choices[0].text."
},{
  "id": "q78",
  "type": "matching-pairs",
  "question": [
    "Complete the Azure CLI command below by matching each placeholder with the correct value to deploy an Azure OpenAI resource using a customer-managed key stored in Azure Key Vault:\n\naz cognitiveservices account create \\ \n  -n myopenai \\ \n  -g myresourcegroup \\ \n  --kind ........... \\ \n  --sku S-1 \\ \n  --location westus2 \\ \n  --.............. '{\\ \n    \"keySource\": \"Microsoft.KeyVault\",\\ \n    \"keyVaultProperties\": {\\ \n      \"keyName\": \"mykey\",\\ \n      \"keyVersion\": \"1.0\",\\ \n      \"keyVaultUri\": \"https://mykeyvault.vault.azure.net/\"\\ \n    }\\ \n  }'"
  ],
  "leftOptions": [
    { "id": "1", "text": "Value for --kind" },
    { "id": "2", "text": "Option for enabling customer-managed key encryption" }
  ],
  "rightOptions": [
    { "id": "a", "text": "OpenAI" },
    { "id": "b", "text": "LanguageAuthoring" },
    { "id": "c", "text": "AlServices" },
    { "id": "d", "text": "--api-properties" },
    { "id": "e", "text": "--assign-identity" },
    { "id": "f", "text": "--encryption" }
  ],
  "correctAnswer": {
    "1": "a",
    "2": "f"
  },
  "explanation": "Use '--kind OpenAI' to specify the Azure OpenAI resource type. Use '--encryption' to configure the resource to use customer-managed keys stored in Azure Key Vault for data encryption."
},{
  "id": "q79",
  "type": "multiple",
  "question": [
    "You need to upload speech samples to a Speech Studio project for use in training.",
    "How should you upload the samples?"
  ],
  "options": [
    { "id": "a", "text": "Combine the speech samples into a single audio file in the .wma format and upload the file." },
    { "id": "b", "text": "Upload a .zip file that contains a collection of audio files in the .wav format and a corresponding text transcript file." },
    { "id": "c", "text": "Upload individual audio files in the FLAC format and manually upload a corresponding transcript in Microsoft Word format." },
    { "id": "d", "text": "Upload individual audio files in the .wma format." }
  ],
  "correctAnswer": ["b"],
  "explanation": "To train custom models in Speech Studio, you should upload a .zip file that includes multiple .wav audio files along with a transcript file in the specified format. Other formats like .wma or Word documents are not supported for training."
},{
  "id": "q80",
  "type": "multiple",
  "question": [
    "What should you upload to a voice talent profile for a custom neural voice in a retail self-service check-out system?"
  ],
  "options": [
    {
      "id": "a",
      "text": "A five-minute .wmp or .mp3 file of the voice talent describing the retailer."
    },
    {
      "id": "b",
      "text": "A .zip file that contains 60-second .wav files and the associated transcripts as .txt files."
    },
    {
      "id": "c",
      "text": "A five-minute .flac audio file and the associated transcript as a .txt file."
    },
    {
      "id": "d",
      "text": "A .wav or .mp3 file of the voice talent consenting to the creation of a synthetic version of their voice."
    }
  ],
  "correctAnswer": ["b"],
  "explanation": "For Custom Neural Voice training in Azure, you must upload a .zip file containing 60-second .wav audio clips and corresponding .txt transcript files. This structured dataset is required to train the synthetic voice model accurately."
},{
  "id": "q81",
  "type": "multiple",
  "question": [
    "You are assigned a new job to develop an app that will be used in motor vehicles which make use of the text-to-speech functionality of Azure AI Speech service.",
    "How can you optimize text-to-speech voice quality for the app using SSML?"
  ],
  "options": [
    { "id": "a", "text": "the style attribute of the mstts:express-as element" },
    { "id": "b", "text": "the effect attribute of the voice element" },
    { "id": "c", "text": "Use mstts:ttsembedding element" },
    { "id": "d", "text": "Use volume attribute in prosody element" }
  ],
  "correctAnswer": ["b"],
  "explanation": "To optimize TTS voice quality in Azure AI Speech using SSML, the `effect` attribute of the `<voice>` element can be used to apply voice transformation effects like 'style', 'whispered', or 'robotic'. This allows fine-tuning of voice output for specialized environments like motor vehicle systems."
},{
  "id": "q82",
  "type": "multiple",
  "question": [
    "How would you complete the following SSML script for a text-to-speech app that uses a custom neural voice, ensuring the voice has a calm tone and imitates a young adult female?",
    "Select two most appropriate answers from the options.",
    "",
    "Example snippet:",
    "",
    "<mstts:express-as ...=\"YoungAdultFemale\" ...=\"gentle\">",
    "How can I assist you?",
    "</mstts:express-as>"
  ],
  "options": [
    { "id": "a", "text": "role" },
    { "id": "b", "text": "style" },
    { "id": "c", "text": "styledegree" },
    { "id": "d", "text": "type" },
    { "id": "e", "text": "voice" }
  ],
  "correctAnswer": ["a", "b"],
  "explanation": "In SSML for Azure TTS, `role` is used to define the persona (e.g., YoungAdultFemale), and `style` defines the speaking tone (e.g., gentle). These attributes help create more natural and expressive speech output."
},{
  "id": "q83",
  "type": "matching-pairs",
  "question": [
    "How do you translate an English lecture into French and German using Azure AI services? The function named append_to_transcript_file takes translated text and a language identifier as input parameters. Complete the code snippet with appropriate options.",
    "",
    "```python",
    "speech_key = os.environ['SPEECH_SUBSCRIPTION_KEY']",
    "service_region = os.environ['SPEECH_SERVICE_REGION']",
    "",
    "def translate_speech():",
    "    translation_config = speechsdk.translation.SpeechTranslationConfig(subscription=speech_key, region=service_region)",
    "    translation_config.speech_recognition_language = \"en\"",
    "",
    "    languages = ",
    "    for language in languages:",
    "        translation_config.add_target_language(language)",
    "",
    "    audio_config = speechsdk.audio.AudioConfig(use_default_microphone=True)",
    "    recognizer = speechsdk.translation.",
    "        translation_config=translation_config, audio_config=audio_config)",
    "",
    "    result = recognizer.recognize_once()",
    "",
    "    if result.reason == speechsdk.ResultReason.TranslatedSpeech:",
    "        append_to_transcript_file(result.text, \"en\")",
    "        for language in result.translations:",
    "            append_to_transcript_file(result.translations[language], language)",
    "",
    "translate_speech()",
    "```"
  ],
  "leftOptions": [
    { "id": "1", "text": "Languages list" },
    { "id": "2", "text": "Recognizer initialization" }
  ],
  "rightOptions": [
    { "id": "a", "text": "(['fr', 'de'])" },
    { "id": "b", "text": "(['French', 'German'])" },
    { "id": "c", "text": "(['en-GB'])" },
    { "id": "d", "text": "(['languages'])" },
    { "id": "e", "text": "SpeechTranslator(" },
    { "id": "f", "text": "SpeechSynthesizer(" },
    { "id": "g", "text": "TranslationRecognizer(" },
    { "id": "h", "text": "SpeakerRecognizer(" }
  ],
  "correctAnswer": {
    "1": "a",
    "2": "g"
  },
  "explanation": "The target languages for translation are specified as language codes like ['fr', 'de']. The correct recognizer class to use for speech translation in Azure Speech SDK is TranslationRecognizer."
},{
  "id": "q84",
  "type": "matching-pairs",
  "question": [
    "A leading online education platform, \"LearnGo\", wants to create engaging video courses with personalized narration. They require a customized voice that resonates with their brand identity. The narration should be synthesized from scripted text.",
    "As a developer, design a solution using Azure AI Services to:",
    "1. Create a custom voice for LearnGo's narration.",
    "2. Generate audio narration from scripted text.",
    "",
    "Match the tasks with the appropriate Azure AI service or tool."
  ],
  "leftOptions": [
    { "id": "1", "text": "Create a custom voice for LearnGo's narration" },
    { "id": "2", "text": "Generate audio narration from scripted text" }
  ],
  "rightOptions": [
    { "id": "a", "text": "Microsoft Bot Framework Composer" },
    { "id": "b", "text": "The Azure portal" },
    { "id": "c", "text": "The Language Understanding portal" },
    { "id": "d", "text": "The Speech Studio portal" },
    { "id": "e", "text": "Language Understanding" },
    { "id": "f", "text": "Speaker Recognition" },
    { "id": "g", "text": "Speech-to-text" },
    { "id": "h", "text": "Text-to-speech" }
  ],
  "correctAnswer": {
    "1": "d",
    "2": "h"
  },
  "explanation": "To create a custom neural voice, you use the Speech Studio portal, which provides tools for voice customization and training. To generate narration from scripted text, Text-to-Speech service is used to synthesize audio from text."
},
  {
    "id": "q85",
    "type": "dropdown-pair",
  "question": [
    "You are developing a customer call handling solution which will receive calls from Spanish-speaking and Hindi-speaking callers. The system is supposed to perform the following tasks:",
    "- The inbound voice is to be captured as text",
    "- Replay the received messages in English.",
    "Which Azure AI Services should you use?"
  ],
    "leftOptions": [
    { "id": "a", "text": "Speaker Recognition" },
    { "id": "b", "text": "Speech-to-text" },
    { "id": "c", "text": "Text-to-speech" },
    { "id": "d", "text": "Translator" }
    ],
    "rightOptions": [
    { "id": "e", "text": "Speech-to-text only" },
    { "id": "f", "text": "Speech-to-text and Language" },
    { "id": "g", "text": "Speaker Recognition and Language" },
    { "id": "h", "text": "Text-to-speech and Language" },
    { "id": "i", "text": "Text-to-speech and Translator" }
    ],
    "correctAnswer": { "left": "b", "right": "i" },
     "explanation": "To capture messages from Spanish and Hindi speakers, the system should use Speech-to-text to transcribe the audio and Translator to convert it into English text. To replay the messages in English, use Text-to-speech to synthesize the translated English text back into audio, combined with Translator to ensure language conversion."
  },
  {
  "id": "q86",
  "type": "single",
  "question": [
    "You have an Azure subscription.",
    "You need to build an app that will compare documents for semantic similarity. The solution must meet the following requirements:",
    "• Return numeric vectors that represent the tokens of each document.",
    "• Minimize development effort.",
    "Which Azure OpenAI model should you use?"
  ],
  "options": [
    { "id": "a", "text": "GPT-3.5" },
    { "id": "b", "text": "GPT-4" },
    { "id": "c", "text": "embeddings" },
    { "id": "d", "text": "DALL-E" }
  ],
  "correctAnswer": "c",
  "explanation": "Embeddings models in Azure OpenAI are designed to return numeric vectors that represent the meaning of text. These vectors can then be compared to compute semantic similarity efficiently, making them ideal for document comparison tasks."
},
{
  "id": "q87",
  "type": "multiple",
  "question": [
    "You have an Azure AI Search service that has experienced a steady increase in query volume over the past 6 months.",
    "You notice that some search requests are being throttled.",
    "What possible action can you take to reduce query throttling?"
  ],
  "options": [
    { "id": "a", "text": "You add indexes." },
    { "id": "b", "text": "You enable customer-managed key (CMK) encryption." },
    { "id": "c", "text": "You add replicas." },
    { "id": "d", "text": "You migrate to an AI Search service that uses a higher tier." },
    { "id": "e", "text": "Deploy the application and a private endpoint to a virtual network." }
  ],
  "correctAnswer": ["c", "d"],
  "explanation": "To reduce query throttling in Azure AI Search, you can add **replicas** to handle more queries simultaneously, or **migrate to a higher tier** for increased capacity. Adding indexes, enabling CMK, or deploying to a private endpoint does not directly address throttling."
},
{
  "id": "q88",
  "type": "single",
  "question": [
    "You're developing a contact management chatbot using Azure AI Language Understanding.",
    "The FindContact intent helps users search for contacts by location.",
    "Your manager provides you with the following list of phrases to use for training the model:",
    "- Find contacts in North Virginia.",
    "- Who do I know in Sydney?",
    "- Search for contacts in Melbourne.",
    "You need to implement these phrases in Language Understanding to improve the model.",
    "What should you do?"
  ],
  "options": [
    { "id": "a", "text": "You create a new utterance for each phrase in the FindContact intent" },
    { "id": "b", "text": "You create a new intent for location." },
    { "id": "c", "text": "You create a new entity for the domain." },
    { "id": "d", "text": "You create a new pattern in the FindContact intent" }
  ],
  "correctAnswer": "a",
  "explanation": "To improve a specific intent like FindContact, you should add more training data by creating new utterances. Each phrase represents a variation in how users might phrase the same intent, helping the model generalize better. Location can be captured as an entity within those utterances."
},
{
  "id": "q89",
  "type": "dropdown-pair",
  "question": [
    "Your factory produces cardboard packaging for food products but experiences intermittent internet connectivity.",
    "Each package must contain four product samples, and you need a Custom Vision model to identify defects and verify product count while providing defect locations to an operator.",
    "Which project type and domain should you choose?"
  ],
  "leftOptions": [
    { "id": "classification", "text": "Classification" },
    { "id": "object-detection", "text": "Object Detection" }
  ],
  "rightOptions": [
    { "id": "retail", "text": "Retail" },
    { "id": "general-compact", "text": "General (compact)" },
    { "id": "general", "text": "General" },
    { "id": "manufacturing", "text": "Manufacturing" }
  ],
  "correctAnswer": {
    "left": "object-detection",
    "right": "general-compact"
  },
  "explanation": "To detect individual product samples and locate defects, 'Object Detection' is required. Since the environment has intermittent internet, the 'General (compact)' domain enables edge deployment on devices without needing constant cloud connectivity."
},
{
  "id": "q90",
  "type": "single",
  "question": [
    "You are developing a solution for a retail store that wants to monitor customer behavior in the store to improve sales and customer experience.",
    "The store has multiple cameras installed throughout and wants to gather data on:",
    "• How many people enter the store.",
    "• Which sections of the store get the most foot traffic.",
    "• How long customers spend in different areas.",
    "You need to analyze video feeds from the cameras to gather these insights in real time.",
    "Which Azure AI service feature should you use to achieve this?"
  ],
  "options": [
    { "id": "a", "text": "Speech-to-text in the Azure AI Speech service" },
    { "id": "b", "text": "Object detection in Azure AI Custom Vision" },
    { "id": "c", "text": "Spatial analysis in Azure AI Vision" },
    { "id": "d", "text": "Azure AI Video Indexer" }
  ],
  "correctAnswer": "c",
  "explanation": "Spatial analysis in Azure AI Vision is designed specifically for real-time video analytics in physical environments. It can track people, count entries, measure dwell time, and identify high-traffic areas—making it ideal for retail behavior analysis. The other options are not optimized for real-time spatial or behavioral video analysis."
},{
  "id": "q91",
  "type": "dropdown-pair",
  "question": [
    "You have a library that contains thousands of images.",
    "You need to tag the images as photographs, drawings, or clipart.",
    "Which service endpoint and response property should you use?",
    "NOTE: Each correct selection is worth one point."
  ],
  "leftOptions": [
    { "id": "doc-ocr", "text": "Document intelligence OCR extraction" },
    { "id": "custom-classification", "text": "Custom vision image classification" },
    { "id": "custom-detection", "text": "Custom vision object detection" },
    { "id": "ai-vision", "text": "AI Vision analyze images" }
  ],
  "rightOptions": [
    { "id": "metadata", "text": "Metadata" },
    { "id": "description", "text": "Description" },
    { "id": "tags", "text": "Tags" },
    { "id": "imagetype", "text": "Imagetype" },
    { "id": "objects", "text": "Objects" }
  ],
  "correctAnswer": {
    "left": "ai-vision",
    "right": "imagetype"
  },
  "explanation": "To determine image categories like photograph, drawing, or clipart, use the Azure AI Vision 'analyze image' API. This includes the 'imagetype' field, which provides image classification. Custom Vision is for custom models, while AI Vision offers built-in visual analysis features."
}, {
  "id": "q92",
  "type": "multiple",
  "question": [
    "A customer uses Azure AI Search and plans to enable server-side encryption using customer-managed keys (CMK) stored in Azure Key Vault.",
    "What are three implications of this change?"
  ],
  "options": [
    { "id": "a", "text": "The index size will increase." },
    { "id": "b", "text": "Query times will increase." },
    { "id": "c", "text": "A self-signed X.509 certificate is required." },
    { "id": "d", "text": "The index size will decrease." },
    { "id": "e", "text": "Query times will decrease." },
    { "id": "f", "text": "Azure Key Vault is required." }
  ],
  "correctAnswer": ["a", "b", "f"],
  "explanation": "When using customer-managed keys (CMK) for server-side encryption in Azure AI Search, integration with Azure Key Vault is mandatory to securely store and manage the encryption keys (f). Accessing CMK adds encryption/decryption overhead, which can lead to increased query latency (b). Additionally, encryption metadata and key wrapping mechanisms may increase the overall index size slightly (a). A self-signed certificate is not required; Azure Key Vault handles the key storage securely without that dependency."
},
{
  "id": "q93",
  "type": "single",
  "question": [
    "You have an Azure subscription and 1000 ASCII files.",
    "You need to identify files that contain specific phrases and the chosen solution must use cosine similarity.",
    "Which Azure OpenAI model should you use?"
  ],
  "options": [
    { "id": "a", "text": "text-embedding-ada-002" },
    { "id": "b", "text": "GPT-4" },
    { "id": "c", "text": "GPT-35 Turbo" },
    { "id": "d", "text": "GPT-4-32k" }
  ],
  "correctAnswer": "a",
  "explanation": "The `text-embedding-ada-002` model is specifically designed for generating vector embeddings that capture the semantic meaning of text. These embeddings can be compared using cosine similarity, making this model ideal for identifying documents with semantically similar content. GPT models are designed for text generation rather than efficient embedding and similarity comparison."
},{
  "id": "q94",
  "type": "drag-sequence",
  "question": [
    "You are building a retail chatbot that uses Custom Question Answering, a feature of Azure AI Language.",
    "You upload an internal support document to train the model. The document contains the following question: \"What is your return policy?\"",
    "Users report that the chatbot returns the default answer when they ask:",
    "\"Can I return a product after purchase?\"",
    "However, the chatbot returns the correct answer when users ask:",
    "\"What is your return policy?\"",
    "Both questions should return the same answer.",
    "You need to increase the accuracy of the chatbot responses.",
    "Which three actions should you perform in sequence?"
  ],
  "options": [
    { "id": "1", "text": "Add a new question and answer (QnA) pair." },
    { "id": "2", "text": "Retrain the model." },
    { "id": "3", "text": "Add additional questions to the document." },
    { "id": "4", "text": "Republish the model." },
    { "id": "5", "text": "Add alternative phrasing to the question and answer (QnA) pair." }
  ],
  "correctAnswer": ["5", "2", "4"],
  "explanation": "First, add alternative phrasing to the QnA pair so the model recognizes varied user questions. Then retrain and republish the model to apply these improvements."
},{
  "id": "q95",
  "type": "drag-sequence",
  "question": [
    "You have a chatbot that uses a custom Question Answering application.",
    "You enable active learning for the knowledge base used by the application.",
    "You need to integrate user input into the model.",
    "Which four actions should you perform in sequence?"
  ],
  "options": [
    { "id": "1", "text": "For the knowledge base, select Show active learning suggestions." },
    { "id": "2", "text": "Create a new model with alternate questions." },
    { "id": "3", "text": "Approve and reject suggestions." },
    { "id": "4", "text": "Save and train the knowledge base." },
    { "id": "5", "text": "Select the properties of the Azure Cognitive Services resource." },
    { "id": "6", "text": "Publish the knowledge base." },
    { "id": "7", "text": "Modify the automation task logic app to run an Azure Resource Manager template that creates the Azure Cognitive Services resource." }
  ],
  "correctAnswer": ["1", "3", "4", "6"],
  "explanation": "To integrate user input using active learning, first show active learning suggestions, then approve or reject them, save and train the knowledge base, and finally publish the updated knowledge base."
},{
  "id": "q96",
  "type": "drag-sequence",
  "question": [
    "Using Azure AI Services you have built a speech model which transcribes physics lectures.",
    "When you put the model for testing it is not able to transcribe scientific terms.",
    "How will you improve the accuracy of the model?",
    "Select the most appropriate five steps which you should perform in sequence to achieve the goal."
  ],
  "options": [
    { "id": "1", "text": "Create a project with inbuilt-scientific term detector." },
    { "id": "2", "text": "Deploy the model." },
    { "id": "3", "text": "Create a speech-to-text model." },
    { "id": "4", "text": "Create a Custom Speech project." },
    { "id": "5", "text": "Upload training datasets." },
    { "id": "6", "text": "Train the model." },
    { "id": "7", "text": "Create a Conversational Language Understanding model." }
  ],
  "correctAnswer": ["4", "3", "5", "6", "2"],
  "explanation": "To improve transcription accuracy for scientific terms, start by creating a Custom Speech project. Then create a speech-to-text model, upload domain-specific training datasets, train the model, and finally deploy it."
},
 {
  "id": "q97",
  "type": "single",
  "question": [
    "You have an application, utilizing a custom Azure AI Document Intelligence model to recognize contract documents.",
    "You now need to support an additional contract format.",
    "Requirements:",
    "Minimize development effort.",
    "Select one option to fulfill the requirement."
  ],
  "options": [
    { "id": "a", "text": "Create a new training set and add the new contract format to the new training set." },
    { "id": "b", "text": "Lower the accuracy threshold of App1." },
    { "id": "c", "text": "Use a pre-trained language model for image recognition." },
    { "id": "d", "text": "Add the additional contract format to the existing training set and retrain the model." },
    { "id": "e", "text": "Create and train a new custom model." }
  ],
  "correctAnswer": "d",
  "explanation": "To support a new contract format with minimal development effort, the best approach is to add examples of the new format to the existing training set and retrain the custom Document Intelligence model. This avoids building a new model from scratch while improving recognition accuracy for the new format."
},
{
  "id": "q98",
  "type": "single",
  "question": [
    "You are developing a solution to process receipt images using the Document Intelligence API.",
    "After uploading the images, the API returns a JSON response with extracted data, including various fields and their confidence scores.",
    "To ensure the accuracy of the extracted information, you need to establish a criterion that triggers a manual review by a designated team if any of the confidence scores for the extracted fields fall below a certain threshold.",
    "Given the JSON structure shown below, which expression should you use to trigger a manual review?",
    "{",
    "  \"documentResults\": [",
    "    {",
    "      \"docType\": \"prebuilt: receipt\",",
    "      \"pageRange\": [],",
    "      \"fields\": {",
    "        \"ReceiptType\": {",
    "          \"type\": \"string\",",
    "          \"valueString\": \"Itemized\",",
    "          \"confidence\": 0.672",
    "        },",
    "        \"MerchantName\": {",
    "          \"type\": \"string\",",
    "          \"valueString\": \"Tailwind\",",
    "          \"text\": \"Tailwind\",",
    "          \"boundingBox\": [],",
    "          \"page\": 1,",
    "          \"confidence\": 0.913,",
    "          \"elements\": []",
    "        }",
    "      }",
    "    }",
    "  ]",
    "}"
  ],
  "options": [
    { "id": "a", "text": "documentResults.fields.ReceiptType.confidence less than 0.5" },
    { "id": "b", "text": "documentResults.fields.ReceiptType.confidence greater than 0.7" },
    { "id": "c", "text": "documentResults.fields.*.confidence less than 0.7" },
    { "id": "d", "text": "documentResults.fields.MerchantName.valueString == \"Tailwind\"" }
  ],
  "correctAnswer": "c",
  "explanation": "To trigger manual review when any extracted field has a confidence score below a threshold, you need to check all fields dynamically. The expression 'documentResults.fields.*.confidence less than 0.7' captures this by evaluating all confidence values in the fields object."
},
{
  "id": "q99",
  "type": "single",
  "question": [
    "Which Azure AI Document Intelligence model is best suited to extract the following data from scanned documents:",
    "",
    "• Billing address",
    "• Due date",
    "• Subtotal",
    "• Shipping address",
    "• Total tax",
    "• Customer ID",
    "• Amount due"
  ],
  "options": [
    { "id": "a", "text": "Contract" },
    { "id": "b", "text": "Invoice" },
    { "id": "c", "text": "General document" },
    { "id": "d", "text": "Custom extraction model" }
  ],
  "correctAnswer": "b",
  "explanation": "The prebuilt 'Invoice' model in Azure AI Document Intelligence is specifically designed to extract common invoice fields such as billing address, due date, subtotal, shipping address, tax, customer ID, and amount due, making it the best fit with minimal development effort."
},{
  "id": "q100",
  "type": "multiple",
  "question": [
    "You are building a solution that students will use to find references for essays.",
    "You use the following code to start building the solution:",
    "",
    "```csharp",
    "using Azure;",
    "using System;",
    "using Azure.AI.TextAnalytics;",
    "",
    "private static readonly AzureKeyCredential credentials = new AzureKeyCredential(\"key\");",
    "private static readonly Uri endpoint = new Uri(\"endpoint\");",
    "",
    "static void EntityLinker (TextAnalyticsClient client)",
    "{",
    "    var response = client.RecognizeLinkedEntities(",
    "        \"Our tour guide took us up the Space Needle during our trip to Seattle last week.\");",
    "}",
    "```",
    "",
    "For each of the following statements, select **Yes** if the statement is true. Otherwise, select **No**."
  ],
  "options": [
    {
      "id": "a",
      "text": "The code will detect the language of documents.",
      "isCorrect": false
    },
    {
      "id": "b",
      "text": "The url attribute returned for each linked entity will be a Bing search link.",
      "isCorrect": false
    },
    {
      "id": "c",
      "text": "The matches attribute returned for each linked entity will provide the location in a document where the entity is referenced.",
      "isCorrect": true
    }
  ],
  "correctAnswer": ["c"],
  "explanation": "The `RecognizeLinkedEntities` method is used to link entities in the text to entries in a knowledge base such as Wikipedia. It does **not** detect language (you'd need a separate call for that), and the `url` is usually a link to a Wikipedia article, **not Bing**. The `matches` attribute identifies text spans where the entity appears, so option C is correct."
},
{
  "id": "q101",
  "type": "matching-pairs",
  "question": [
    "Match the actions with the correct method for accessing Azure OpenAI deployments."
  ],
  "leftOptions": [
    { "id": "1", "text": "Provide access to AI1dev by using" },
    { "id": "2", "text": "Connect to the deployment by using" }
  ],
  "rightOptions": [
    { "id": "a", "text": "A bearer token" },
    { "id": "b", "text": "An API key" },
    { "id": "c", "text": "An AD token" },
    { "id": "d", "text": "A deployment name" },
    { "id": "e", "text": "A deployment endpoint" }
  ],
  "correctAnswer": {
    "1": "b",
    "2": "d"
  },
  "explanation": "To access Azure OpenAI resources, you typically use an API key. Each app should connect using the specific deployment name configured in the Azure OpenAI Studio."
},{
  "id": "q102",
  "type": "single",
  "question": [
    "You are setting up an Azure OpenAI resource for a chatbot application using an Azure Resource Manager (ARM) template.",
    "The chatbot is expected to handle a significant volume of queries, approximately 600 requests per minute.",
    "You need to configure the Azure OpenAI resource to support 600 requests per minute.",
    "How would you adjust the deployment template to meet this requirement?"
  ],
  "options": [
    { "id": "a", "text": "count, 100" },
    { "id": "b", "text": "capacity, 600" },
    { "id": "c", "text": "capacity, 100" },
    { "id": "d", "text": "size, 1" }
  ],
  "correctAnswer": "c",
  "explanation": "In Azure OpenAI ARM templates, the `capacity` field under `sku` defines the number of compute units allocated. Setting `capacity: 100` enables the resource to handle higher throughput, such as 600 requests per minute, depending on the model and request complexity."
},{
  "id": "q103",
  "type": "single",
  "question": [
    "You've integrated Azure OpenAI's GPT-4 model into your application, MyApp.",
    "MyApp occasionally returns responses containing hate speech.",
    "You need to prevent MyApp from generating responses with hate speech.",
    "How do you achieve the goal?"
  ],
  "options": [
    { "id": "a", "text": "the Temperature parameter" },
    { "id": "b", "text": "a content filter" },
    { "id": "c", "text": "abuse monitoring" },
    { "id": "d", "text": "the Frequency penalty parameter" }
  ],
  "correctAnswer": "b",
  "explanation": "To prevent harmful or inappropriate outputs such as hate speech, Azure OpenAI provides built-in content filtering. The content filter analyzes and blocks inappropriate content before it's returned to the user. Parameters like Temperature or Frequency penalty affect creativity and repetition but do not prevent unsafe content."
},{
  "id": "q104",
  "type": "single",
  "question": [
    "You're developing an e-commerce platform that requires automated generation of product titles and descriptions from existing product specifications.",
    "Which Azure AI model is best suited for generating concise and relevant product titles from detailed product descriptions?"
  ],
  "options": [
    { "id": "a", "text": "Embeddings" },
    { "id": "b", "text": "DALL·E" },
    { "id": "c", "text": "Whisper" },
    { "id": "d", "text": "GPT-4" }
  ],
  "correctAnswer": "d",
  "explanation": "GPT-4 is a large language model optimized for natural language generation tasks such as summarization, content generation, and rephrasing. It is best suited for transforming detailed product specs into concise, human-readable titles and descriptions. Embeddings are used for similarity search, DALL·E for image generation, and Whisper for speech recognition."
},
{
  "id": "q105",
  "type": "single",
  "question": [
    "You are developing the shopping on-the-go project.",
    "You need to build the Adaptive Card for the chatbot.",
    "How should you complete Blank1 in the following code snippet?",
    "",
    "\"$schema\": \"http://adaptivecards.io/schemas/adaptive-card.json\",",
    "\"type\": \"AdaptiveCard\",",
    "\"version\": \"1.3\",",
    "\"body\": [",
    "  {",
    "    \"type\": \"TextBlock\",",
    "    \"size\": \"Medium\",",
    "    \"weight\": \"Bolder\",",
    "    \"text\": \"${_}\"   <- Blank1",
    "  },",
    "  {",
    "    \"type\": \"TextBlock\",",
    "    (Blank2),",
    "    \"color\": \"Attention\"",
    "  },",
    "  {",
    "    \"type\": \"Image\",",
    "    \"url\": \"${image.uri}\",",
    "    \"size\": \"Medium\",",
    "    \"altText\": \"${_}\"",
    "  }",
    "]"
  ],
  "options": [
    { "id": "a", "text": "If(language 'en', 'en', name)" },
    { "id": "b", "text": "name" },
    { "id": "c", "text": "name.en" },
    { "id": "d", "text": "name[language]" }
  ],
  "correctAnswer": "d",
  "explanation": "The correct expression to localize text dynamically in Adaptive Cards is 'name[language]'. This accesses the correct localized value of the 'name' field based on the current language setting."
},
{
  "id": "q106",
  "type": "single",
  "question": [
    "You are developing the shopping on-the-go project.",
    "You need to build the Adaptive Card for the chatbot.",
    "How should you complete Blank2 in the following code snippet?",
    "",
    "{",
    "  \"$schema\": \"http://adaptivecards.io/schemas/adaptive-card.json\",",
    "  \"type\": \"AdaptiveCard\",",
    "  \"version\": \"1.3\",",
    "  \"body\": [",
    "    {},",
    "    {",
    "      \"type\": \"TextBlock\",",
    "      \"size\": \"Medium\",",
    "      \"weight\": \"Bolder\",",
    "      \"text\": \"${...}\",",
    "      \"$when\": Blank2,",
    "      \"color\": \"Attention\"",
    "    },",
    "    {",
    "      \"type\": \"Image\",",
    "      \"url\": \"${image.uri}\",",
    "      \"size\": \"Medium\",",
    "      \"altText\": \"${...}\"",
    "    }",
    "  ]",
    "}"
  ],
  "options": [
    { "id": "a", "text": "stockLevel != 'OK'" },
    { "id": "b", "text": "$[stockLevel -- \"OK\"]" },
    { "id": "c", "text": "$[stockLevel.OK)" }
  ],
  "correctAnswer": "a",
  "explanation": "In Adaptive Cards, the `$when` property uses a logical expression to determine if an element should be rendered. To conditionally render an element when the `stockLevel` is not equal to `'OK'`, the correct syntax is `stockLevel != 'OK'`. This is evaluated at runtime by the Adaptive Card host environment."
},{
  "id": "q107",
  "type": "single",
  "question": [
    "You are developing the shopping on-the-go project.",
    "You need to build the Adaptive Card for the chatbot.",
    "How should you complete Blank3 for the following code snippet?",
    "",
    "{",
    "  \"$schema\": \"http://adaptivecards.io/schemas/adaptive-card.json\",",
    "  \"type\": \"AdaptiveCard\",",
    "  \"version\": \"1.3\",",
    "  \"body\": [",
    "    {",
    "      \"type\": \"TextBlock\",",
    "      \"size\": \"Medium\",",
    "      \"weight\": \"Bolder\",",
    "      \"text\": \"${...}\"",
    "    },",
    "    {",
    "      \"type\": \"TextBlock\",",
    "      \"$when\": \"...\",",
    "      \"color\": \"Attention\"",
    "    },",
    "    {",
    "      \"type\": \"Image\",",
    "      \"url\": \"${image.uri}\",",
    "      \"size\": \"Medium\",",
    "      \"altText\": \"${Blank3}\"",
    "    }",
    "  ]",
    "}"
  ],
  "options": [
    { "id": "a", "text": "image.altText.en" },
    { "id": "b", "text": "image.altText.language" },
    { "id": "c", "text": "image.altText.[language]" },
    { "id": "d", "text": "image.altText[language]" }
  ],
  "correctAnswer": "d",
  "explanation": "In Adaptive Cards, to access a dynamic property like a language-specific key from an object (e.g., `image.altText`), bracket notation is used. `image.altText[language]` accesses the property where `language` is a variable holding the key (e.g., 'en', 'fr'). Dot notation does not evaluate dynamic keys."
},{
  "id": "q108",
  "type": "single",
  "question": [
    "You are developing the shopping on-the-go project.",
    "You are configuring access to the QnA Maker (classic) resources.",
    "Which role should you assign to AllUsers?"
  ],
  "options": [
    { "id": "a", "text": "Cognitive Services User" },
    { "id": "b", "text": "Contributor" },
    { "id": "c", "text": "Owner" },
    { "id": "d", "text": "QnA Maker Editor" },
    { "id": "e", "text": "QnA Maker Reader" }
  ],
  "correctAnswer": "d",
  "explanation": "The 'QnA Maker Editor' role provides the necessary permissions to create, update, delete, and publish QnA Maker knowledge bases. It is specifically designed for users who manage content in QnA Maker (classic). This is the most appropriate role to assign to users who need authoring access without granting broader access like Contributor or Owner."
},{
  "id": "q109",
  "type": "single",
  "question": [
    "You are developing the shopping on-the-go project.",
    "You are configuring access to the QnA Maker (classic) resources.",
    "Which role should you assign to Leadership Team?"
  ],
  "options": [
    { "id": "a", "text": "Cognitive Service User" },
    { "id": "b", "text": "Contributor" },
    { "id": "c", "text": "Owner" },
    { "id": "d", "text": "QnA Maker Editor" },
    { "id": "e", "text": "QnA Maker Read" }
  ],
  "correctAnswer": "a",
  "explanation": "The 'Cognitive Services User' role is required for users to access and use QnA Maker (classic) runtime endpoints. This includes the ability to query the knowledge base via the bot or application, making it suitable for stakeholders like the Leadership Team who need to interact with the QnA bot but do not require authoring permissions."
},{
  "id": "q110",
  "type": "multiple",
  "question": [
    "You are developing the smart e-commerce project.",
    "You need to implement autocompletion as part of the Cognitive Search solution.",
    "Which three actions should you perform?"
  ],
  "options": [
    {
      "id": "a",
      "text": "Make API queries to the autocomplete endpoint and include suggesterName in the body"
    },
    {
      "id": "b",
      "text": "Add a suggester that has the three product name fields as source fields"
    },
    {
      "id": "c",
      "text": "Make API queries to the search endpoint and include the product name fields in the searchFields query parameter"
    },
    {
      "id": "d",
      "text": "Add a suggester for each of the three product name fields"
    },
    {
      "id": "e",
      "text": "Set the searchAnalyzer property for the three product name variants"
    },
    {
      "id": "f",
      "text": "Set the analyzer property for the three product name variants"
    }
  ],
  "correctAnswer": ["a", "b", "f"],
  "explanation": [
    "To enable **autocomplete** in Azure Cognitive Search:",
    "- You need to **make API queries to the autocomplete endpoint** and pass the `suggesterName` in the query (Option A).",
    "- You must **add a suggester** to the index, listing the fields (e.g., product names) to be used for suggestions (Option B).",
    "- For proper text tokenization and indexing, set the **`analyzer` property** on the suggester fields, such as using `standard.lucene` or a custom analyzer (Option F).",
    "",
    "❌ Option C is for full text search, not autocomplete.",
    "❌ Option D is incorrect because **only one suggester is allowed per index**.",
    "❌ Option E (`searchAnalyzer`) is used for query-time analysis in search, not in suggesters."
  ]
},{
  "id": "q111",
  "type": "single",
  "question": [
    "You are planning the product creation project.",
    "You need to build the REST endpoint to create the multilingual product descriptions.",
    "How should you complete Blank 1 of the following URI?"
  ],
  "options": [
    {
      "id": "a",
      "text": "api.cognitive.microsofttranslator.com"
    },
    {
      "id": "b",
      "text": "api-nam.cognitive.microsofttranslator.com"
    },
    {
      "id": "c",
      "text": "westus.tts.speech.microsoft.com"
    },
    {
      "id": "d",
      "text": "wwics.cognitiveservices.azure.com/translator"
    }
  ],
  "correctAnswer": "b",
  "explanation": [
    "The correct endpoint to use for Microsoft Translator REST API in the **North America (NAM)** region is:",
    "`https://api-nam.cognitive.microsofttranslator.com`",
    "",
    "- ✅ Option B is correct: It is the region-specific endpoint for Translator API.",
    "- ❌ Option A is outdated and no longer recommended.",
    "- ❌ Option C is related to Text-to-Speech, not translation.",
    "- ❌ Option D is an invalid or misformatted endpoint."
  ]
},{
  "id": "q112",
  "type": "single",
  "question": [
    "You are planning the product creation project.",
    "You need to build the REST endpoint to create the multilingual product descriptions.",
    "How should you complete Blank 2 of the following URI?"
  ],
  "options": [
    {
      "id": "a",
      "text": "detect"
    },
    {
      "id": "b",
      "text": "languages"
    },
    {
      "id": "c",
      "text": "text-to-speech"
    },
    {
      "id": "d",
      "text": "translate"
    }
  ],
  "correctAnswer": "d",
  "explanation": [
    "The REST endpoint for creating multilingual product descriptions involves the translation service.",
    "- ✅ Option D 'translate' is correct because it is the endpoint for translation operations.",
    "- ❌ Option A 'detect' is for language detection, not translation.",
    "- ❌ Option B 'languages' is for listing supported languages.",
    "- ❌ Option C 'text-to-speech' is unrelated to translation."
  ]
},{
  "id": "q113",
  "type": "single",
  "question": [
    "You need to develop code to upload images for the product creation project.",
    "The solution must meet the accessibility requirements.",
    "How should you fill Blank 1 of the following code?"
  ],
  "options": [
    {
      "id": "a",
      "text": "Dictionary"
    },
    {
      "id": "b",
      "text": "stream"
    },
    {
      "id": "c",
      "text": "string"
    }
  ],
  "correctAnswer": "c",
  "explanation": [
    "Blank 1 represents the parameter type for the image input to the AnalyzeImageAsync method.",
    "- ✅ Option C 'string' is correct because the image is typically passed as a URL string.",
    "- ❌ Option A 'Dictionary' is not a valid image input type for this method.",
    "- ❌ Option B 'stream' is used for binary image data but is not the correct choice for this specific code snippet."
  ]
},{
  "id": "q114",
  "type": "single",
  "question": [
    "You need to develop code to upload images for the product creation project.",
    "The solution must meet the accessibility requirements.",
    "How should you fill Blank 2 of the following code?"
  ],
  "options": [
    {
      "id": "a",
      "text": "VisualFeatureTypes.Description"
    },
    {
      "id": "b",
      "text": "VisualFeatureTypes.ImageType"
    },
    {
      "id": "c",
      "text": "VisualFeatureTypes.Objects"
    },
    {
      "id": "d",
      "text": "VisualFeatureTypes.Tags"
    }
  ],
  "correctAnswer": "a",
  "explanation": [
    "Blank 2 is where you specify which visual features to analyze in the image.",
    "- ✅ Option A 'VisualFeatureTypes.Description' is correct because it enables generating image descriptions necessary for alt text.",
    "- ❌ Option B 'VisualFeatureTypes.ImageType' identifies the type of image but does not provide description.",
    "- ❌ Option C 'VisualFeatureTypes.Objects' detects objects but doesn't provide a description summary.",
    "- ❌ Option D 'VisualFeatureTypes.Tags' provides tags but not descriptive text suitable for alt text."
  ]
},{
  "id": "q115",
  "type": "single",
  "question": [
    "You need to develop code to upload images for the product creation project.",
    "The solution must meet the accessibility requirements.",
    "How should you fill Blank 3 of the following code?"
  ],
  "options": [
    {
      "id": "a",
      "text": "var c = results.Brands.DetectedBrands[0]"
    },
    {
      "id": "b",
      "text": "var c = results.Description.Captions[0]"
    },
    {
      "id": "c",
      "text": "var c = results.Metadata[0]"
    },
    {
      "id": "d",
      "text": "var c = results.Objects[0]"
    }
  ],
  "correctAnswer": "b",
  "explanation": [
    "Blank 3 is where you extract the best caption for the image description.",
    "- ✅ Option B 'var c = results.Description.Captions[0]' is correct because the 'Description.Captions' contains descriptive captions with confidence scores.",
    "- ❌ Option A 'Brands.DetectedBrands' relates to brand detection, not image description.",
    "- ❌ Option C 'Metadata' contains image metadata such as dimensions, not descriptions.",
    "- ❌ Option D 'Objects' provides object detection results, not descriptive captions."
  ]
},{
  "id": "q116",
  "type": "single",
  "question": [
    "You are planning the product creation project.",
    "You need to recommend a process for analyzing videos.",
    "Which four actions should you perform in sequence?"
  ],
  "options": [
    {
      "id": "a",
      "text": "Upload the video to blob storage --> Index the video by using the Azure Video Analyzer for Media (previously Video Indexer) API --> Send the transcript to the Language Understanding API as an utterance --> Translate the transcript by using the Translator API"
    },
    {
      "id": "b",
      "text": "Upload the video to blob storage --> Index the video by using the Azure Video Analyzer for Media (previously Video Indexer) API --> Extract the transcript from the Azure Video Analyzer for Media (previously Video Indexer) API --> Translate the transcript by using the Translator API"
    },
    {
      "id": "c",
      "text": "Upload the video to blob storage --> Analyze the video by using the Computer Vision API --> Extract the transcript from Microsoft Stream --> Translate the transcript by using the Translator API"
    },
    {
      "id": "d",
      "text": "Upload the video to blob storage --> Analyze the video by using the Computer Vision API --> Extract the transcript from the Azure Video Analyzer for Media (previously Video Indexer) API --> Translate the transcript by using the Translator API"
    }
  ],
  "correctAnswer": "b",
  "explanation": [
    "The correct sequence for analyzing videos is:",
    "1. Upload the video to blob storage.",
    "2. Index the video using Azure Video Analyzer for Media (formerly Video Indexer).",
    "3. Extract the transcript from Azure Video Analyzer for Media API.",
    "4. Translate the transcript using the Translator API.",
    "- Option B correctly describes this process.",
    "- Options A, C, and D include incorrect steps or incorrect order."
  ]
},{
  "id": "q117",
  "type": "single",
  "question": [
    "Which expression should you use to trigger a manual review of the extracted information by a member of the Consultant-Bookkeeper?"
  ],
  "options": [
    {
      "id": "a",
      "text": "documentResults.docType == \"prebuilt:receipt\""
    },
    {
      "id": "b",
      "text": "documentResults.fields..confidence < 0.7"
    },
    {
      "id": "c",
      "text": "documentResults.fields.ReceiptType.confidence > 0.7"
    },
    {
      "id": "d",
      "text": "documentResults.fields.MerchantName.confidence < 0.7"
    }
  ],
  "correctAnswer": "b",
  "explanation": [
    "To trigger a manual review, you want to flag documents where the confidence of the extracted fields is below a certain threshold.",
    "- ✅ Option B 'documentResults.fields..confidence < 0.7' is correct as it checks for low confidence in any field.",
    "- ❌ Option A checks only the document type and not confidence.",
    "- ❌ Option C checks for high confidence, which does not trigger a review.",
    "- ❌ Option D checks confidence on a single field only; broader check is better."
  ]
},{
  "id": "q118",
  "type": "single",
  "question": [
    "You are developing the knowledgebase by using Azure Cognitive Search.",
    "You need to meet the knowledgebase requirements for searching equivalent terms.",
    "What should you include in the solution?"
  ],
  "options": [
    {
      "id": "a",
      "text": "synonym map"
    },
    {
      "id": "b",
      "text": "a suggester"
    },
    {
      "id": "c",
      "text": "a custom analyzer"
    },
    {
      "id": "d",
      "text": "a built-in key phrase extraction skill"
    }
  ],
  "correctAnswer": "a",
  "explanation": [
    "To enable searching for equivalent terms, you should use a synonym map.",
    "- ✅ Option A 'synonym map' allows you to define synonyms so searches for one term include results for its equivalents.",
    "- ❌ Option B 'a suggester' provides autocomplete suggestions, not synonym matching.",
    "- ❌ Option C 'a custom analyzer' processes text but does not handle synonyms specifically.",
    "- ❌ Option D 'key phrase extraction skill' extracts important phrases but does not affect term equivalency in search."
  ]
},{
  "id": "q119",
  "type": "single",
  "question": [
    "You are developing the knowledgebase by using Azure Cognitive Search.",
    "You need to process wiki content to meet the technical requirements.",
    "What should you include in the solution?"
  ],
  "options": [
    {
      "id": "a",
      "text": "an indexer for Azure Blob storage attached to a skillset that contains the language detection skill and the text translation skill"
    },
    {
      "id": "b",
      "text": "an indexer for Azure Blob storage attached to a skillset that contains the language detection skill"
    },
    {
      "id": "c",
      "text": "an indexer for Azure Cosmos DB attached to a skillset that contains the document extraction skill and the text translation skill"
    },
    {
      "id": "d",
      "text": "an indexer for Azure Cosmos DB attached to a skillset that contains the language detection skill and the text translation skill"
    }
  ],
  "correctAnswer": "d",
  "explanation": [
    "For processing wiki content stored in Azure Cosmos DB, using an indexer attached to Cosmos DB is appropriate.",
    "- ✅ Option D uses an indexer for Azure Cosmos DB with a skillset that contains language detection and text translation, meeting the multilingual and content processing needs.",
    "- ❌ Options A and B use Blob storage, which may not align with where the wiki content is stored.",
    "- ❌ Option C uses document extraction skill which may not be necessary here."
  ]
},{
  "id": "q120",
  "type": "multiple",
  "question": [
    "You are developing the knowledgebase by using Azure Cognitive Search.",
    "You need to build a skill that will be used by indexers.",
    "How should you fill in the blank for Line1? (Choose 3)"
  ],
  "options": [
    {
      "id": "a",
      "text": "Email"
    },
    {
      "id": "b",
      "text": "Person"
    },
    {
      "id": "c",
      "text": "Organization"
    },
    {
      "id": "d",
      "text": "Location"
    },
    {
      "id": "e",
      "text": "null"
    }
  ],
  "correctAnswer": ["b", "c", "d"],
  "explanation": [
    "For the EntityRecognitionSkill in Azure Cognitive Search, the common categories to include are:",
    "- ✅ 'Person' to extract person entities.",
    "- ✅ 'Organization' to extract organization entities.",
    "- ✅ 'Location' to extract location entities.",
    "- ❌ 'Email' is not a typical category for this skill.",
    "- ❌ 'null' is invalid and should not be used."
  ]
},{
  "id": "q121",
  "type": "multiple",
  "question": [
    "You are developing the document processing workflow.",
    "You need to identify which API endpoints to use that meet the document processing requirements.",
    "Which two API endpoints should you identify?"
  ],
  "options": [
    {
      "id": "a",
      "text": "/vision/read/analyze"
    },
    {
      "id": "b",
      "text": "/formrecognizer/v2.0/custom/models/{modelid}/analyze"
    },
    {
      "id": "c",
      "text": "/formrecognizer/v2.0/prebuilt/receipt/analyze"
    },
    {
      "id": "d",
      "text": "/vision/v3.1/describe"
    },
    {
      "id": "e",
      "text": "/vision/v3.1/describe/analyzeresult"
    }
  ],
  "correctAnswer": ["a", "b"],
  "explanation": [
    "For document processing, the following endpoints are commonly used:",
    "- ✅ Option A '/vision/read/analyze' is used for OCR and text extraction.",
    "- ✅ Option B '/formrecognizer/v2.0/custom/models/{modelid}/analyze' is used for analyzing documents with custom Form Recognizer models.",
    "- ❌ Option C '/formrecognizer/v2.0/prebuilt/receipt/analyze' is for receipts only, not general documents.",
    "- ❌ Options D and E are for image description, not document processing."
  ]
},{
  "id": "q122",
  "type": "drag-sequence",
  "question": [
    "You are developing a solution for the Management-Bookkeepers group to meet the document processing requirements.",
    "The solution must contain a Form Recognizer resource and an Azure web app that hosts the Form Recognizer sample labeling tool.",
    "The Management-Bookkeepers group needs to create a custom table extractor by using the sample labeling tool.",
    "Which three actions should the Management-Bookkeepers group perform in sequence?"
  ],
  "options": [
    {
      "id": "a",
      "text": "Train custom model → Label the sample documents → Create a new project and load sample documents"
    },
    {
      "id": "b",
      "text": "Create a new project and load sample documents → Train custom model → Create a composite model"
    },
    {
      "id": "c",
      "text": "Create a new project and load sample documents → Create a composite model → Label the sample documents"
    },
    {
      "id": "d",
      "text": "Create a new project and load sample documents → Label the sample documents → Train a custom model"
    }
  ],
  "correctAnswer": "d",
  "explanation": [
    "The correct workflow for creating a custom table extractor using the Form Recognizer sample labeling tool is:",
    "1. Create a new project and load sample documents to start the labeling process.",
    "2. Label the sample documents to define the data to extract.",
    "3. Train a custom model based on the labeled data.",
    "- Option D correctly represents this sequence.",
    "- Options A, B, and C are out of order or include unrelated steps."
  ]
},{
  "id": "q123",
  "type": "single",
  "question": [
    "You are developing the knowledgebase.",
    "You use Azure Video Indexer (previously Video Analyzer for Media) to obtain transcripts of webinars.",
    "You need to ensure that the solution meets the knowledgebase requirements.",
    "What should you do?"
  ],
  "options": [
    {
      "id": "a",
      "text": "Create a custom language model"
    },
    {
      "id": "b",
      "text": "Configure audio indexing for videos only"
    },
    {
      "id": "c",
      "text": "Enable multilanguage detection for videos"
    },
    {
      "id": "d",
      "text": "Build a custom Person model for webinar presenters"
    }
  ],
  "correctAnswer": "a",
  "explanation": [
    "Creating a custom language model improves the accuracy of transcription by adapting to domain-specific vocabulary.",
    "- ✅ Option A 'Create a custom language model' is correct to meet transcript quality and relevance requirements.",
    "- ❌ Options B, C, and D do not directly improve transcription quality for the knowledgebase."
  ]
},{
  "id": "q124",
  "type": "single",
  "question": [
    "Which Computer Vision client library feature is used to analyze image description?"
  ],
  "options": [
    {
      "id": "a",
      "text": "VisualFeatureTypes.Description"
    },
    {
      "id": "b",
      "text": "VisualFeatureTypes.Tags"
    },
    {
      "id": "c",
      "text": "VisualFeatureTypes.Categories"
    },
    {
      "id": "d",
      "text": "VisualFeatureTypes.Color"
    }
  ],
  "correctAnswer": "a",
  "explanation": [
    "The 'Description' feature extracts a textual description of the image content.",
    "- ✅ Option A 'VisualFeatureTypes.Description' is correct for analyzing image descriptions.",
    "- ❌ Other options provide metadata or tags, but not a description."
  ]
},{
  "id": "q125",
  "type": "single",
  "question": [
    "Which Computer Vision client library feature is used to analyze image tags?"
  ],
  "options": [
    {
      "id": "a",
      "text": "VisualFeatureTypes.Description"
    },
    {
      "id": "b",
      "text": "VisualFeatureTypes.Tags"
    },
    {
      "id": "c",
      "text": "VisualFeatureTypes.Categories"
    },
    {
      "id": "d",
      "text": "VisualFeatureTypes.Color"
    }
  ],
  "correctAnswer": "b",
  "explanation": [
    "The 'Tags' feature provides tags (keywords) describing the content of an image.",
    "- ✅ Option B 'VisualFeatureTypes.Tags' is correct for analyzing image tags.",
    "- ❌ Other options relate to description, categories, or color, not tags."
  ]
}
,{
  "id": "q126",
  "type": "single",
  "question": [
    "How to read a file from local file system?"
  ],
  "options": [
    {
      "id": "a",
      "text": "File.OpenRead"
    },
    {
      "id": "b",
      "text": "LocalFile.Read"
    },
    {
      "id": "c",
      "text": "ReadFile.Local"
    },
    {
      "id": "d",
      "text": "LocalRead.OpenFile"
    }
  ],
  "correctAnswer": "a",
  "explanation": [
    "`File.OpenRead` is the correct .NET method to open a file stream for reading from the local file system.",
    "- ✅ Option A is correct.",
    "- ❌ Other options are invalid or do not exist in .NET."
  ]
},{
  "id": "q127",
  "type": "single",
  "question": [
    "Which Azure Service is best suited for analyzing visual content in images & videos to categorize content?"
  ],
  "options": [
    {
      "id": "a",
      "text": "Azure AI Vision"
    },
    {
      "id": "b",
      "text": "Azure Cosmos DB"
    },
    {
      "id": "c",
      "text": "Azure Cognitive Search"
    },
    {
      "id": "d",
      "text": "Azure Functions"
    }
  ],
  "correctAnswer": "a",
  "explanation": [
    "Azure AI Vision provides prebuilt AI models to analyze images and videos, extract information, and categorize visual content.",
    "- ✅ Option A 'Azure AI Vision' is the best suited for analyzing visual content.",
    "- ❌ Option B 'Azure Cosmos DB' is a globally distributed database service, not for image/video analysis.",
    "- ❌ Option C 'Azure Cognitive Search' is for text search and indexing, not primarily for visual content analysis.",
    "- ❌ Option D 'Azure Functions' is a serverless compute service and does not analyze visual content directly."
  ]
}
,{
  "id": "q128",
  "type": "single",
  "question": [
    "Which of the following best describes the predictions made by a machine learning model?"
  ],
  "options": [
    {
      "id": "a",
      "text": "Probabilistic values based on correlations found in training data"
    },
    {
      "id": "b",
      "text": "Randomly selected values with an equal chance of selection"
    },
    {
      "id": "c",
      "text": "Absolutely correct values based on conditional logic"
    }
  ],
  "correctAnswer": "a",
  "explanation": [
    "Machine learning models make predictions based on learned correlations in training data and provide probabilistic outputs.",
    "- ✅ Option A is correct because predictions are typically probabilistic, reflecting uncertainty.",
    "- ❌ Option B is incorrect as predictions are not random.",
    "- ❌ Option C is incorrect because ML models do not produce absolute or deterministic values based solely on logic."
  ]
}
,{
  "id": "q129",
  "type": "single",
  "question": [
    "To enhance security, which of the following steps should be taken to protect account keys for Azure AI Services?"
  ],
  "options": [
    {
      "id": "a",
      "text": "Store account keys in plaintext file on server"
    },
    {
      "id": "b",
      "text": "Store account keys in public repository"
    },
    {
      "id": "c",
      "text": "Store account keys in environment variable"
    },
    {
      "id": "d",
      "text": "Store keys in Azure Key Vault"
    }
  ],
  "correctAnswer": "d",
  "explanation": [
    "Storing keys in Azure Key Vault ensures secure, centralized, and controlled access to secrets.",
    "- ✅ Option D is correct and best practice.",
    "- ❌ Option A and B expose keys to security risks.",
    "- ❌ Option C is better than plaintext files but still less secure than Key Vault."
  ]
}
,{
  "id": "q130",
  "type": "single",
  "question": [
    "You want to index a collection of text documents, and search them from a mobile application.",
    "Which service should you use to create the index?"
  ],
  "options": [
    {
      "id": "a",
      "text": "Azure AI Service"
    },
    {
      "id": "b",
      "text": "Azure OpenAI Service"
    },
    {
      "id": "c",
      "text": "Azure AI Search"
    },
    {
      "id": "d",
      "text": "Azure AI Index Service"
    }
  ],
  "correctAnswer": "c",
  "explanation": [
    "Azure AI Search (Azure Cognitive Search) is the service designed for indexing and searching documents.",
    "- ✅ Option C is correct.",
    "- ❌ Other options do not provide indexing and search capabilities."
  ]
}
,{
  "id": "q131",
  "type": "single",
  "question": [
    "A data scientist has used Azure Machine Learning to train a machine learning model.",
    "How can you use the model in your application?"
  ],
  "options": [
    {
      "id": "a",
      "text": "Use Azure Machine Learning to publish the model as a web service"
    },
    {
      "id": "b",
      "text": "Export the model as an Azure AI service"
    },
    {
      "id": "c",
      "text": "You must build your application using the Azure Machine Learning designer"
    }
  ],
  "correctAnswer": "a",
  "explanation": [
    "Publishing the model as a web service enables your application to consume the model via REST APIs.",
    "- ✅ Option A is correct.",
    "- ❌ Option B is incorrect because you cannot export the model as an Azure AI service directly.",
    "- ❌ Option C is incorrect because using Azure ML Designer is not mandatory to consume the model."
  ]
}
,{
  "id": "q132",
  "type": "single",
  "question": [
    "When configuring diagnostic logging for Azure AI Services, which of the below actions are necessary?"
  ],
  "options": [
    {
      "id": "a",
      "text": "Define metrics to be collected"
    },
    {
      "id": "b",
      "text": "Enable Diagnostic settings & Configure log destination"
    },
    {
      "id": "c",
      "text": "Set up data export schedule"
    }
  ],
  "correctAnswer": "b",
  "explanation": [
    "To enable diagnostic logging, you must enable diagnostic settings and configure where the logs will be sent (log destination).",
    "- ✅ Option B is correct.",
    "- ❌ Option A is optional and depends on the service.",
    "- ❌ Option C may be part of advanced setup but is not necessary initially."
  ]
}
,{
  "id": "q133",
  "type": "single",
  "question": [
    "You build a bot by using the Microsoft Bot Framework SDK.",
    "You need to validate the functionality of the bot.",
    "What should you do before you connect to the bot?"
  ],
  "options": [
    {
      "id": "a",
      "text": "Run the Bot Framework Emulator"
    },
    {
      "id": "b",
      "text": "Run the Bot Framework Composer"
    },
    {
      "id": "c",
      "text": "Register the bot with Azure Bot Service"
    },
    {
      "id": "d",
      "text": "Run Windows Terminal"
    }
  ],
  "correctAnswer": "a",
  "explanation": [
    "Running the Bot Framework Emulator allows you to test and validate your bot's functionality locally before deploying or connecting it.",
    "- ✅ Option A is correct.",
    "- ❌ Option B is a tool for building bots but not primarily used for validating functionality before connection.",
    "- ❌ Option C is for registration and deployment, not local testing.",
    "- ❌ Option D is a terminal and does not provide bot validation capabilities."
  ]
},{
  "id": "q134",
  "type": "single",
  "question": [
    "Which Azure Service provides capability to understand human language, sentiment analysis, and language translation?"
  ],
  "options": [
    {
      "id": "a",
      "text": "Azure AI Document Intelligence"
    },
    {
      "id": "b",
      "text": "Azure AI Decision"
    },
    {
      "id": "c",
      "text": "Azure AI Bot Service"
    },
    {
      "id": "d",
      "text": "Azure AI Language"
    }
  ],
  "correctAnswer": "d",
  "explanation": [
    "Azure AI Language provides capabilities for natural language understanding, sentiment analysis, and language translation.",
    "- ✅ Option D is correct.",
    "- ❌ Options A, B, and C provide different AI functionalities but not comprehensive natural language processing."
  ]
}
,{
  "id": "q135",
  "type": "single",
  "question": [
    "When designing a system to provide personalized recommendations, which service would be most appropriate?"
  ],
  "options": [
    {
      "id": "a",
      "text": "Azure Functions"
    },
    {
      "id": "b",
      "text": "Azure AI Personalizer"
    },
    {
      "id": "c",
      "text": "Azure Machine Learning"
    }
  ],
  "correctAnswer": "b",
  "explanation": [
    "Azure AI Personalizer is a service specifically designed to provide personalized user experiences by learning from user interactions.",
    "- ✅ Option B is correct.",
    "- ❌ Option A is a serverless compute service, not specialized for personalization.",
    "- ❌ Option C is a general ML platform but does not provide ready-to-use personalization out of the box."
  ]
}
,{
  "id": "q136",
  "type": "single",
  "question": [
    "You are building a solution in Azure that will use Azure Cognitive Service for Language to process sensitive customer data.",
    "You need to ensure that only specific Azure processes can access the Language service.",
    "The solution must minimize administrative effort.",
    "What should you include in the solution?"
  ],
  "options": [
    {
      "id": "a",
      "text": "IPsec rules"
    },
    {
      "id": "b",
      "text": "Virtual network rules"
    },
    {
      "id": "c",
      "text": "Azure Application Gateway"
    },
    {
      "id": "d",
      "text": "Virtual network gateway"
    }
  ],
  "correctAnswer": "b",
  "explanation": [
    "Virtual network rules allow you to restrict access to the Azure Cognitive Service to specific virtual networks or subnets, limiting access to only authorized Azure processes.",
    "- ✅ Option B is correct.",
    "- ❌ Option A IPsec rules are lower-level security policies and require more administrative effort.",
    "- ❌ Option C Azure Application Gateway is a web traffic load balancer and does not provide direct network access control for Cognitive Services.",
    "- ❌ Option D Virtual network gateway connects on-premises networks to Azure but does not restrict access to services."
  ]
}
,{
  "id": "q137",
  "type": "single",
  "question": [
    "When evaluating a solution for compliance with Responsible AI principles, which of the following factors should be considered?"
  ],
  "options": [
    {
      "id": "a",
      "text": "Accountability"
    },
    {
      "id": "b",
      "text": "Cost Optimization"
    },
    {
      "id": "c",
      "text": "Scalability"
    },
    {
      "id": "d",
      "text": "Contracts"
    }
  ],
  "correctAnswer": "a",
  "explanation": [
    "Responsible AI principles emphasize fairness, transparency, accountability, privacy, and safety in AI systems.",
    "- ✅ Option A 'Accountability' ensures that there is clear ownership and responsibility for AI outcomes, making it a key factor.",
    "- ❌ Option B 'Cost Optimization' relates to financial planning, not ethical AI compliance.",
    "- ❌ Option C 'Scalability' refers to system growth, not ethical responsibility.",
    "- ❌ Option D 'Contracts' are legal tools, not principles of Responsible AI."
  ]
}
,{
  "id": "q138",
  "type": "single",
  "question": [
    "Which of the following steps are crucial when creating an Azure AI Resource?"
  ],
  "options": [
    {
      "id": "a",
      "text": "Configure networking settings"
    },
    {
      "id": "b",
      "text": "Select appropriate Azure region and right pricing tier"
    },
    {
      "id": "c",
      "text": "Set up data export schedule"
    }
  ],
  "correctAnswer": "b",
  "explanation": [
    "When creating an Azure AI Resource, choosing the correct Azure region and pricing tier ensures performance, cost efficiency, and compliance with data residency requirements.",
    "- ✅ Option B is crucial as it directly affects how the resource performs and what it costs.",
    "- ❌ Option A 'Configure networking settings' is optional and can be configured later depending on security needs.",
    "- ❌ Option C 'Set up data export schedule' is related to monitoring/logging but not essential at resource creation time."
  ]
}
,{
  "id": "q139",
  "type": "single",
  "question": [
    "You plan to perform predictive maintenance.",
    "You collect IoT sensor data from 100 industrial machines over a year, each with 50 sensors recording every minute.",
    "You need to identify unusual values in each time series to help predict machinery failures.",
    "Which Azure service should you use?"
  ],
  "options": [
    {
      "id": "a",
      "text": "Azure AI Computer Vision"
    },
    {
      "id": "b",
      "text": "Cognitive Search"
    },
    {
      "id": "c",
      "text": "Azure AI Document Intelligence"
    },
    {
      "id": "d",
      "text": "Azure AI Anomaly Detector"
    }
  ],
  "correctAnswer": "d",
  "explanation": [
    "Azure AI Anomaly Detector is ideal for analyzing time series data and detecting outliers, which is essential for predictive maintenance.",
    "- ✅ Option D is correct.",
    "- ❌ Other services are not suited for analyzing sensor-based time series data."
  ]
}
,{
  "id": "q140",
  "type": "single",
  "question": [
    "How are client applications typically granted access to an Azure AI Services endpoint?"
  ],
  "options": [
    {
      "id": "a",
      "text": "The application must specify a valid subscription key for the Azure resource"
    },
    {
      "id": "b",
      "text": "Access to Azure AI Services is granted to anonymous users by default"
    },
    {
      "id": "c",
      "text": "The user of the application must enter a user name and password associated with the Azure subscription"
    }
  ],
  "correctAnswer": "a",
  "explanation": [
    "Client applications access Azure AI Services by specifying a valid subscription key.",
    "This key is passed as part of the HTTP request headers and is required unless you're using Azure AD for authentication.",
    "- ✅ Option A is correct.",
    "- ❌ Other options do not represent valid or secure authentication methods for Azure services."
  ]
}
,{
  "id": "q141",
  "type": "single",
  "question": [
    "In which format are messages exchanged between a client application and an Azure AI Services resource when using a REST API?"
  ],
  "options": [
    {
      "id": "a",
      "text": "XML"
    },
    {
      "id": "b",
      "text": "JSON"
    },
    {
      "id": "c",
      "text": "HTML"
    }
  ],
  "correctAnswer": "b",
  "explanation": [
    "Azure AI Services REST APIs typically exchange data in JSON format.",
    "JSON is lightweight, easy to parse, and widely used for web service communication.",
    "- ✅ Option B is correct.",
    "- ❌ XML and HTML are not used for request/response formats in this context."
  ]
}
,{
  "id": "q143",
  "type": "single",
  "question": [
    "You have been assigned the task of using the Face service in Azure to identify the gender and age of individuals in a particular dataset.",
    "You have created a FaceClient object to achieve this and plan to use the Face client library in C#.",
    "You must determine the method to call to find the rectangular face coordinates or locations.",
    "Which method would you choose?"
  ],
  "options": [
    {
      "id": "a",
      "text": "GroupAsync"
    },
    {
      "id": "b",
      "text": "FindSimilarAsync"
    },
    {
      "id": "c",
      "text": "DetectWithUrlAsync"
    },
    {
      "id": "d",
      "text": "IdentifyAsync"
    }
  ],
  "correctAnswer": "c",
  "explanation": [
    "`DetectWithUrlAsync` is the correct method to call when you want to analyze a face in an image by URL.",
    "It returns the rectangular coordinates of the face, facial landmarks, and attributes like gender and age.",
    "- ✅ Option C is correct.",
    "- ❌ `GroupAsync` is used to group similar faces in a set.",
    "- ❌ `FindSimilarAsync` is used to find a matching face from a face list.",
    "- ❌ `IdentifyAsync` is used to identify a face against a known person group."
  ]
}
,{
  "id": "q145",
  "type": "single",
  "question": [
    "When running code on your computer that connects to Azure AI Services, you receive an error that access is denied.",
    "Which configuration do you need to set in the Azure AI Services instance?"
  ],
  "options": [
    {
      "id": "a",
      "text": "In the Networking properties, configure Selected Networks and Private Endpoints."
    },
    {
      "id": "b",
      "text": "In Networking properties, add your client IP address to the Firewall allowed list."
    },
    {
      "id": "c",
      "text": "In Access control, add your Microsoft Entra ID user account to the role."
    }
  ],
  "correctAnswer": "b",
  "explanation": [
    "If access is denied when connecting to Azure AI Services from your computer, the likely cause is that your client IP is not allowed.",
    "Adding your client IP address to the Firewall allowed list under Networking properties grants your computer permission to connect.",
    "- ✅ Option B is correct.",
    "- ❌ Option A is more advanced and typically used for network isolation scenarios.",
    "- ❌ Option C controls user permissions but does not affect IP-based access."
  ]
}




]
